# Tutorial: Pipeline de Dados dos Scraps

## Contexto do Projeto

O projeto EH-FAKE tem como objetivo principal a identifica√ß√£o e classifica√ß√£o de not√≠cias desinformativas. Para isso, √© fundamental um pipeline robusto de coleta e processamento de dados, que envolve a extra√ß√£o de conte√∫do de not√≠cias de diversas fontes. Este documento detalha o fluxo de dados desde a coleta inicial de URLs at√© a extra√ß√£o e classifica√ß√£o do conte√∫do.

## Vis√£o Geral do Pipeline

O pipeline de dados dos scraps no projeto EH-FAKE √© composto por tr√™s fases principais:

1.  **Coleta de URLs (Spiders)**: Respons√°vel por navegar em portais de not√≠cias e identificar URLs de artigos v√°lidos.
2.  **Extra√ß√£o de Conte√∫do (Plays)**: Acessa as URLs coletadas e extrai o conte√∫do textual relevante (t√≠tulo, descri√ß√£o, corpo da not√≠cia, tags).
3.  **Classifica√ß√£o e Armazenamento**: Processa o conte√∫do extra√≠do, classifica-o utilizando um modelo de linguagem (LLM) e armazena os dados e artefatos gerados.

Este fluxo garante que o projeto tenha acesso a um volume cont√≠nuo de dados de not√≠cias para an√°lise e identifica√ß√£o de desinforma√ß√£o.




## üï∑Ô∏è SPIDERS - Coleta de URLs

### Fun√ß√£o dos Spiders

Os *spiders* s√£o componentes do pipeline respons√°veis por navegar nas p√°ginas iniciais dos portais de not√≠cias e coletar URLs de artigos/not√≠cias v√°lidos. Eles atuam como a primeira etapa na aquisi√ß√£o de dados, identificando e filtrando links que ser√£o posteriormente processados para extra√ß√£o de conte√∫do.

### ‚ö†Ô∏è Altera√ß√µes Necess√°rias nos Spiders Existentes

Para garantir a efici√™ncia e a relev√¢ncia dos dados coletados, algumas altera√ß√µes s√£o cruciais nos *spiders* j√° implementados:

#### 1. Melhorar Filtragem de URLs

Muitos *spiders* existentes podem n√£o filtrar adequadamente os URLs, resultando na coleta de links irrelevantes ou duplicados. √â fundamental aprimorar a l√≥gica de filtragem para incluir apenas URLs que correspondam a artigos de not√≠cias. O *spider* `gazetaDoPovo.py` serve como um excelente exemplo de implementa√ß√£o de boa filtragem, utilizando an√°lise da URL para blacklist de se√ß√µes puras e valida√ß√£o de segmentos e *slugs* para identificar URLs de not√≠cias leg√≠timas.

**Exemplo de filtragem a ser evitada (r7.py):**

```python
def allow_url(self, entry_url):
    return (
        entry_url.startswith("https://")
        and len(entry_url) > 100
        and re.match(
            r"https://(entretenimento|esportes|record|noticias)\.r7\.com", entry_url
        )
    )
```

**Exemplo de filtragem recomendada (gazetaDoPovo.py):**

```python
def allow_url(self, url: str) -> bool:
    p = urlparse(url)
    path = p.path.rstrip("/")

    # 1) blacklist pure sections
    if path in {"/videos", "/vozes", "/podcasts", "/newsletter", "/ebooks"}:
        self.logger.info(f"Blacklisted URL: {url}")
        return False

    # 2) require at least two non-empty segments (section + slug)
    segments = [seg for seg in path.split("/") if seg]
    if len(segments) < 2:
        self.logger.info(f"Blacklisted URL: {url}")
        return False

    slug = segments[-1]
    # 3a) long slugs by hyphens or 3b) by character length
    if slug.count("-") >= 3 or len(slug) > 30:
        return True

    return False
```

#### 2. Remover `yield scrapy.Request` Recursivos

O objetivo dos *spiders* √© coletar apenas as URLs de not√≠cias presentes na p√°gina inicial do portal, sem realizar um *crawling* recursivo. Portanto, quaisquer chamadas `yield scrapy.Request` que iniciem novas requisi√ß√µes recursivas devem ser removidas do m√©todo `parse`.

**Exemplo de c√≥digo a ser removido:**

```python
def parse(self, response):
    seen = set()
    for entry in response.css("a"):
        url = entry.attrib.get("href")
        if url and url not in seen and self.allow_url(url):
            seen.add(url)
            yield URLItem(url=url)
            yield scrapy.Request(url=url, callback=self.parse)  # ‚Üê REMOVER ESTA LINHA
```

**Exemplo de c√≥digo correto:**

```python
def parse(self, response):
    seen = set()
    for entry in response.css("a"):
        url = entry.attrib.get("href")
        if url and url not in seen and self.allow_url(url):
            seen.add(url)
            yield URLItem(url=url)  # ‚Üê MANTER APENAS ISTO
```

### üÜï Criando um Novo Spider

Para adicionar um novo portal ao pipeline de coleta, siga a estrutura base para a cria√ß√£o de um novo *spider*. √â crucial adaptar a l√≥gica de filtragem de URLs (`allow_url`) e os seletores CSS/XPath (`response.css` ou `response.xpath`) para o portal espec√≠fico, garantindo que apenas URLs de not√≠cias v√°lidas sejam coletadas.

**Estrutura base para um novo spider:**

```python
import scrapy
from spiders.base import BaseSpider
from spiders.items import URLItem
from urllib.parse import urlparse

class NovoPortalSpider(BaseSpider):
    name = "novoportalspider"
    start_urls = ["https://www.novoportal.com.br/"]
    allowed_domains = ["novoportal.com.br"]

    custom_settings = {
        **BaseSpider.custom_settings,
        "COOKIES_ENABLED": True,
        "DOWNLOAD_DELAY": 3,
        "DEFAULT_REQUEST_HEADERS": {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Cache-Control": "max-age=0",
        }
    }

    def allow_url(self, url: str) -> bool:
        """
        Implementar l√≥gica espec√≠fica do portal para filtrar apenas URLs de not√≠cias
        """
        p = urlparse(url)
        path = p.path.rstrip("/")
        
        # Exemplo de filtragem - adaptar para cada portal
        segments = [seg for seg in path.split("/") if seg]
        if len(segments) < 2:
            return False
            
        # Verificar se √© uma not√≠cia real (slug longo, m√∫ltiplos h√≠fens, etc.)
        slug = segments[-1]
        return slug.count("-") >= 2 or len(slug) > 20

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(
                url,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_redirect": True, "handle_httpstatus_list": [403]},
            )

    def parse(self, response):
        # Adaptar seletor CSS/XPath para o portal espec√≠fico
        links = response.css("a")  # ou response.xpath("//a")
        
        for link in links:
            url = link.attrib.get("href")
            if not url:
                continue
                
            # Construir URL absoluta
            full_url = response.urljoin(url)
            full_url = full_url.split("#", 1)[0].split("", 1)[0]  # Remove fragmentos e query params
            
            if self.allow_url(full_url):
                yield URLItem(url=full_url)
```




## üé≠ PLAYS - Extra√ß√£o de Conte√∫do

### Fun√ß√£o dos Plays

Os *plays* s√£o a segunda etapa do pipeline de dados, respons√°veis por acessar cada URL de not√≠cia coletada pelos *spiders* e extrair o conte√∫do textual relevante. Isso inclui o t√≠tulo, a descri√ß√£o (subt√≠tulo/resumo), o corpo completo da mat√©ria e as tags/categorias associadas √† not√≠cia.

### ‚ö†Ô∏è Altera√ß√µes Necess√°rias nos Plays Existentes

Para otimizar a extra√ß√£o de conte√∫do e focar nos dados necess√°rios para a an√°lise de desinforma√ß√£o, as seguintes altera√ß√µes s√£o essenciais nos *plays* existentes:

#### 1. Remover `take_screenshot`

Anteriormente, os *plays* poderiam ter funcionalidades para capturar *screenshots* das p√°ginas. No entanto, para o objetivo atual de an√°lise de texto, as capturas de tela n√£o s√£o mais necess√°rias. Todas as chamadas para `take_screenshot` e o armazenamento de caminhos de *screenshot* devem ser removidos.

**Exemplo de c√≥digo a ser removido:**

```python
entry_screenshot_path = self.take_screenshot(page, self.url, goto=False)

# E na cria√ß√£o do EntryItem:
return EntryItem(
    title=entry_title,
    ads=ad_items,
    url=self.url,
    screenshot_path=entry_screenshot_path,  # ‚Üê REMOVER
)
```

#### 2. Atualizar TODOS os Seletores

A estrutura HTML dos portais de not√≠cias √© din√¢mica e pode mudar frequentemente. √â **IMPERATIVO** que todos os seletores CSS/XPath utilizados nos *plays* para extrair o t√≠tulo, corpo, descri√ß√£o e tags sejam revisados e atualizados. Isso garante que o conte√∫do correto seja extra√≠do, mesmo ap√≥s altera√ß√µes no layout dos sites.

*   **Seletores existentes** (para t√≠tulo e corpo) podem estar desatualizados e precisam de verifica√ß√£o.
*   **Novos seletores** s√£o necess√°rios para a extra√ß√£o da descri√ß√£o (subt√≠tulo/resumo) e das tags/categorias, que s√£o campos novos e cruciais para a an√°lise.

#### 3. Focar na Extra√ß√£o de Conte√∫do e Remover C√≥digo de An√∫ncios

O foco principal do projeto mudou da an√°lise de an√∫ncios para a an√°lise de not√≠cias. Portanto, todo o c√≥digo relacionado √† detec√ß√£o, extra√ß√£o e processamento de an√∫ncios deve ser removido dos *plays*. O foco deve ser exclusivamente na extra√ß√£o de:

*   **title**: T√≠tulo da not√≠cia (revisar seletores existentes).
*   **description**: Subt√≠tulo/resumo da not√≠cia (NOVO campo - identificar seletores).
*   **body**: Corpo completo da not√≠cia (revisar seletores existentes).
*   **tags**: Tags/categorias da not√≠cia (NOVO campo - identificar seletores).

Especificamente, remover:

*   M√©todos como `find_items` que eram usados para an√∫ncios.
*   Vari√°veis como `n_expected_ads`.
*   Locators espec√≠ficos para elementos de an√∫ncio (ex: `.videoCube`, `#taboola-*`).

### üÜï Criando um Novo Play

Ao adicionar um novo portal ao pipeline, um novo *play* correspondente deve ser criado. A estrutura base a seguir serve como ponto de partida, mas a implementa√ß√£o do m√©todo `run` e a identifica√ß√£o dos seletores devem ser adaptadas √† estrutura HTML de cada portal para garantir a extra√ß√£o precisa do conte√∫do.

**Estrutura base para um novo play:**

```python
import time
from playwright.sync_api import sync_playwright
from plays.base import BasePlay
from plays.items import EntryItem
from plog import logger

class NovoPortalPlay(BasePlay):
    name = "novoportal"

    @classmethod
    def match(cls, url):
        return "novoportal.com.br" in url

    def pre_run(self):
        pass

    def run(self) -> EntryItem:
        with sync_playwright() as p:
            browser = self.launch_browser(p, viewport={"width": 1920, "height": 1080})
            page = browser.new_page()
            logger.info(f"[{self.name}] Opening URL \'{self.url}\'...")
            page.goto(self.url, timeout=180_000)
            
            # Implementar a l√≥gica de extra√ß√£o de t√≠tulo, descri√ß√£o, corpo e tags aqui
            # Exemplo (adaptar para o portal):
            entry_title = page.locator("h1.title").inner_text() if page.locator("h1.title").is_visible() else ""
            entry_description = page.locator("meta[name=\"description\"]").get_attribute("content") if page.locator("meta[name=\"description\"]").is_visible() else ""
            entry_body = " ".join([p.inner_text() for p in page.locator("div.article-body p").all()])
            entry_tags = [tag.inner_text() for tag in page.locator("a.tag").all()]

            return EntryItem(
                title=entry_title,
                description=entry_description,
                body=entry_body,
                tags=entry_tags,
                url=self.url,
            )
```




## Classifica√ß√£o e Armazenamento

Ap√≥s a coleta e extra√ß√£o do conte√∫do das not√≠cias pelos *spiders* e *plays*, a etapa final do pipeline de dados envolve a classifica√ß√£o do conte√∫do e o armazenamento dos dados processados.

### Classifica√ß√£o com LLM

Cada not√≠cia coletada e seu conte√∫do extra√≠do s√£o submetidos a um modelo de linguagem grande (LLM) para classifica√ß√£o. Esta classifica√ß√£o √© opcional e depende da configura√ß√£o do ambiente, exigindo uma chave de API para o servi√ßo de LLM (por exemplo, OpenAI).

O processo de classifica√ß√£o visa categorizar as not√≠cias em diferentes temas ou identificar a presen√ßa de desinforma√ß√£o, conforme os objetivos do projeto EH-FAKE. As categorias de classifica√ß√£o s√£o definidas em arquivos espec√≠ficos do projeto (ex: `llm/categories.py`).

### Armazenamento de Dados

Os dados coletados e classificados s√£o armazenados em um banco de dados, cuja estrutura √© definida em `models.py`. As tabelas principais incluem:

*   **Portal**: Informa√ß√µes sobre os portais de not√≠cias analisados.
*   **Entry**: Detalhes das not√≠cias coletadas de cada portal, incluindo o conte√∫do extra√≠do.
*   **URLQueue**: Fila de URLs para processamento pelos *spiders* e *plays*.
*   **QueueStatus**: Status de cada fila de *scraping*.

Al√©m dos dados textuais, artefatos como capturas de tela (se habilitadas, embora n√£o mais necess√°rias para o objetivo atual) e outros metadados podem ser armazenados para fins de auditoria ou an√°lise futura. O armazenamento √© configurado para garantir a persist√™ncia e a acessibilidade dos dados para as etapas subsequentes de an√°lise e apresenta√ß√£o.

### Execu√ß√£o dos Scripts

O pipeline √© orquestrado atrav√©s de scripts de execu√ß√£o que gerenciam as etapas de coleta, extra√ß√£o e, implicitamente, a classifica√ß√£o e o armazenamento. Os comandos principais para iniciar os servi√ßos e executar as etapas s√£o:

*   `make start`: Inicia os servi√ßos necess√°rios, como containers Docker para o banco de dados e ambiente de execu√ß√£o.
*   `make init_db`: Cria as tabelas necess√°rias no banco de dados.
*   `make migrate_db`: Aplica migra√ß√µes pendentes no banco de dados.
*   `make crawl`: Executa os *spiders* para coletar URLs de not√≠cias.
*   `make scrape`: Executa os *plays* para extrair o conte√∫do das not√≠cias a partir das URLs coletadas.

Esses comandos garantem que o fluxo de dados seja executado de forma sequencial e que os dados sejam devidamente processados e armazenados para uso posterior na an√°lise de desinforma√ß√£o.



