# Pipeline de Dados do Check-up

## Contexto do Projeto

O projeto **Check-up** tem como objetivo principal analisar a presen√ßa de desinforma√ß√£o em an√∫ncios publicit√°rios de sa√∫de que circulam em grandes sites de not√≠cias do Brasil. Este documento detalha o fluxo completo de dados desde a coleta inicial de URLs at√© a apresenta√ß√£o final no frontend web.

## üîÑ Vis√£o Geral do Pipeline

O pipeline de dados do Check-up √© composto por **cinco etapas principais**:

1. **üï∑Ô∏è Crawling (Spiders)**: Coleta URLs de artigos dos portais de not√≠cias
2. **üì∞ Scraping (Plays)**: Extrai conte√∫do e an√∫ncios das p√°ginas coletadas  
3. **üóÑÔ∏è Armazenamento**: Salva dados no PostgreSQL e arquivos no MinIO
4. **üîó API Backend**: Serve dados via FastAPI
5. **üñ•Ô∏è Frontend**: Interface React para visualiza√ß√£o


![Diagrama do Pipeline](../assets/diagrama-pipeline.png)

## üìä Estrutura de Dados

### Modelos Principais (PostgreSQL)

**Portal**: Informa√ß√µes dos portais de not√≠cias
```python
- id: Integer (Primary Key)
- name: String (Nome do portal)
- url: URL (URL base do portal)
- slug: String (Identificador √∫nico)
- created_at: DateTime
```

**URLQueue**: Fila de URLs para processamento
```python
- id: Integer (Primary Key)  
- url: URL (URL do artigo)
- created_at: DateTime
- statuses: List[QueueStatus] (Status de processamento)
```

**Entry**: Artigos de not√≠cias processados
```python
- id: Integer (Primary Key)
- portal_id: ForeignKey (Portal)
- title: String (T√≠tulo do artigo)
- url: URL (URL do artigo)
- description: String (Subt√≠tulo/resumo)
- body: Text (Corpo completo do artigo)
- tags: JSON (Tags/categorias)
- screenshot: String (Caminho no MinIO)
- created_at: DateTime
```

**Advertisement**: An√∫ncios encontrados nos artigos
```python
- id: Integer (Primary Key)
- entry_id: ForeignKey (Entry)
- url: URL (URL do an√∫ncio)
- title: String (T√≠tulo do an√∫ncio)
- tag: String (Categoria do an√∫ncio)
- thumbnail: URL (URL da thumbnail)
- screenshot: String (Caminho no MinIO)
- excerpt: String (Descri√ß√£o)
- category: Integer (Categoria LLM)
- category_verbose: String (Descri√ß√£o da categoria)
- created_at: DateTime
```




## üï∑Ô∏è ETAPA 1: Spiders - Coleta de URLs

### Fun√ß√£o dos Spiders

Os **spiders** s√£o componentes Scrapy respons√°veis por navegar nas p√°ginas iniciais dos portais de not√≠cias e coletar URLs de artigos v√°lidos. Eles constituem o ponto de entrada do pipeline, identificando e filtrando links que ser√£o posteriormente processados.

### üîß Como Funcionam

1. **Navega√ß√£o**: Acessam a p√°gina inicial do portal (ex: `https://www.metropoles.com/`)
2. **Extra√ß√£o**: Identificam links de artigos usando seletores CSS/XPath
3. **Filtragem**: Aplicam regras para aceitar apenas URLs de not√≠cias v√°lidas
4. **Armazenamento**: Salvam URLs aprovadas na tabela `URLQueue`

### üìã Portais Suportados

**‚úÖ Funcionais:**
- Metr√≥poles (`metropolesspider`)
- IG (`igspider`) 
- MaisGoi√°s (`maisgoiasspider`)
- AliadosBrasil (`aliadosbrasilspider`)
- Veja (`vejaspider`)
- R7 (`r7spider`)
- UOL (`uolspider`)
- Folha (`folhaspider`)

### ‚öôÔ∏è Configura√ß√£o dos Spiders

**Estrutura Base:**
```python
class MetropolesSpider(BaseSpider):
    name = "metropolesspider"
    start_urls = ["https://www.metropoles.com/"]
    allowed_domains = ["metropoles.com"]
    
    def allow_url(self, entry_url):
        # L√≥gica de filtragem espec√≠fica do portal
        return True
    
    def parse(self, response):
        # Extrai URLs usando seletores CSS
        for entry in response.css(".noticia__titulo > a"):
            url = entry.attrib.get("href")
            if url and self.allow_url(url):
                yield URLItem(url=url)
```

**Pipeline de Processamento:**
```python
# pipelines.py
class PostgresPipeline:
    def process_item(self, item, spider):
        # Verifica se URL j√° existe
        exists = self.session.query(URLQueue).filter(
            URLQueue.url == item["url"]
        ).first()
        
        if exists is None:
            # Cria nova entrada na fila
            URLQueue.create(self.session, item["url"])
        
        return item
```

### üöÄ Execu√ß√£o

**Comandos individuais:**
```bash
make crawl_metropoles  # Coleta URLs do Metr√≥poles
make crawl_ig          # Coleta URLs do IG
make crawl_veja        # Coleta URLs da Veja
```

**Execu√ß√£o em massa:**
```bash
make crawl_all_working # Todos os portais funcionais
```

**Verifica√ß√£o:**
```sql
-- Visualizar URLs coletadas
SELECT COUNT(*) FROM urlqueue;
SELECT url FROM urlqueue ORDER BY created_at DESC LIMIT 10;
```




## üé≠ ETAPA 2: Plays - Extra√ß√£o de Conte√∫do e An√∫ncios

### Fun√ß√£o dos Plays

Os **plays** s√£o componentes baseados em Playwright que acessam cada URL coletada pelos spiders e extraem tanto o conte√∫do editorial quanto os an√∫ncios presentes na p√°gina. Eles constituem o cora√ß√£o do sistema de an√°lise, capturando dados estruturados para posterior classifica√ß√£o.

### üîß Como Funcionam

1. **Sele√ß√£o de URLs**: Buscam URLs na `URLQueue` filtradas por dom√≠nio
2. **Navega√ß√£o**: Usam Playwright para abrir as p√°ginas em um browser real
3. **Extra√ß√£o Dual**:
   - **Conte√∫do Editorial**: t√≠tulo, descri√ß√£o, corpo, tags
   - **An√∫ncios**: t√≠tulo, URL, thumbnail, categoria
4. **Armazenamento**: Salvam dados nas tabelas `Entry` e `Advertisement`
5. **Screenshots**: Capturam imagens das p√°ginas no MinIO

### üìä Estrutura dos Dados Extra√≠dos

**EntryItem (Conte√∫do Editorial):**
```python
@dataclass
class EntryItem:
    title: str           # T√≠tulo principal do artigo
    url: str            # URL do artigo  
    description: str    # Subt√≠tulo/resumo
    body: str          # Corpo completo do texto
    tags: List[str]    # Tags/categorias
    ads: List[AdItem]  # An√∫ncios encontrados
    screenshot_path: str # Caminho da screenshot
```

**AdItem (An√∫ncios):**
```python
@dataclass  
class AdItem:
    title: str          # T√≠tulo do an√∫ncio
    url: str           # URL de destino
    thumbnail_url: str # URL da imagem
    tag: str          # Categoria/origem (ex: "Taboola")
    screenshot_path: str # Screenshot do an√∫ncio
    excerpt: str      # Descri√ß√£o/texto
```

### üéØ Exemplo de Play Funcional

**Metr√≥poles Play:**
```python
class MetropolesPlay(BasePlay):
    name = "metropoles"
    
    @classmethod
    def match(cls, url):
        return "metropoles.com" in url
    
    def run(self) -> EntryItem:
        with sync_playwright() as p:
            browser = self.launch_browser(p)
            page = browser.new_page()
            page.goto(self.url, timeout=180_000)
            
            # Extra√ß√£o do conte√∫do editorial
            entry_title = page.locator("//h1").first.inner_text()
            
            description = ""
            try:
                description = page.locator(".noticiaCabecalho__subtitulo").inner_text()
            except:
                pass
            
            # Corpo do artigo
            body_paragraphs = []
            selectors = [".m-content", "article", ".article-content"]
            for selector in selectors:
                try:
                    elements = page.locator(f"{selector} p").all()
                    body_paragraphs = [p.inner_text() for p in elements]
                    if body_paragraphs:
                        break
                except:
                    continue
            
            body = " ".join(body_paragraphs)
            
            # Extra√ß√£o de an√∫ncios (Taboola)
            ad_items = []
            try:
                page.locator("#taboola-below-article-thumbnails").scroll_into_view_if_needed()
                time.sleep(3)
                
                ad_elements = page.locator(".videoCube").all()
                for element in ad_elements:
                    if element.is_visible():
                        content = element.inner_html()
                        ad_item = self.find_items(content)
                        if ad_item.is_valid():
                            ad_items.append(ad_item)
            except:
                pass
            
            return EntryItem(
                title=entry_title,
                description=description, 
                body=body,
                tags=[],
                ads=ad_items,
                url=self.url
            )
```

### üóÑÔ∏è Processo de Armazenamento

**1. Sele√ß√£o de URLs para Processamento:**
```python
# scrape_no_openai.py
def get_urls_for_platform(session, domain):
    """Busca URLs da URLQueue filtradas por dom√≠nio"""
    return session.query(URLQueue).join(URLQueue.statuses).filter(
        URLQueue.url.like(f"%{domain}%"),
        QueueStatus.value == "created" 
    ).limit(100).all()
```

**2. Processamento e Armazenamento:**
```python
# Para cada URL processada:
1. Play extrai conte√∫do ‚Üí EntryItem
2. Salva Entry no PostgreSQL
3. Para cada an√∫ncio ‚Üí Advertisement no PostgreSQL  
4. Screenshots salvos no MinIO
5. Atualiza QueueStatus para "completed"
```

### üöÄ Execu√ß√£o

**Comandos por portal:**
```bash
make scrape_metropoles     # Scraping do Metr√≥poles
make scrape_ig            # Scraping do IG  
make scrape_maisgoias     # Scraping do MaisGoi√°s
```

**Execu√ß√£o em massa:**
```bash
make scrape_all_working   # Todos os portais funcionais
```

**Comando direto (com argumentos):**
```bash
docker compose exec scraper python scrape_no_openai.py --platform metropoles.com
```

### üìà Monitoramento

**Verifica√ß√£o no banco:**
```sql
-- Artigos processados
SELECT COUNT(*) FROM entry;

-- An√∫ncios encontrados  
SELECT COUNT(*) FROM advertisement;

-- Status por portal
SELECT 
    SUBSTRING(url FROM 'https://([^/]+)') as portal,
    COUNT(*) as total_urls
FROM urlqueue 
GROUP BY portal;
```

**Verifica√ß√£o no MinIO:**
- Console: `http://localhost:9001`
- Credenciais: `minioadmin/minioadmin`
- Buckets organizados por portal: `metropoles_com/`, `ig_com_br/`, etc.




## üóÑÔ∏è ETAPA 3: Armazenamento de Dados

### PostgreSQL - Banco Principal

O Check-up utiliza PostgreSQL como banco principal para armazenar dados estruturados. O schema √© definido em `models.py` usando SQLAlchemy ORM.

**Fluxo de Armazenamento:**
```mermaid
graph TD
    A[Spider coleta URL] --> B[URLQueue - Fila de URLs]
    B --> C[Play processa URL]
    C --> D[Entry - Artigo principal]
    C --> E[Advertisement - An√∫ncios encontrados]
    D --> F[Portal - Info do portal]
    E --> F
```

**Relacionamentos:**
- `Portal` 1:N `Entry` (Um portal tem muitos artigos)
- `Entry` 1:N `Advertisement` (Um artigo tem muitos an√∫ncios)
- `URLQueue` 1:N `QueueStatus` (Uma URL tem hist√≥rico de status)

### MinIO - Armazenamento de Arquivos

O MinIO √© usado para armazenar screenshots e m√≠dias capturadas durante o scraping.

**Estrutura de Buckets:**
```
scraped-articles/
‚îú‚îÄ‚îÄ metropoles_com/
‚îÇ   ‚îú‚îÄ‚îÄ 2025/01/07/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entry_screenshots/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ad_screenshots/
‚îú‚îÄ‚îÄ ig_com_br/
‚îÇ   ‚îú‚îÄ‚îÄ 2025/01/07/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entry_screenshots/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ad_screenshots/
```

**Processo de Upload:**
```python
# storage.py
def upload_file(local_path, minio_path):
    """Upload de arquivo para MinIO"""
    client.fput_object(
        bucket_name="scraped-articles",
        object_name=minio_path,
        file_path=local_path
    )
```

### üîÑ Status de Processamento

O sistema mant√©m controle detalhado do status de cada URL atrav√©s da tabela `QueueStatus`:

**Estados poss√≠veis:**
- `created`: URL coletada, aguardando processamento
- `processing`: URL sendo processada por um play
- `completed`: Processamento conclu√≠do com sucesso
- `failed`: Erro durante processamento
- `retry`: Agendada para nova tentativa

**Exemplo de consulta de status:**
```sql
SELECT 
    u.url,
    qs.value as status,
    qs.created_at
FROM urlqueue u
JOIN queuestatus qs ON u.id = qs.url_queue_id
WHERE qs.value = 'completed'
ORDER BY qs.created_at DESC;
```

## üîó ETAPA 4: API Backend (FastAPI)

### Estrutura da API

O backend em FastAPI (`web/server/main.py`) serve como ponte entre os dados armazenados e o frontend, fornecendo endpoints RESTful para acesso aos dados coletados.

**Endpoints Principais:**

**1. Listar Portais:**
```python
@app.get("/portais", response_model=List[str])
def listar_portais():
    """Retorna lista de portais com dados dispon√≠veis"""
    # Busca diretamente nos buckets do MinIO
    portais = set()
    for obj in minio_client.list_objects(MINIO_BUCKET, recursive=True):
        portal = obj.object_name.split('/')[0]
        portais.add(portal)
    return sorted(list(portais))
```

**2. Buscar Not√≠cias:**
```python
@app.get("/noticias/{portal}")
def buscar_noticias(
    portal: str,
    q: str = Query(None, description="Termo de busca"),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=1, le=100)
):
    """Busca not√≠cias de um portal espec√≠fico com pagina√ß√£o"""
    # Busca arquivos JSON no MinIO do portal especificado
    # Aplica filtros de busca se fornecidos
    # Retorna dados paginados
```

**3. Detalhes da Not√≠cia:**
```python
@app.get("/noticia/{portal}/{filename}")
def obter_noticia(portal: str, filename: str):
    """Retorna dados completos de uma not√≠cia espec√≠fica"""
    # Busca arquivo JSON espec√≠fico no MinIO
    # Retorna conte√∫do completo incluindo an√∫ncios
```

### üîÑ Fluxo de Dados da API

```mermaid
graph TD
    A[Frontend Request] --> B[FastAPI Endpoint]
    B --> C[Consulta MinIO]
    C --> D[Processa Dados JSON]
    D --> E[Aplica Filtros/Pagina√ß√£o]
    E --> F[Retorna Response]
    F --> G[Frontend Atualiza UI]
```

### üìä Formato de Resposta

**Estrutura de Not√≠cia:**
```json
{
  "title": "T√≠tulo da not√≠cia",
  "url": "https://portal.com/noticia",
  "description": "Subt√≠tulo ou resumo",
  "body": "Corpo completo da not√≠cia...",
  "tags": ["pol√≠tica", "sa√∫de"],
  "timestamp": "2025-01-07T10:30:00Z",
  "portal": "metropoles_com",
  "ads": [
    {
      "title": "An√∫ncio de produto",
      "url": "https://anuncio.com",
      "tag": "Taboola",
      "thumbnail": "https://thumb.com/img.jpg"
    }
  ]
}
```

## üñ•Ô∏è ETAPA 5: Frontend (React)

### Arquitetura do Frontend

O frontend em React (`web/client/src/`) oferece interface intuitiva para explorar os dados coletados, com foco na visualiza√ß√£o de not√≠cias e an√∫ncios de sa√∫de.

**Componentes Principais:**

**1. Home.tsx - P√°gina Principal:**
```tsx
const Home = () => {
  const [portais, setPortais] = useState<string[]>([]);
  const [noticias, setNoticias] = useState<any[]>([]);
  const [portalSelecionado, setPortalSelecionado] = useState<string | null>(null);
  
  // Busca lista de portais dispon√≠veis
  useEffect(() => {
    fetch(`${API_URL}/portais`)
      .then(res => res.json())
      .then(setPortais);
  }, []);
  
  // Busca not√≠cias quando portal √© selecionado
  useEffect(() => {
    if (portalSelecionado) {
      fetch(`${API_URL}/noticias/${portalSelecionado}`)
        .then(res => res.json())
        .then(setNoticias);
    }
  }, [portalSelecionado]);
}
```

**2. Funcionalidades Implementadas:**

- **Sele√ß√£o de Portal**: Tabs para escolher entre portais dispon√≠veis
- **Busca**: Campo de texto para filtrar not√≠cias por t√≠tulo/conte√∫do
- **Pagina√ß√£o**: Navega√ß√£o atrav√©s de grandes volumes de dados
- **Visualiza√ß√£o Detalhada**: Modal com conte√∫do completo da not√≠cia
- **Estat√≠sticas**: Contadores de not√≠cias por portal

### üé® Interface do Usu√°rio

**Layout Principal:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Check-up - An√°lise de An√∫ncios de Sa√∫de ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ [Metr√≥poles] [IG] [Veja] [R7] [UOL]...  ‚îÇ <- Tabs dos portais
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üîç [___________________] [Buscar]       ‚îÇ <- Campo de busca
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ üì∞ T√≠tulo da Not√≠cia               ‚îÇ ‚îÇ
‚îÇ ‚îÇ üè∑Ô∏è  Portal: Metr√≥poles             ‚îÇ ‚îÇ <- Cards de not√≠cias
‚îÇ ‚îÇ üìÖ 07/01/2025 - üîó 2 an√∫ncios      ‚îÇ ‚îÇ
‚îÇ ‚îÇ [Ver Detalhes]                     ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Üê 1 2 3 ... 10 ‚Üí                       ‚îÇ <- Pagina√ß√£o
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîÑ Fluxo de Intera√ß√£o

1. **Carregamento Inicial**: Busca lista de portais dispon√≠veis
2. **Sele√ß√£o de Portal**: Usu√°rio clica em tab do portal desejado
3. **Exibi√ß√£o de Not√≠cias**: Lista paginada de not√≠cias do portal
4. **Busca**: Usu√°rio pode filtrar por termos espec√≠ficos
5. **Detalhamento**: Modal mostra conte√∫do completo + an√∫ncios
6. **Navega√ß√£o**: Pagina√ß√£o para explorar mais resultados

### üì± Recursos de UX

- **Loading States**: Skeletons durante carregamento
- **Error Handling**: Mensagens de erro amig√°veis  
- **Responsive Design**: Adapt√°vel a diferentes tamanhos de tela
- **Busca Reativa**: Resultados atualizados em tempo real
- **Indicadores Visuais**: Badges para n√∫mero de an√∫ncios

## üîÑ FLUXO COMPLETO DE DADOS

### Pipeline End-to-End

```mermaid
graph TD
    A[Portal de Not√≠cias] --> B[Spider - Scrapy]
    B --> C[URLQueue - PostgreSQL]
    C --> D[Play - Playwright]
    D --> E[Entry - PostgreSQL]
    D --> F[Advertisement - PostgreSQL]
    D --> G[Screenshots - MinIO]
    E --> H[FastAPI Backend]
    F --> H
    G --> H
    H --> I[React Frontend]
    I --> J[Interface do Usu√°rio]
    
    style A fill:#e1f5fe
    style J fill:#e8f5e8
    style H fill:#fff3e0
```

### ‚è±Ô∏è Cronograma de Execu√ß√£o

**1. Configura√ß√£o (√∫nica vez):**
```bash
make setup  # ~5-10 minutos
```

**2. Coleta de URLs (di√°ria):**
```bash
make crawl_all_working  # ~10-15 minutos
# Resultado: ~500-1000 URLs novas por portal
```

**3. Extra√ß√£o de Conte√∫do (cont√≠nua):**
```bash
make scrape_all_working  # ~2-4 horas
# Resultado: ~100-200 artigos processados por hora
```

**4. Visualiza√ß√£o (imediata):**
```bash
# Frontend dispon√≠vel em: http://localhost:5173
# API dispon√≠vel em: http://localhost:8000
```

## üöÄ Comandos de Execu√ß√£o

### Setup Inicial
```bash
# Configura√ß√£o completa do ambiente
make setup

# Verificar se servi√ßos est√£o rodando
docker compose ps

# Acessar logs em tempo real
docker compose logs -f
```

### Execu√ß√£o do Pipeline

**1. Coleta de URLs (Crawling):**
```bash
# Todos os portais funcionais
make crawl_all_working

# Portais individuais
make crawl_metropoles
make crawl_ig
make crawl_veja
make crawl_uol
```

**2. Extra√ß√£o de Conte√∫do (Scraping):**
```bash
# Todos os portais funcionais  
make scrape_all_working

# Portais individuais
make scrape_metropoles
make scrape_ig
make scrape_maisgoias
```

**3. Pipeline Completo:**
```bash
# Executa crawl + scraping sequencialmente
make pipeline_complete

# Workflow otimizado
make collect_working
```

### Monitoramento e Debugging

**Acesso ao container:**
```bash
make bash
```

**Verifica√ß√µes no banco:**
```bash
# Dentro do container
python -c "
from models import *
from sqlalchemy import create_engine
from sqlalchemy.orm import Session
from decouple import config

engine = create_engine(config('DATABASE_URL'))
session = Session(engine)

print('URLs na fila:', URLQueue.select().count())
print('Artigos processados:', Entry.select().count()) 
print('An√∫ncios encontrados:', Advertisement.select().count())
"
```

**Verifica√ß√£o do MinIO:**
- Interface: `http://localhost:9001`
- Credenciais: `minioadmin/minioadmin`

**Frontend e API:**
- Frontend: `http://localhost:5173`
- API: `http://localhost:8000`
- Docs da API: `http://localhost:8000/docs`

### Troubleshooting

**Problema: Playwright browsers n√£o instalados**
```bash
docker compose down
docker compose build --no-cache
make start
```

**Problema: Banco n√£o conecta**
```bash
make wait-for-db
# ou
make stop && make start
```

**Problema: MinIO n√£o acess√≠vel**
```bash
docker compose ps
# Verificar se container healthcheck_minio est√° "healthy"
```

## üîß Configura√ß√£o e Customiza√ß√£o

### Vari√°veis de Ambiente (.env)

```bash
# Banco de dados
DATABASE_URL=postgresql://postgres:supersecret@healthcheck_db:5432/healthcheck

# MinIO
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_SECURE=False

# Opcional: OpenAI para classifica√ß√£o
OPENAI_API_KEY=sk-...

# Credenciais de portais (se necess√°rio)
FOLHA_USERNAME=usuario
FOLHA_PASSWORD=senha
```

### Adicionando Novo Portal

**1. Criar Spider:**
```python
# spiders/novoportal.py
class NovoPortalSpider(BaseSpider):
    name = "novoportalspider"
    start_urls = ["https://novoportal.com/"]
    # ... implementa√ß√£o espec√≠fica
```

**2. Criar Play:**
```python  
# plays/novoportal.py
class NovoPortalPlay(BasePlay):
    name = "novoportal"
    
    @classmethod
    def match(cls, url):
        return "novoportal.com" in url
    # ... implementa√ß√£o espec√≠fica
```

**3. Adicionar comandos no Makefile:**
```makefile
crawl_novoportal:
	docker compose run scraper python crawl.py novoportalspider

scrape_novoportal:
	docker compose exec scraper python scrape_no_openai.py --platform novoportal.com
```

**4. Testar:**
```bash
make crawl_novoportal
make scrape_novoportal
```

## üìà M√©tricas e An√°lises

### Consultas √öteis

**Resumo geral:**
```sql
SELECT 
    COUNT(DISTINCT p.name) as total_portais,
    COUNT(DISTINCT e.id) as total_artigos,
    COUNT(DISTINCT a.id) as total_anuncios,
    AVG(ads_por_artigo.count) as media_anuncios_por_artigo
FROM portal p
LEFT JOIN entry e ON p.id = e.portal_id
LEFT JOIN advertisement a ON e.id = a.entry_id
LEFT JOIN (
    SELECT entry_id, COUNT(*) as count 
    FROM advertisement 
    GROUP BY entry_id
) ads_por_artigo ON e.id = ads_por_artigo.entry_id;
```

**Top portais por volume:**
```sql
SELECT 
    p.name,
    COUNT(e.id) as artigos,
    COUNT(a.id) as anuncios
FROM portal p
LEFT JOIN entry e ON p.id = e.portal_id  
LEFT JOIN advertisement a ON e.id = a.entry_id
GROUP BY p.name
ORDER BY artigos DESC;
```

**Evolu√ß√£o temporal:**
```sql
SELECT 
    DATE(e.created_at) as data,
    COUNT(e.id) as artigos_processados,
    COUNT(a.id) as anuncios_encontrados
FROM entry e
LEFT JOIN advertisement a ON e.id = a.entry_id
WHERE e.created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY DATE(e.created_at)
ORDER BY data DESC;
```

## üéØ Pr√≥ximos Passos

### Melhorias Planejadas

1. **Classifica√ß√£o Autom√°tica**: Integra√ß√£o com LLM para categoriza√ß√£o de an√∫ncios
2. **Dashboard Analytics**: Painel com m√©tricas em tempo real
3. **Alertas**: Notifica√ß√µes para an√∫ncios suspeitos de desinforma√ß√£o  
4. **API P√∫blica**: Endpoints para acesso externo aos dados
5. **Exporta√ß√£o**: Funcionalidades para download de relat√≥rios

### Escalabilidade

1. **Processamento Paralelo**: Multiple workers para scraping
2. **Cache Redis**: Cache de consultas frequentes
3. **CDN**: Distribui√ß√£o de screenshots via CDN
4. **Monitoring**: Prometheus + Grafana para m√©tricas
5. **Auto-scaling**: Ajuste autom√°tico de recursos baseado na carga



