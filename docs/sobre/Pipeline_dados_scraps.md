# Pipeline de Dados do Check-up

## Contexto do Projeto

O projeto **Check-up** tem como objetivo principal analisar a presenÃ§a de desinformaÃ§Ã£o em anÃºncios publicitÃ¡rios de saÃºde que circulam em grandes sites de notÃ­cias do Brasil. Este documento detalha o fluxo completo de dados desde a coleta inicial de URLs atÃ© a apresentaÃ§Ã£o final no frontend web.

## ğŸ”„ VisÃ£o Geral do Pipeline

O pipeline de dados do Check-up Ã© composto por **cinco etapas principais**:

1. **ğŸ•·ï¸ Crawling (Spiders)**: Coleta URLs de artigos dos portais de notÃ­cias
2. **ğŸ“° Scraping (Plays)**: Extrai conteÃºdo e anÃºncios das pÃ¡ginas coletadas  
3. **ğŸ—„ï¸ Armazenamento**: Salva dados no PostgreSQL e arquivos no MinIO
4. **ğŸ”— API Backend**: Serve dados via FastAPI
5. **ğŸ–¥ï¸ Frontend**: Interface React para visualizaÃ§Ã£o


![Diagrama do Pipeline](../assets/diagrama-pipeline.png)

## ğŸ“Š Estrutura de Dados

### Modelos Principais (PostgreSQL)

**Portal**: InformaÃ§Ãµes dos portais de notÃ­cias
```python
- id: Integer (Primary Key)
- name: String (Nome do portal)
- url: URL (URL base do portal)
- slug: String (Identificador Ãºnico)
- created_at: DateTime
```

**URLQueue**: Fila de URLs para processamento
```python
- id: Integer (Primary Key)  
- url: URL (URL do artigo)
- created_at: DateTime
- statuses: List[QueueStatus] (Status de processamento)
```

**Entry**: Artigos de notÃ­cias processados
```python
- id: Integer (Primary Key)
- portal_id: ForeignKey (Portal)
- title: String (TÃ­tulo do artigo)
- url: URL (URL do artigo)
- description: String (SubtÃ­tulo/resumo)
- body: Text (Corpo completo do artigo)
- tags: JSON (Tags/categorias)
- screenshot: String (Caminho no MinIO)
- created_at: DateTime
```

**Advertisement**: AnÃºncios encontrados nos artigos
```python
- id: Integer (Primary Key)
- entry_id: ForeignKey (Entry)
- url: URL (URL do anÃºncio)
- title: String (TÃ­tulo do anÃºncio)
- tag: String (Categoria do anÃºncio)
- thumbnail: URL (URL da thumbnail)
- screenshot: String (Caminho no MinIO)
- excerpt: String (DescriÃ§Ã£o)
- category: Integer (Categoria LLM)
- category_verbose: String (DescriÃ§Ã£o da categoria)
- created_at: DateTime
```




## ğŸ•·ï¸ ETAPA 1: Spiders - Coleta de URLs

### FunÃ§Ã£o dos Spiders

Os **spiders** sÃ£o componentes Scrapy responsÃ¡veis por navegar nas pÃ¡ginas iniciais dos portais de notÃ­cias e coletar URLs de artigos vÃ¡lidos. Eles constituem o ponto de entrada do pipeline, identificando e filtrando links que serÃ£o posteriormente processados.

### ğŸ”§ Como Funcionam

1. **NavegaÃ§Ã£o**: Acessam a pÃ¡gina inicial do portal (ex: `https://www.metropoles.com/`)
2. **ExtraÃ§Ã£o**: Identificam links de artigos usando seletores CSS/XPath
3. **Filtragem**: Aplicam regras para aceitar apenas URLs de notÃ­cias vÃ¡lidas
4. **Armazenamento**: Salvam URLs aprovadas na tabela `URLQueue`

### ğŸ“‹ Portais Suportados

**âœ… Funcionais:**
- MetrÃ³poles (`metropolesspider`)
- IG (`igspider`) 
- MaisGoiÃ¡s (`maisgoiasspider`)
- AliadosBrasil (`aliadosbrasilspider`)
- Veja (`vejaspider`)
- R7 (`r7spider`)
- UOL (`uolspider`)
- Folha (`folhaspider`)

### âš™ï¸ ConfiguraÃ§Ã£o dos Spiders

**Estrutura Base:**
```python
class MetropolesSpider(BaseSpider):
    name = "metropolesspider"
    start_urls = ["https://www.metropoles.com/"]
    allowed_domains = ["metropoles.com"]
    
    def allow_url(self, entry_url):
        # LÃ³gica de filtragem especÃ­fica do portal
        return True
    
    def parse(self, response):
        # Extrai URLs usando seletores CSS
        for entry in response.css(".noticia__titulo > a"):
            url = entry.attrib.get("href")
            if url and self.allow_url(url):
                yield URLItem(url=url)
```

**Pipeline de Processamento:**
```python
# pipelines.py
class PostgresPipeline:
    def process_item(self, item, spider):
        # Verifica se URL jÃ¡ existe
        exists = self.session.query(URLQueue).filter(
            URLQueue.url == item["url"]
        ).first()
        
        if exists is None:
            # Cria nova entrada na fila
            URLQueue.create(self.session, item["url"])
        
        return item
```

### ğŸš€ ExecuÃ§Ã£o

**Comandos individuais:**
```bash
make crawl_metropoles  # Coleta URLs do MetrÃ³poles
make crawl_ig          # Coleta URLs do IG
make crawl_veja        # Coleta URLs da Veja
```

**ExecuÃ§Ã£o em massa:**
```bash
make crawl_all_working # Todos os portais funcionais
```

**VerificaÃ§Ã£o:**
```sql
-- Visualizar URLs coletadas
SELECT COUNT(*) FROM urlqueue;
SELECT url FROM urlqueue ORDER BY created_at DESC LIMIT 10;
```




## ğŸ­ ETAPA 2: Plays - ExtraÃ§Ã£o de ConteÃºdo e AnÃºncios

### FunÃ§Ã£o dos Plays

Os **plays** sÃ£o componentes baseados em Playwright que acessam cada URL coletada pelos spiders e extraem tanto o conteÃºdo editorial quanto os anÃºncios presentes na pÃ¡gina. Eles constituem o coraÃ§Ã£o do sistema de anÃ¡lise, capturando dados estruturados para posterior classificaÃ§Ã£o.

### ğŸ”§ Como Funcionam

1. **SeleÃ§Ã£o de URLs**: Buscam URLs na `URLQueue` filtradas por domÃ­nio
2. **NavegaÃ§Ã£o**: Usam Playwright para abrir as pÃ¡ginas em um browser real
3. **ExtraÃ§Ã£o Dual**:
   - **ConteÃºdo Editorial**: tÃ­tulo, descriÃ§Ã£o, corpo, tags
   - **AnÃºncios**: tÃ­tulo, URL, thumbnail, categoria
4. **Armazenamento**: Salvam dados nas tabelas `Entry` e `Advertisement`
5. **Screenshots**: Capturam imagens das pÃ¡ginas no MinIO

### ğŸ“Š Estrutura dos Dados ExtraÃ­dos

**EntryItem (ConteÃºdo Editorial):**
```python
@dataclass
class EntryItem:
    title: str           # TÃ­tulo principal do artigo
    url: str            # URL do artigo  
    description: str    # SubtÃ­tulo/resumo
    body: str          # Corpo completo do texto
    tags: List[str]    # Tags/categorias
    ads: List[AdItem]  # AnÃºncios encontrados
    screenshot_path: str # Caminho da screenshot
```

**AdItem (AnÃºncios):**
```python
@dataclass  
class AdItem:
    title: str          # TÃ­tulo do anÃºncio
    url: str           # URL de destino
    thumbnail_url: str # URL da imagem
    tag: str          # Categoria/origem (ex: "Taboola")
    screenshot_path: str # Screenshot do anÃºncio
    excerpt: str      # DescriÃ§Ã£o/texto
```

### ğŸ¯ Exemplo de Play Funcional

**MetrÃ³poles Play:**
```python
class MetropolesPlay(BasePlay):
    name = "metropoles"
    
    @classmethod
    def match(cls, url):
        return "metropoles.com" in url
    
    def run(self) -> EntryItem:
        with sync_playwright() as p:
            browser = self.launch_browser(p)
            page = browser.new_page()
            page.goto(self.url, timeout=180_000)
            
            # ExtraÃ§Ã£o do conteÃºdo editorial
            entry_title = page.locator("//h1").first.inner_text()
            
            description = ""
            try:
                description = page.locator(".noticiaCabecalho__subtitulo").inner_text()
            except:
                pass
            
            # Corpo do artigo
            body_paragraphs = []
            selectors = [".m-content", "article", ".article-content"]
            for selector in selectors:
                try:
                    elements = page.locator(f"{selector} p").all()
                    body_paragraphs = [p.inner_text() for p in elements]
                    if body_paragraphs:
                        break
                except:
                    continue
            
            body = " ".join(body_paragraphs)
            
            # ExtraÃ§Ã£o de anÃºncios (Taboola)
            ad_items = []
            try:
                page.locator("#taboola-below-article-thumbnails").scroll_into_view_if_needed()
                time.sleep(3)
                
                ad_elements = page.locator(".videoCube").all()
                for element in ad_elements:
                    if element.is_visible():
                        content = element.inner_html()
                        ad_item = self.find_items(content)
                        if ad_item.is_valid():
                            ad_items.append(ad_item)
            except:
                pass
            
            return EntryItem(
                title=entry_title,
                description=description, 
                body=body,
                tags=[],
                ads=ad_items,
                url=self.url
            )
```

### ğŸ—„ï¸ Processo de Armazenamento

**1. SeleÃ§Ã£o de URLs para Processamento:**
```python
# scrape_no_openai.py
def get_urls_for_platform(session, domain):
    """Busca URLs da URLQueue filtradas por domÃ­nio"""
    return session.query(URLQueue).join(URLQueue.statuses).filter(
        URLQueue.url.like(f"%{domain}%"),
        QueueStatus.value == "created" 
    ).limit(100).all()
```

**2. Processamento e Armazenamento:**
```python
# Para cada URL processada:
1. Play extrai conteÃºdo â†’ EntryItem
2. Salva Entry no PostgreSQL
3. Para cada anÃºncio â†’ Advertisement no PostgreSQL  
4. Screenshots salvos no MinIO
5. Atualiza QueueStatus para "completed"
```

### ğŸš€ ExecuÃ§Ã£o

**Comandos por portal:**
```bash
make scrape_metropoles     # Scraping do MetrÃ³poles
make scrape_ig            # Scraping do IG  
make scrape_maisgoias     # Scraping do MaisGoiÃ¡s
```

**ExecuÃ§Ã£o em massa:**
```bash
make scrape_all_working   # Todos os portais funcionais
```

**Comando direto (com argumentos):**
```bash
docker compose exec scraper python scrape_no_openai.py --platform metropoles.com
```

### ğŸ“ˆ Monitoramento

**VerificaÃ§Ã£o no banco:**
```sql
-- Artigos processados
SELECT COUNT(*) FROM entry;

-- AnÃºncios encontrados  
SELECT COUNT(*) FROM advertisement;

-- Status por portal
SELECT 
    SUBSTRING(url FROM 'https://([^/]+)') as portal,
    COUNT(*) as total_urls
FROM urlqueue 
GROUP BY portal;
```

**VerificaÃ§Ã£o no MinIO:**
- Console: `http://localhost:9001`
- Credenciais: `minioadmin/minioadmin`
- Buckets organizados por portal: `metropoles_com/`, `ig_com_br/`, etc.




## ğŸ—„ï¸ ETAPA 3: Armazenamento de Dados

### PostgreSQL - Banco Principal

O Check-up utiliza PostgreSQL como banco principal para armazenar dados estruturados. O schema Ã© definido em `models.py` usando SQLAlchemy ORM.

**Fluxo de Armazenamento:**
```mermaid
graph TD
    A[Spider coleta URL] --> B[URLQueue - Fila de URLs]
    B --> C[Play processa URL]
    C --> D[Entry - Artigo principal]
    C --> E[Advertisement - AnÃºncios encontrados]
    D --> F[Portal - Info do portal]
    E --> F
```

**Relacionamentos:**
- `Portal` 1:N `Entry` (Um portal tem muitos artigos)
- `Entry` 1:N `Advertisement` (Um artigo tem muitos anÃºncios)
- `URLQueue` 1:N `QueueStatus` (Uma URL tem histÃ³rico de status)

### MinIO - Armazenamento de Arquivos

O MinIO Ã© usado para armazenar screenshots e mÃ­dias capturadas durante o scraping.

**Estrutura de Buckets:**
```
scraped-articles/
â”œâ”€â”€ metropoles_com/
â”‚   â”œâ”€â”€ 2025/01/07/
â”‚   â”‚   â”œâ”€â”€ entry_screenshots/
â”‚   â”‚   â””â”€â”€ ad_screenshots/
â”œâ”€â”€ ig_com_br/
â”‚   â”œâ”€â”€ 2025/01/07/
â”‚   â”‚   â”œâ”€â”€ entry_screenshots/
â”‚   â”‚   â””â”€â”€ ad_screenshots/
```

**Processo de Upload:**
```python
# storage.py
def upload_file(local_path, minio_path):
    """Upload de arquivo para MinIO"""
    client.fput_object(
        bucket_name="scraped-articles",
        object_name=minio_path,
        file_path=local_path
    )
```

### ğŸ”„ Status de Processamento

O sistema mantÃ©m controle detalhado do status de cada URL atravÃ©s da tabela `QueueStatus`:

**Estados possÃ­veis:**
- `created`: URL coletada, aguardando processamento
- `processing`: URL sendo processada por um play
- `completed`: Processamento concluÃ­do com sucesso
- `failed`: Erro durante processamento
- `retry`: Agendada para nova tentativa

**Exemplo de consulta de status:**
```sql
SELECT 
    u.url,
    qs.value as status,
    qs.created_at
FROM urlqueue u
JOIN queuestatus qs ON u.id = qs.url_queue_id
WHERE qs.value = 'completed'
ORDER BY qs.created_at DESC;
```

## ğŸ”— ETAPA 4: API Backend (FastAPI)

### Estrutura da API

O backend em FastAPI (`web/server/main.py`) serve como ponte entre os dados armazenados e o frontend, fornecendo endpoints RESTful para acesso aos dados coletados.

**Endpoints Principais:**

**1. Listar Portais:**
```python
@app.get("/portais", response_model=List[str])
def listar_portais():
    """Retorna lista de portais com dados disponÃ­veis"""
    # Busca diretamente nos buckets do MinIO
    portais = set()
    for obj in minio_client.list_objects(MINIO_BUCKET, recursive=True):
        portal = obj.object_name.split('/')[0]
        portais.add(portal)
    return sorted(list(portais))
```

**2. Buscar NotÃ­cias:**
```python
@app.get("/noticias/{portal}")
def buscar_noticias(
    portal: str,
    q: str = Query(None, description="Termo de busca"),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=1, le=100)
):
    """Busca notÃ­cias de um portal especÃ­fico com paginaÃ§Ã£o"""
    # Busca arquivos JSON no MinIO do portal especificado
    # Aplica filtros de busca se fornecidos
    # Retorna dados paginados
```

**3. Detalhes da NotÃ­cia:**
```python
@app.get("/noticia/{portal}/{filename}")
def obter_noticia(portal: str, filename: str):
    """Retorna dados completos de uma notÃ­cia especÃ­fica"""
    # Busca arquivo JSON especÃ­fico no MinIO
    # Retorna conteÃºdo completo incluindo anÃºncios
```

### ğŸ”„ Fluxo de Dados da API

```mermaid
graph TD
    A[Frontend Request] --> B[FastAPI Endpoint]
    B --> C[Consulta MinIO]
    C --> D[Processa Dados JSON]
    D --> E[Aplica Filtros/PaginaÃ§Ã£o]
    E --> F[Retorna Response]
    F --> G[Frontend Atualiza UI]
```

### ğŸ“Š Formato de Resposta

**Estrutura de NotÃ­cia:**
```json
{
  "title": "TÃ­tulo da notÃ­cia",
  "url": "https://portal.com/noticia",
  "description": "SubtÃ­tulo ou resumo",
  "body": "Corpo completo da notÃ­cia...",
  "tags": ["polÃ­tica", "saÃºde"],
  "timestamp": "2025-01-07T10:30:00Z",
  "portal": "metropoles_com",
  "ads": [
    {
      "title": "AnÃºncio de produto",
      "url": "https://anuncio.com",
      "tag": "Taboola",
      "thumbnail": "https://thumb.com/img.jpg"
    }
  ]
}
```

## ğŸ–¥ï¸ ETAPA 5: Frontend (React)

### Arquitetura do Frontend

O frontend em React (`web/client/src/`) oferece interface intuitiva para explorar os dados coletados, com foco na visualizaÃ§Ã£o de notÃ­cias e anÃºncios de saÃºde.

**Componentes Principais:**

**1. Home.tsx - PÃ¡gina Principal:**
```tsx
const Home = () => {
  const [portais, setPortais] = useState<string[]>([]);
  const [noticias, setNoticias] = useState<any[]>([]);
  const [portalSelecionado, setPortalSelecionado] = useState<string | null>(null);
  
  // Busca lista de portais disponÃ­veis
  useEffect(() => {
    fetch(`${API_URL}/portais`)
      .then(res => res.json())
      .then(setPortais);
  }, []);
  
  // Busca notÃ­cias quando portal Ã© selecionado
  useEffect(() => {
    if (portalSelecionado) {
      fetch(`${API_URL}/noticias/${portalSelecionado}`)
        .then(res => res.json())
        .then(setNoticias);
    }
  }, [portalSelecionado]);
}
```

**2. Funcionalidades Implementadas:**

- **SeleÃ§Ã£o de Portal**: Tabs para escolher entre portais disponÃ­veis
- **Busca**: Campo de texto para filtrar notÃ­cias por tÃ­tulo/conteÃºdo
- **PaginaÃ§Ã£o**: NavegaÃ§Ã£o atravÃ©s de grandes volumes de dados
- **VisualizaÃ§Ã£o Detalhada**: Modal com conteÃºdo completo da notÃ­cia
- **EstatÃ­sticas**: Contadores de notÃ­cias por portal

### ğŸ¨ Interface do UsuÃ¡rio

**Layout Principal:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check-up - AnÃ¡lise de AnÃºncios de SaÃºde â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [MetrÃ³poles] [IG] [Veja] [R7] [UOL]...  â”‚ <- Tabs dos portais
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ” [___________________] [Buscar]       â”‚ <- Campo de busca
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ“° TÃ­tulo da NotÃ­cia               â”‚ â”‚
â”‚ â”‚ ğŸ·ï¸  Portal: MetrÃ³poles             â”‚ â”‚ <- Cards de notÃ­cias
â”‚ â”‚ ğŸ“… 07/01/2025 - ğŸ”— 2 anÃºncios      â”‚ â”‚
â”‚ â”‚ [Ver Detalhes]                     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â† 1 2 3 ... 10 â†’                       â”‚ <- PaginaÃ§Ã£o
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Fluxo de InteraÃ§Ã£o

1. **Carregamento Inicial**: Busca lista de portais disponÃ­veis
2. **SeleÃ§Ã£o de Portal**: UsuÃ¡rio clica em tab do portal desejado
3. **ExibiÃ§Ã£o de NotÃ­cias**: Lista paginada de notÃ­cias do portal
4. **Busca**: UsuÃ¡rio pode filtrar por termos especÃ­ficos
5. **Detalhamento**: Modal mostra conteÃºdo completo + anÃºncios
6. **NavegaÃ§Ã£o**: PaginaÃ§Ã£o para explorar mais resultados

### ğŸ“± Recursos de UX

- **Loading States**: Skeletons durante carregamento
- **Error Handling**: Mensagens de erro amigÃ¡veis  
- **Responsive Design**: AdaptÃ¡vel a diferentes tamanhos de tela
- **Busca Reativa**: Resultados atualizados em tempo real
- **Indicadores Visuais**: Badges para nÃºmero de anÃºncios

## ğŸ”„ FLUXO COMPLETO DE DADOS

### Pipeline End-to-End

```mermaid
graph TD
    A[Portal de NotÃ­cias] --> B[Spider - Scrapy]
    B --> C[URLQueue - PostgreSQL]
    C --> D[Play - Playwright]
    D --> E[Entry - PostgreSQL]
    D --> F[Advertisement - PostgreSQL]
    D --> G[Screenshots - MinIO]
    E --> H[FastAPI Backend]
    F --> H
    G --> H
    H --> I[React Frontend]
    I --> J[Interface do UsuÃ¡rio]
    
    style A fill:#e1f5fe
    style J fill:#e8f5e8
    style H fill:#fff3e0
```

### â±ï¸ Cronograma de ExecuÃ§Ã£o

**1. ConfiguraÃ§Ã£o (Ãºnica vez):**
```bash
make setup  # ~5-10 minutos
```

**2. Coleta de URLs (diÃ¡ria):**
```bash
make crawl_all_working  # ~10-15 minutos
# Resultado: ~500-1000 URLs novas por portal
```

**3. ExtraÃ§Ã£o de ConteÃºdo (contÃ­nua):**
```bash
make scrape_all_working  # ~2-4 horas
# Resultado: ~100-200 artigos processados por hora
```

**4. VisualizaÃ§Ã£o (imediata):**
```bash
# Frontend disponÃ­vel em: http://localhost:5173
# API disponÃ­vel em: http://localhost:8000
```

## ğŸš€ Comandos de ExecuÃ§Ã£o

### Setup Inicial
```bash
# ConfiguraÃ§Ã£o completa do ambiente
make setup

# Verificar se serviÃ§os estÃ£o rodando
docker compose ps

# Acessar logs em tempo real
docker compose logs -f
```

### ExecuÃ§Ã£o do Pipeline

**1. Coleta de URLs (Crawling):**
```bash
# Todos os portais funcionais
make crawl_all_working

# Portais individuais
make crawl_metropoles
make crawl_ig
make crawl_veja
make crawl_uol
```

**2. ExtraÃ§Ã£o de ConteÃºdo (Scraping):**
```bash
# Todos os portais funcionais  
make scrape_all_working

# Portais individuais
make scrape_metropoles
make scrape_ig
make scrape_maisgoias
```

**3. Pipeline Completo:**
```bash
# Executa crawl + scraping sequencialmente
make pipeline_complete

# Workflow otimizado
make collect_working
```

### Monitoramento e Debugging

**Acesso ao container:**
```bash
make bash
```

**VerificaÃ§Ãµes no banco:**
```bash
# Dentro do container
python -c "
from models import *
from sqlalchemy import create_engine
from sqlalchemy.orm import Session
from decouple import config

engine = create_engine(config('DATABASE_URL'))
session = Session(engine)

print('URLs na fila:', URLQueue.select().count())
print('Artigos processados:', Entry.select().count()) 
print('AnÃºncios encontrados:', Advertisement.select().count())
"
```

**VerificaÃ§Ã£o do MinIO:**
- Interface: `http://localhost:9001`
- Credenciais: `minioadmin/minioadmin`

**Frontend e API:**
- Frontend: `http://localhost:5173`
- API: `http://localhost:8000`
- Docs da API: `http://localhost:8000/docs`

### Troubleshooting

**Problema: Playwright browsers nÃ£o instalados**
```bash
docker compose down
docker compose build --no-cache
make start
```

**Problema: Banco nÃ£o conecta**
```bash
make wait-for-db
# ou
make stop && make start
```

**Problema: MinIO nÃ£o acessÃ­vel**
```bash
docker compose ps
# Verificar se container healthcheck_minio estÃ¡ "healthy"
```

## ğŸ”§ ConfiguraÃ§Ã£o e CustomizaÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```bash
# Banco de dados
DATABASE_URL=postgresql://postgres:supersecret@healthcheck_db:5432/healthcheck

# MinIO
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_SECURE=False

# Opcional: OpenAI para classificaÃ§Ã£o
OPENAI_API_KEY=sk-...

# Credenciais de portais (se necessÃ¡rio)
FOLHA_USERNAME=usuario
FOLHA_PASSWORD=senha
```

### Adicionando Novo Portal

**1. Criar Spider:**
```python
# spiders/novoportal.py
class NovoPortalSpider(BaseSpider):
    name = "novoportalspider"
    start_urls = ["https://novoportal.com/"]
    # ... implementaÃ§Ã£o especÃ­fica
```

**2. Criar Play:**
```python  
# plays/novoportal.py
class NovoPortalPlay(BasePlay):
    name = "novoportal"
    
    @classmethod
    def match(cls, url):
        return "novoportal.com" in url
    # ... implementaÃ§Ã£o especÃ­fica
```

**3. Adicionar comandos no Makefile:**
```makefile
crawl_novoportal:
	docker compose run scraper python crawl.py novoportalspider

scrape_novoportal:
	docker compose exec scraper python scrape_no_openai.py --platform novoportal.com
```

**4. Testar:**
```bash
make crawl_novoportal
make scrape_novoportal
```

## ğŸ“ˆ MÃ©tricas e AnÃ¡lises

### Consultas Ãšteis

**Resumo geral:**
```sql
SELECT 
    COUNT(DISTINCT p.name) as total_portais,
    COUNT(DISTINCT e.id) as total_artigos,
    COUNT(DISTINCT a.id) as total_anuncios,
    AVG(ads_por_artigo.count) as media_anuncios_por_artigo
FROM portal p
LEFT JOIN entry e ON p.id = e.portal_id
LEFT JOIN advertisement a ON e.id = a.entry_id
LEFT JOIN (
    SELECT entry_id, COUNT(*) as count 
    FROM advertisement 
    GROUP BY entry_id
) ads_por_artigo ON e.id = ads_por_artigo.entry_id;
```

**Top portais por volume:**
```sql
SELECT 
    p.name,
    COUNT(e.id) as artigos,
    COUNT(a.id) as anuncios
FROM portal p
LEFT JOIN entry e ON p.id = e.portal_id  
LEFT JOIN advertisement a ON e.id = a.entry_id
GROUP BY p.name
ORDER BY artigos DESC;
```

**EvoluÃ§Ã£o temporal:**
```sql
SELECT 
    DATE(e.created_at) as data,
    COUNT(e.id) as artigos_processados,
    COUNT(a.id) as anuncios_encontrados
FROM entry e
LEFT JOIN advertisement a ON e.id = a.entry_id
WHERE e.created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY DATE(e.created_at)
ORDER BY data DESC;
```

## ğŸ¯ PrÃ³ximos Passos

### Melhorias Planejadas

1. **ClassificaÃ§Ã£o AutomÃ¡tica**: IntegraÃ§Ã£o com LLM para categorizaÃ§Ã£o de anÃºncios
2. **Dashboard Analytics**: Painel com mÃ©tricas em tempo real
3. **Alertas**: NotificaÃ§Ãµes para anÃºncios suspeitos de desinformaÃ§Ã£o  
4. **API PÃºblica**: Endpoints para acesso externo aos dados
5. **ExportaÃ§Ã£o**: Funcionalidades para download de relatÃ³rios

### Escalabilidade

1. **Processamento Paralelo**: Multiple workers para scraping
2. **Cache Redis**: Cache de consultas frequentes
3. **CDN**: DistribuiÃ§Ã£o de screenshots via CDN
4. **Monitoring**: Prometheus + Grafana para mÃ©tricas
5. **Auto-scaling**: Ajuste automÃ¡tico de recursos baseado na carga



