# Fa√ßa sua raspagem de not√≠cias em sites de noticias com Playwright e Scrapy! 
> Tutorial de como criar ou alterar spiders e plays para coletar e extrair conte√∫do de not√≠cias.

## Contexto do Projeto

O projeto foi modificado para focar na **extra√ß√£o de conte√∫do de not√≠cias** em vez de an√∫ncios. O objetivo agora √© extrair o texto das not√≠cias para posterior an√°lise de fake news por uma LLM.

### üîÑ **Campos que PRECISAM ser revisados:**
- **T√≠tulo**: Seletores podem ter mudado
- **Corpo**: Estrutura de conte√∫do pode ter mudado  
- **Description**: NOVO campo - precisa ser identificado
- **Tags**: NOVO campo - precisa ser identificado

### ‚úÖ **Processo de atualiza√ß√£o:**
1. **RE-INSPECIONAR** cada portal no navegador atual
2. **COMPARAR** seletores antigos vs estrutura atual
3. **IDENTIFICAR** novos seletores para description e tags
4. **TESTAR** extra√ß√£o com URLs reais atuais

## üï∑Ô∏è SPIDERS - Coleta de URLs

### Fun√ß√£o dos Spiders
Os spiders s√£o respons√°veis por navegar na p√°gina inicial dos portais de not√≠cia e coletar URLs de artigos/not√≠cias v√°lidos.

### ‚ö†Ô∏è Altera√ß√µes Necess√°rias nos Spiders Existentes

#### 1. Melhorar Filtragem de URLs
Muitos spiders n√£o filtram adequadamente os URLs. Use o **gazetaDoPovo** como refer√™ncia de boa filtragem:

**‚ùå Exemplo de filtragem ruim (r7.py):**
```python
def allow_url(self, entry_url):
    return (
        entry_url.startswith("https://")
        and len(entry_url) > 100
        and re.match(
            r"https://(entretenimento|esportes|record|noticias)\.r7\.com", entry_url
        )
    )
```

**‚úÖ Exemplo de filtragem boa (gazetaDoPovo.py):**
```python
def allow_url(self, url: str) -> bool:
    p = urlparse(url)
    path = p.path.rstrip('/')

    # 1) blacklist pure sections
    if path in {"/videos", "/vozes", "/podcasts", "/newsletter", "/ebooks"}:
        self.logger.info(f"Blacklisted URL: {url}")
        return False

    # 2) require at least two non-empty segments (section + slug)
    segments = [seg for seg in path.split('/') if seg]
    if len(segments) < 2:
        self.logger.info(f"Blacklisted URL: {url}")
        return False

    slug = segments[-1]
    # 3a) long slugs by hyphens or 3b) by character length
    if slug.count('-') >= 3 or len(slug) > 30:
        return True

    return False
```

#### 2. Remover yield scrapy.Request Recursivos
**Queremos apenas as not√≠cias da p√°gina inicial**, ent√£o remova os `yield scrapy.Request` que fazem crawling recursivo:

**‚ùå Remover:**
```python
def parse(self, response):
    seen = set()
    for entry in response.css("a"):
        url = entry.attrib.get("href")
        if url and url not in seen and self.allow_url(url):
            seen.add(url)
            yield URLItem(url=url)
            yield scrapy.Request(url=url, callback=self.parse)  # ‚Üê REMOVER ESTA LINHA
```

**‚úÖ Manter apenas:**
```python
def parse(self, response):
    seen = set()
    for entry in response.css("a"):
        url = entry.attrib.get("href")
        if url and url not in seen and self.allow_url(url):
            seen.add(url)
            yield URLItem(url=url)  # ‚Üê MANTER APENAS ISTO
```

### üÜï Criando um Novo Spider

**Estrutura base para um novo spider:**

```python
import scrapy
from spiders.base import BaseSpider
from spiders.items import URLItem
from urllib.parse import urlparse

class NovoPortalSpider(BaseSpider):
    name = "novoportalspider"
    start_urls = ["https://www.novoportal.com.br/"]
    allowed_domains = ["novoportal.com.br"]

    custom_settings = {
        **BaseSpider.custom_settings,
        "COOKIES_ENABLED": True,
        "DOWNLOAD_DELAY": 3,
        "DEFAULT_REQUEST_HEADERS": {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Cache-Control": "max-age=0",
        }
    }

    def allow_url(self, url: str) -> bool:
        """
        Implementar l√≥gica espec√≠fica do portal para filtrar apenas URLs de not√≠cias
        """
        p = urlparse(url)
        path = p.path.rstrip('/')
        
        # Exemplo de filtragem - adaptar para cada portal
        segments = [seg for seg in path.split('/') if seg]
        if len(segments) < 2:
            return False
            
        # Verificar se √© uma not√≠cia real (slug longo, m√∫ltiplos h√≠fens, etc.)
        slug = segments[-1]
        return slug.count('-') >= 2 or len(slug) > 20

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(
                url,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_redirect": True, "handle_httpstatus_list": [403]},
            )

    def parse(self, response):
        # Adaptar seletor CSS/XPath para o portal espec√≠fico
        links = response.css("a")  # ou response.xpath("//a")
        
        for link in links:
            url = link.attrib.get("href")
            if not url:
                continue
                
            # Construir URL absoluta
            full_url = response.urljoin(url)
            full_url = full_url.split('#', 1)[0].split('?', 1)[0]  # Remove fragmentos e query params
            
            if self.allow_url(full_url):
                yield URLItem(url=full_url)
```

## üé≠ PLAYS - Extra√ß√£o de Conte√∫do

### Fun√ß√£o dos Plays
Os plays s√£o respons√°veis por acessar cada URL de not√≠cia e extrair o conte√∫do: **t√≠tulo, descri√ß√£o, corpo da mat√©ria e tags**.

### ‚ö†Ô∏è Altera√ß√µes Necess√°rias nos Plays Existentes

#### 1. Remover take_screenshot
**N√£o precisamos mais de screenshots**, apenas do texto. Remover todas as chamadas para `take_screenshot`:

**‚ùå Remover:**
```python
entry_screenshot_path = self.take_screenshot(page, self.url, goto=False)

# E na cria√ß√£o do EntryItem:
return EntryItem(
    title=entry_title,
    ads=ad_items,
    url=self.url,
    screenshot_path=entry_screenshot_path,  # ‚Üê REMOVER
)
```

#### 2. Atualizar TODOS os Seletores
**‚ö†Ô∏è IMPORTANTE**: Portais de not√≠cia frequentemente alteram sua estrutura HTML. **TODOS os seletores precisam ser revisados e atualizados**:

- **Seletores existentes** (t√≠tulo, corpo) podem estar desatualizados
- **Novos seletores** s√£o necess√°rios para description e tags

#### 3. Focar na Extra√ß√£o de Conte√∫do
**Remover todo c√≥digo relacionado a an√∫ncios** e focar na extra√ß√£o de:
- **title**: T√≠tulo da not√≠cia *(REVISAR seletores existentes)*
- **description**: Subt√≠tulo/resumo da not√≠cia *(NOVO campo - identificar seletores)*
- **body**: Corpo completo da not√≠cia *(REVISAR seletores existentes)*
- **tags**: Tags/categorias da not√≠cia *(NOVO campo - identificar seletores)*

#### 4. Remover C√≥digo de An√∫ncios (opcional)
**Remover:**
- M√©todos `find_items`
- Vari√°veis `n_expected_ads`
- Locators para elementos de an√∫ncio (`.videoCube`, `#taboola-*`, etc.)

### üÜï Criando um Novo Play

**Estrutura base para um novo play:**

```python
import time
from playwright.sync_api import sync_playwright
from plays.base import BasePlay
from plays.items import EntryItem
from plog import logger

class NovoPortalPlay(BasePlay):
    name = "novoportal"

    @classmethod
    def match(cls, url):
        return "novoportal.com.br" in url

    def pre_run(self):
        pass

    def run(self) -> EntryItem:
        with sync_playwright() as p:
            browser = self.launch_browser(p, viewport={"width": 1920, "height": 1080})
            page = browser.new_page()
            logger.info(f"[{self.name}] Opening URL '{self.url}'...")
            page.goto(self.url, timeout=180_000)
            
            # Aguardar carregamento do conte√∫do principal
            page.wait_for_selector("h1", timeout=30000)
            
            # 1. EXTRAIR T√çTULO
            entry_title = ""
            try:
                # Adaptar seletor para o portal espec√≠fico
                entry_title = page.locator("h1").first.inner_text()
            except Exception as e:
                logger.warning(f"[{self.name}] Failed to extract title: {str(e)}")
            
            # 2. EXTRAIR DESCRI√á√ÉO (quando dispon√≠vel)
            description = ""
            try:
                # Exemplos de seletores comuns para descri√ß√£o/subt√≠tulo
                selectors = [
                    ".subtitle",
                    ".description", 
                    ".lead",
                    ".summary",
                    "h2",
                    ".article-subtitle"
                ]
                
                for selector in selectors:
                    try:
                        desc_element = page.locator(selector)
                        if desc_element.count() > 0:
                            description = desc_element.inner_text()
                            if description.strip():
                                break
                    except Exception:
                        continue
                        
            except Exception as e:
                logger.warning(f"[{self.name}] Failed to extract description: {str(e)}")
            
            # 3. EXTRAIR CORPO DA NOT√çCIA
            body = ""
            try:
                # Exemplos de seletores comuns para o corpo da mat√©ria
                selectors = [
                    ".article-content",
                    ".post-content", 
                    ".entry-content",
                    ".content",
                    "article",
                    ".text",
                    ".article-body"
                ]
                
                for selector in selectors:
                    try:
                        content_element = page.locator(selector)
                        if content_element.count() > 0:
                            body = content_element.inner_text()
                            if body.strip():
                                logger.info(f"[{self.name}] Successfully extracted body using: {selector}")
                                break
                    except Exception:
                        continue
                        
                if not body.strip():
                    logger.warning(f"[{self.name}] Failed to extract article body")
                    
            except Exception as e:
                logger.warning(f"[{self.name}] Failed to extract article body: {str(e)}")
            
            # 4. EXTRAIR TAGS (quando dispon√≠vel)
            tags = []
            try:
                # Exemplos de seletores comuns para tags
                tag_selectors = [
                    ".tags a",
                    ".categories a", 
                    ".post-tags a",
                    ".article-tags a",
                    "[rel='tag']"
                ]
                
                for selector in tag_selectors:
                    try:
                        tag_elements = page.locator(selector)
                        if tag_elements.count() > 0:
                            for i in range(tag_elements.count()):
                                tag_text = tag_elements.nth(i).inner_text().strip()
                                if tag_text:
                                    tags.append(tag_text)
                            if tags:
                                break
                    except Exception:
                        continue
                        
            except Exception as e:
                logger.warning(f"[{self.name}] Failed to extract tags: {str(e)}")

            return EntryItem(
                title=entry_title,
                url=self.url,
                description=description,
                body=body,
                tags=tags,
            )
```

## üìã Exemplos de Refer√™ncia

### ‚úÖ Bons Exemplos para Seguir:

1. **gazetaDoPovo.py** (spider + play) - Boa filtragem de URLs e extra√ß√£o de conte√∫do
2. **metropoles.py** (play) - Extra√ß√£o completa com t√≠tulo, descri√ß√£o, corpo e tags

### ‚ùå Exemplos que Precisam ser Atualizados:

Todos os outros plays que ainda usam:
- `take_screenshot`
- C√≥digo para extra√ß√£o de an√∫ncios (opcional)
- `yield scrapy.Request` recursivo nos spiders

## üîç Dicas para Desenvolver

### Para Spiders:
1. **Inspecionar a p√°gina inicial** do portal no navegador
2. **Identificar os links de not√≠cias** (geralmente t√™m URLs com slugs longos)
3. **Criar filtros espec√≠ficos** para separar not√≠cias de outras p√°ginas
4. **Testar a filtragem** para garantir que s√≥ coleta URLs v√°lidos

### Para Plays:
1. **‚ö†Ô∏è SEMPRE inspecionar NOVAMENTE** cada portal - estruturas HTML mudam frequentemente
2. **Identificar seletores para TODOS os 4 campos**:
   - **T√≠tulo**: Pode ter mudado desde a √∫ltima vers√£o
   - **Descri√ß√£o**: NOVO campo - encontrar subt√≠tulo/resumo
   - **Corpo**: Pode ter mudado desde a √∫ltima vers√£o  
   - **Tags**: NOVO campo - encontrar categorias/etiquetas
3. **Testar m√∫ltiplos seletores** como fallback (sites podem ter varia√ß√µes)
4. **Validar na p√°gina real** - n√£o confiar apenas no c√≥digo antigo

### üõ†Ô∏è Processo de Identifica√ß√£o de Seletores:

#### 1. Para campos EXISTENTES (t√≠tulo, corpo):
- **N√£o assumir** que seletores antigos ainda funcionam
- **Re-inspecionar** elementos na p√°gina atual
- **Comparar** seletores antigos vs novos

#### 2. Para campos NOVOS (description, tags):
- **Procurar** por subt√≠tulos, resumos, leads
- **Identificar** se√ß√µes de tags, categorias, etiquetas
- **Testar** se elementos existem em diferentes tipos de not√≠cia

### Ferramentas √öteis:
- **Inspetor do navegador** (F12) para identificar seletores atualizados
- **Console do navegador** para testar seletores CSS em tempo real

## üöÄ Pr√≥ximos Passos

1. **Atualizar spiders existentes** removendo `yield scrapy.Request` recursivo
2. **Melhorar filtragem de URLs** seguindo o padr√£o do gazetaDoPovo
3. **‚ö†Ô∏è RE-INSPECIONAR todos os portais** - estruturas HTML podem ter mudado
4. **Atualizar TODOS os seletores existentes** (t√≠tulo, corpo) - n√£o assumir que ainda funcionam
5. **Identificar seletores para campos novos** (description, tags)
6. **Atualizar plays existentes** removendo `take_screenshot` e c√≥digo de an√∫ncios
7. **Implementar extra√ß√£o completa** (t√≠tulo, descri√ß√£o, corpo, tags)
8. **Testar cada portal individualmente** com URLs reais atuais
9. **Validar extra√ß√£o** - verificar se todos os 4 campos s√£o capturados corretamente

## üìù Checklist de Valida√ß√£o

### Spider:
- [ ] Remove `yield scrapy.Request` recursivo
- [ ] Implementa `allow_url()` com boa filtragem
- [ ] Coleta apenas URLs da p√°gina inicial
- [ ] URLs coletados s√£o realmente de not√≠cias

### Play:
- [ ] **RE-INSPECIONA** o portal no navegador atual
- [ ] **ATUALIZA seletores existentes** (t√≠tulo, corpo) - n√£o usar c√≥digo antigo cegamente
- [ ] **IDENTIFICA seletores novos** para description e tags
- [ ] Remove `take_screenshot()` e vari√°veis relacionadas
- [ ] **TESTA extra√ß√£o** de t√≠tulo com seletores atualizados
- [ ] **IMPLEMENTA extra√ß√£o** de descri√ß√£o (NOVO campo)
- [ ] **TESTA extra√ß√£o** de corpo com seletores atualizados
- [ ] **IMPLEMENTA extra√ß√£o** de tags (NOVO campo)
- [ ] **VALIDA** que todos os 4 campos s√£o extra√≠dos corretamente
- [ ] Retorna `EntryItem` com os dados corretos
- [ ] **TESTA com URLs reais** do portal atual 