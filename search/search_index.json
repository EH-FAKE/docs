{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#eh-fake","title":"Eh fake","text":"<p>Bem vindo ao Eh fake!</p> <p>Se voc\u00ea n\u00e3o foi redirecionado automaticamente, clique aqui.</p>"},{"location":"atas/ata-1/","title":"ATA de Reuni\u00e3o \u2013 22/04/2025","text":"<p>Data: 22 de abril de 2025 Hor\u00e1rio: 20h \u00e0s 21h15 Plataforma: app.gather.town Pr\u00f3xima reuni\u00e3o agendada: 29 de abril de 2025 Respons\u00e1vel pelo registro: Let\u00edcia Paiva</p>"},{"location":"atas/ata-1/#1-participantes-presentes","title":"1. Participantes Presentes","text":"<ul> <li>Amanda Campos</li> <li>Fause Carlos</li> <li>Raissa Andrade</li> <li>Luis Miranda</li> <li>Vinicius Vieira</li> <li>Vinicius Martins</li> <li>Tiago Lima</li> <li>Let\u00edcia Paiva</li> <li>Philipe Barbosa</li> </ul>"},{"location":"atas/ata-1/#2-pautas-da-reuniao","title":"2. Pautas da Reuni\u00e3o","text":"<ul> <li>Apresenta\u00e7\u00e3o do projeto para novo integrante.</li> <li>Passagem e valida\u00e7\u00e3o parcial do checklist de release.</li> </ul> <p>Checklist de refer\u00eancia: Checklist de Disciplina</p>"},{"location":"atas/ata-1/#3-decisoes-tomadas","title":"3. Decis\u00f5es Tomadas","text":"<ul> <li>Defini\u00e7\u00e3o de pap\u00e9is:<ul> <li>Community Manager: Amanda</li> <li>Mantenedores: Fause, Luis</li> <li>Release Managers: Vinicius Vieira, Tiago</li> <li>Docs Leads: Raissa, Vinicius Martins</li> <li>Registro de Atas: Let\u00edcia, Philipe</li> </ul> </li> </ul>"},{"location":"atas/ata-1/#4-itens-finalizados-do-checklist","title":"4. Itens Finalizados do Checklist","text":"<ul> <li>Reposit\u00f3rio p\u00fablico no GitHub/GitLab com hist\u00f3rico limpo e organizado.</li> <li>Uso de git-flow ou estrat\u00e9gia similar para controle de branches.</li> <li>Aplica\u00e7\u00e3o do versionamento sem\u00e2ntico (SemVer).</li> <li>Documenta\u00e7\u00e3o inicial conclu\u00edda (README, tecnologias, instala\u00e7\u00e3o).</li> <li>Contribui\u00e7\u00e3o e conduta documentadas (CONTRIBUTING.md e CODE_OF_CONDUCT.md).</li> <li>Cria\u00e7\u00e3o dos \u00e9picos, features e hist\u00f3rias de usu\u00e1rio.</li> <li>Desenvolvimento do roadmap.</li> </ul>"},{"location":"atas/ata-1/#5-proximas-etapas","title":"5. Pr\u00f3ximas Etapas","text":"<ul> <li>Designa\u00e7\u00e3o e in\u00edcio do desenvolvimento do front-end b\u00e1sico.</li> <li>Estudo aprofundado das tecnologias utilizadas.</li> <li>Continua\u00e7\u00e3o da valida\u00e7\u00e3o do checklist.</li> </ul>"},{"location":"atas/ata-2/","title":"ATA de Reuni\u00e3o \u2013 27/04/2025","text":"<p>Data: 27 de abril de 2025 Hor\u00e1rio: 18h \u00e0s 23h Plataforma: Discord Pr\u00f3xima reuni\u00e3o agendada: 29 de abril de 2025 Respons\u00e1vel pelo registro: Amanda Campos</p>"},{"location":"atas/ata-2/#1-participantes-presentes","title":"1. Participantes Presentes","text":"<ul> <li>Amanda Campos</li> <li>Fause Carlos</li> <li>Raissa Andrade</li> <li>Luis Miranda</li> <li>Vinicius Vieira</li> <li>Vinicius Martins</li> <li>Philipe Barbosa</li> </ul>"},{"location":"atas/ata-2/#2-pautas-da-reuniao","title":"2. Pautas da Reuni\u00e3o","text":"<ul> <li>Rodar localmente o RAGflow.</li> <li>Finalizar Dockerfile e docker-compose da aplica\u00e7\u00e3o.</li> <li>Iniciar a popula\u00e7\u00e3o do banco vetorial.</li> </ul>"},{"location":"atas/ata-2/#3-problemas-enfrentados","title":"3. Problemas Enfrentados","text":"<ul> <li>Servidor gratuito: AWS EC2 e Google Cloud apresentaram limita\u00e7\u00f5es de recursos.</li> <li>Infraestrutura LAPPIS: Falta de acesso ao servidor institucional.</li> <li>Armazenamento: Espa\u00e7o limitado para subir o modelo Maritaca (Sabi\u00e1).</li> </ul>"},{"location":"atas/ata-2/#4-conclusoes","title":"4. Conclus\u00f5es","text":"<ul> <li>Execu\u00e7\u00e3o do RAGflow em m\u00e1quina pessoal (Raissa Andrade).</li> <li>Finaliza\u00e7\u00e3o do Dockerfile e das configura\u00e7\u00f5es de ambiente.</li> <li>Atualiza\u00e7\u00e3o dos documentos:<ul> <li>README.md</li> </ul> </li> </ul>"},{"location":"atas/ata-4/","title":"Ata 4 \u2013 29/04/2025","text":""},{"location":"atas/ata-4/#informacoes-gerais","title":"Informa\u00e7\u00f5es Gerais","text":"<p>Data: 29 de abril de 2025 Hor\u00e1rio: 20h \u00e0s 20h40 Plataforma: app.gather.town Pr\u00f3xima reuni\u00e3o: 06 de maio de 2025 Respons\u00e1vel pelo registro: Let\u00edcia Paiva</p>"},{"location":"atas/ata-4/#participantes","title":"Participantes","text":"<ul> <li>Amanda Campos  </li> <li>Fause Carlos  </li> <li>Raissa Andrade  </li> <li>Luis Miranda  </li> <li>Vinicius Vieira  </li> <li>Vinicius Martins  </li> <li>Tiago Lima  </li> <li>Let\u00edcia Paiva  </li> <li>Philipe Barbosa  </li> </ul>"},{"location":"atas/ata-4/#pautas-da-reuniao","title":"Pautas da Reuni\u00e3o","text":"<p>Reuni\u00e3o para esclarecimento de d\u00favidas com o monitor.</p> <ul> <li>Acesso \u00e0 infraestrutura: n\u00e3o ser\u00e1 total, por se tratar de uma estrutura interna do laborat\u00f3rio. Permiss\u00f5es espec\u00edficas ser\u00e3o concedidas conforme necessidade.</li> <li>Apresenta\u00e7\u00e3o da arquitetura geral do projeto.</li> <li>Pontos discutidos:</li> <li>Acesso ao Airflow (ainda pendente);</li> <li>Elabora\u00e7\u00e3o de scripts para o pipeline de dados;</li> <li>Ferramentas previstas: Spark ou Pandas.</li> </ul>"},{"location":"atas/ata-4/#etapa-inicial","title":"Etapa Inicial","text":"<p>A primeira fase ser\u00e1 focada em atividades de engenharia de dados. Tarefas previstas: - Cria\u00e7\u00e3o de scripts para extra\u00e7\u00e3o de dados dos jornais; - Inser\u00e7\u00e3o dos dados crus no banco de dados; - Limpeza e tratamento dos dados; - Estrutura\u00e7\u00e3o em formato JSON; - Armazenamento no MinIO; - Defini\u00e7\u00e3o do m\u00e9todo de extra\u00e7\u00e3o; - Implementa\u00e7\u00e3o de rotina de coleta di\u00e1ria (not\u00edcias do dia anterior).</p> <p>Obs.: enquanto o Airflow n\u00e3o estiver operacional, os scripts ser\u00e3o executados localmente.</p>"},{"location":"atas/ata-4/#ambiente-de-desenvolvimento","title":"Ambiente de Desenvolvimento","text":"<ul> <li>Configura\u00e7\u00e3o do ambiente ser\u00e1 realizada durante a semana, para uso pela equipe.</li> </ul>"},{"location":"atas/ata-4/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>Libera\u00e7\u00e3o de acesso ao Airflow;</li> <li>Organiza\u00e7\u00e3o das tasks e defini\u00e7\u00e3o do m\u00e9todo de extra\u00e7\u00e3o;</li> <li>Estudo e pr\u00e1tica com as ferramentas definidas.</li> </ul>"},{"location":"atas/ata-5/","title":"Ata 5 \u2013 13/05/2025","text":""},{"location":"atas/ata-5/#informacoes-gerais","title":"Informa\u00e7\u00f5es Gerais","text":"<p>Data: 13 de maio de 2025 Hor\u00e1rio: 20h \u00e0s 21h10 Plataforma: app.gather.town Pr\u00f3xima reuni\u00e3o: 20 de maio de 2025 Respons\u00e1vel pelo registro: Let\u00edcia Paiva</p>"},{"location":"atas/ata-5/#participantes","title":"Participantes","text":"<ul> <li>Amanda Campos  </li> <li>Fause Carlos  </li> <li>Raissa Andrade  </li> <li>Luis Miranda  </li> <li>Vinicius Vieira  </li> <li>Vinicius Martins  </li> <li>Tiago Lima  </li> <li>Let\u00edcia Paiva  </li> <li>Philipe Barbosa  </li> </ul>"},{"location":"atas/ata-5/#pautas-da-reuniao","title":"Pautas da Reuni\u00e3o","text":"<ul> <li>Orienta\u00e7\u00e3o t\u00e9cnica geral e explica\u00e7\u00e3o da estrutura do projeto.  </li> <li>Apoio do monitor (Leo) para rodar o projeto localmente.  </li> <li>Alinhamento das pr\u00f3ximas tarefas e divis\u00e3o de grupos.  </li> <li>Discuss\u00e3o sobre documenta\u00e7\u00e3o, funcionalidades pendentes e planejamento de releases.</li> </ul>"},{"location":"atas/ata-5/#atualizacoes-recentes","title":"Atualiza\u00e7\u00f5es Recentes","text":"<p>Apoio do Leo: - Auxiliou os membros a rodarem o projeto com sucesso. - Esclareceu d\u00favidas importantes sobre a arquitetura. - Refor\u00e7ou os pr\u00f3ximos passos, incluindo:   - Adapta\u00e7\u00e3o dos plays.   - Desenvolvimento de spiders com foco na extra\u00e7\u00e3o estruturada de not\u00edcias (t\u00edtulo, descri\u00e7\u00e3o, corpo e tags).</p>"},{"location":"atas/ata-5/#pontos-discutidos-com-a-professora","title":"Pontos Discutidos com a Professora","text":"<ul> <li>Documentar todo o projeto.  </li> <li>Divis\u00e3o da equipe em dois grupos:</li> <li>Grupo 1: desenvolvimento de spiders para novos jornais.  </li> <li>Grupo 2: aprimoramento dos spiders existentes.  </li> <li>Elabora\u00e7\u00e3o do roadmap do projeto.  </li> <li>Cria\u00e7\u00e3o do esquem\u00e1tico da arquitetura do sistema.</li> </ul>"},{"location":"atas/ata-5/#ambiente-de-desenvolvimento","title":"Ambiente de Desenvolvimento","text":"<p>A configura\u00e7\u00e3o do ambiente de desenvolvimento seguir\u00e1 durante a semana, com suporte para todos os membros da equipe.</p>"},{"location":"atas/ata-5/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>At\u00e9 sexta-feira, todos devem conseguir rodar o projeto localmente.  </li> <li>Ra\u00edssa ser\u00e1 respons\u00e1vel por organizar a lista de tarefas.  </li> <li>Ser\u00e1 definido um dia da semana para as releases recorrentes.</li> </ul>"},{"location":"atas/ata-6/","title":"Ata 5 \u2013 20/05/2025","text":""},{"location":"atas/ata-6/#informacoes-gerais","title":"Informa\u00e7\u00f5es Gerais","text":"<p>Data: 20 de maio de 2025 Hor\u00e1rio: 20h \u00e0s 21h Plataforma: app.gather.town Pr\u00f3xima reuni\u00e3o: 27 de maio de 2025 Respons\u00e1vel pelo registro: Let\u00edcia Paiva</p>"},{"location":"atas/ata-6/#participantes","title":"Participantes","text":"<ul> <li>Amanda Campos  </li> <li>Fause Carlos  </li> <li>Raissa Andrade  </li> <li>Luis Miranda  </li> <li>Vinicius Vieira  </li> <li>Vinicius Martins  </li> <li>Tiago Lima  </li> <li>Let\u00edcia Paiva  </li> <li>Philipe Barbosa  </li> </ul>"},{"location":"atas/ata-6/#pautas-da-reuniao","title":"Pautas da Reuni\u00e3o","text":"<ul> <li>Orienta\u00e7\u00e3o sobre os pr\u00f3ximos passos gerais com os monitores  </li> <li>Alinhamento das pr\u00f3ximas tarefas e divis\u00e3o dos grupos  </li> <li>Atualiza\u00e7\u00e3o sobre o status da Release 1  </li> </ul>"},{"location":"atas/ata-6/#atualizacoes-recentes","title":"Atualiza\u00e7\u00f5es Recentes","text":"<ul> <li>O monitor Eric atuar\u00e1 como suporte t\u00e9cnico da equipe na \u00e1rea de engenharia de dados.  </li> <li>As discuss\u00f5es e melhorias nos spiders e scrapers foram iniciadas.  </li> <li>A equipe foi dividida em dois grupos com responsabilidades espec\u00edficas:</li> </ul> <p>Grupo de Refatora\u00e7\u00e3o (respons\u00e1vel por revisar e melhorar spiders e scrapers existentes):   - Fause   - Raissa   - Amanda   - Let\u00edcia   - Vinicius Vieira  </p> <p>Grupo de Cria\u00e7\u00e3o (respons\u00e1vel pela cria\u00e7\u00e3o de novos spiders e scrapers):   - Philipe   - Tiago   - Luis Miranda   - Vinicius Mendes  </p> <ul> <li>Vinicius Mendes finalizou e publicou a Release 1, dispon\u00edvel em: Releases \u00b7 EH-FAKE/docs</li> </ul>"},{"location":"atas/ata-6/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>At\u00e9 a pr\u00f3xima reuni\u00e3o de equipe, as melhorias discutidas devem estar em desenvolvimento ou j\u00e1 implementadas.  </li> <li>Tiago ficar\u00e1 respons\u00e1vel por organizar e realizar a Release 2.</li> </ul>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/","title":"Fa\u00e7a sua raspagem de not\u00edcias em sites de noticias com Playwright e Scrapy!","text":"<p>Tutorial de como criar ou alterar spiders e plays para coletar e extrair conte\u00fado de not\u00edcias.</p>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#contexto-do-projeto","title":"Contexto do Projeto","text":"<p>O projeto foi modificado para focar na extra\u00e7\u00e3o de conte\u00fado de not\u00edcias em vez de an\u00fancios. O objetivo agora \u00e9 extrair o texto das not\u00edcias para posterior an\u00e1lise de fake news por uma LLM.</p>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#campos-que-precisam-ser-revisados","title":"\ud83d\udd04 Campos que PRECISAM ser revisados:","text":"<ul> <li>T\u00edtulo: Seletores podem ter mudado</li> <li>Corpo: Estrutura de conte\u00fado pode ter mudado  </li> <li>Description: NOVO campo - precisa ser identificado</li> <li>Tags: NOVO campo - precisa ser identificado</li> </ul>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#processo-de-atualizacao","title":"\u2705 Processo de atualiza\u00e7\u00e3o:","text":"<ol> <li>RE-INSPECIONAR cada portal no navegador atual</li> <li>COMPARAR seletores antigos vs estrutura atual</li> <li>IDENTIFICAR novos seletores para description e tags</li> <li>TESTAR extra\u00e7\u00e3o com URLs reais atuais</li> </ol>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#spiders-coleta-de-urls","title":"\ud83d\udd77\ufe0f SPIDERS - Coleta de URLs","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#funcao-dos-spiders","title":"Fun\u00e7\u00e3o dos Spiders","text":"<p>Os spiders s\u00e3o respons\u00e1veis por navegar na p\u00e1gina inicial dos portais de not\u00edcia e coletar URLs de artigos/not\u00edcias v\u00e1lidos.</p>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#alteracoes-necessarias-nos-spiders-existentes","title":"\u26a0\ufe0f Altera\u00e7\u00f5es Necess\u00e1rias nos Spiders Existentes","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#1-melhorar-filtragem-de-urls","title":"1. Melhorar Filtragem de URLs","text":"<p>Muitos spiders n\u00e3o filtram adequadamente os URLs. Use o gazetaDoPovo como refer\u00eancia de boa filtragem:</p> <p>\u274c Exemplo de filtragem ruim (r7.py):</p> <pre><code>def allow_url(self, entry_url):\n    return (\n        entry_url.startswith(\"https://\")\n        and len(entry_url) &gt; 100\n        and re.match(\n            r\"https://(entretenimento|esportes|record|noticias)\\.r7\\.com\", entry_url\n        )\n    )\n</code></pre> <p>\u2705 Exemplo de filtragem boa (gazetaDoPovo.py):</p> <pre><code>def allow_url(self, url: str) -&gt; bool:\n    p = urlparse(url)\n    path = p.path.rstrip('/')\n\n    # 1) blacklist pure sections\n    if path in {\"/videos\", \"/vozes\", \"/podcasts\", \"/newsletter\", \"/ebooks\"}:\n        self.logger.info(f\"Blacklisted URL: {url}\")\n        return False\n\n    # 2) require at least two non-empty segments (section + slug)\n    segments = [seg for seg in path.split('/') if seg]\n    if len(segments) &lt; 2:\n        self.logger.info(f\"Blacklisted URL: {url}\")\n        return False\n\n    slug = segments[-1]\n    # 3a) long slugs by hyphens or 3b) by character length\n    if slug.count('-') &gt;= 3 or len(slug) &gt; 30:\n        return True\n\n    return False\n</code></pre>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#2-remover-yield-scrapyrequest-recursivos","title":"2. Remover yield scrapy.Request Recursivos","text":"<p>Queremos apenas as not\u00edcias da p\u00e1gina inicial, ent\u00e3o remova os <code>yield scrapy.Request</code> que fazem crawling recursivo:</p> <p>\u274c Remover:</p> <pre><code>def parse(self, response):\n    seen = set()\n    for entry in response.css(\"a\"):\n        url = entry.attrib.get(\"href\")\n        if url and url not in seen and self.allow_url(url):\n            seen.add(url)\n            yield URLItem(url=url)\n            yield scrapy.Request(url=url, callback=self.parse)  # \u2190 REMOVER ESTA LINHA\n</code></pre> <p>\u2705 Manter apenas:</p> <pre><code>def parse(self, response):\n    seen = set()\n    for entry in response.css(\"a\"):\n        url = entry.attrib.get(\"href\")\n        if url and url not in seen and self.allow_url(url):\n            seen.add(url)\n            yield URLItem(url=url)  # \u2190 MANTER APENAS ISTO\n</code></pre>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#criando-um-novo-spider","title":"\ud83c\udd95 Criando um Novo Spider","text":"<p>Estrutura base para um novo spider:</p> <pre><code>import scrapy\nfrom spiders.base import BaseSpider\nfrom spiders.items import URLItem\nfrom urllib.parse import urlparse\n\nclass NovoPortalSpider(BaseSpider):\n    name = \"novoportalspider\"\n    start_urls = [\"https://www.novoportal.com.br/\"]\n    allowed_domains = [\"novoportal.com.br\"]\n\n    custom_settings = {\n        **BaseSpider.custom_settings,\n        \"COOKIES_ENABLED\": True,\n        \"DOWNLOAD_DELAY\": 3,\n        \"DEFAULT_REQUEST_HEADERS\": {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n            \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n            \"Accept-Encoding\": \"gzip, deflate, br\",\n            \"Connection\": \"keep-alive\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n            \"Cache-Control\": \"max-age=0\",\n        }\n    }\n\n    def allow_url(self, url: str) -&gt; bool:\n        \"\"\"\n        Implementar l\u00f3gica espec\u00edfica do portal para filtrar apenas URLs de not\u00edcias\n        \"\"\"\n        p = urlparse(url)\n        path = p.path.rstrip('/')\n\n        # Exemplo de filtragem - adaptar para cada portal\n        segments = [seg for seg in path.split('/') if seg]\n        if len(segments) &lt; 2:\n            return False\n\n        # Verificar se \u00e9 uma not\u00edcia real (slug longo, m\u00faltiplos h\u00edfens, etc.)\n        slug = segments[-1]\n        return slug.count('-') &gt;= 2 or len(slug) &gt; 20\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield scrapy.Request(\n                url,\n                callback=self.parse,\n                dont_filter=True,\n                meta={\"dont_redirect\": True, \"handle_httpstatus_list\": [403]},\n            )\n\n    def parse(self, response):\n        # Adaptar seletor CSS/XPath para o portal espec\u00edfico\n        links = response.css(\"a\")  # ou response.xpath(\"//a\")\n\n        for link in links:\n            url = link.attrib.get(\"href\")\n            if not url:\n                continue\n\n            # Construir URL absoluta\n            full_url = response.urljoin(url)\n            full_url = full_url.split('#', 1)[0].split('?', 1)[0]  # Remove fragmentos e query params\n\n            if self.allow_url(full_url):\n                yield URLItem(url=full_url)\n</code></pre>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#plays-extracao-de-conteudo","title":"\ud83c\udfad PLAYS - Extra\u00e7\u00e3o de Conte\u00fado","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#funcao-dos-plays","title":"Fun\u00e7\u00e3o dos Plays","text":"<p>Os plays s\u00e3o respons\u00e1veis por acessar cada URL de not\u00edcia e extrair o conte\u00fado: t\u00edtulo, descri\u00e7\u00e3o, corpo da mat\u00e9ria e tags.</p>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#alteracoes-necessarias-nos-plays-existentes","title":"\u26a0\ufe0f Altera\u00e7\u00f5es Necess\u00e1rias nos Plays Existentes","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#1-remover-take_screenshot","title":"1. Remover take_screenshot","text":"<p>N\u00e3o precisamos mais de screenshots, apenas do texto. Remover todas as chamadas para <code>take_screenshot</code>:</p> <p>\u274c Remover:</p> <pre><code>entry_screenshot_path = self.take_screenshot(page, self.url, goto=False)\n\n# E na cria\u00e7\u00e3o do EntryItem:\nreturn EntryItem(\n    title=entry_title,\n    ads=ad_items,\n    url=self.url,\n    screenshot_path=entry_screenshot_path,  # \u2190 REMOVER\n)\n</code></pre>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#2-atualizar-todos-os-seletores","title":"2. Atualizar TODOS os Seletores","text":"<p>\u26a0\ufe0f IMPORTANTE: Portais de not\u00edcia frequentemente alteram sua estrutura HTML. TODOS os seletores precisam ser revisados e atualizados:</p> <ul> <li>Seletores existentes (t\u00edtulo, corpo) podem estar desatualizados</li> <li>Novos seletores s\u00e3o necess\u00e1rios para description e tags</li> </ul>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#3-focar-na-extracao-de-conteudo","title":"3. Focar na Extra\u00e7\u00e3o de Conte\u00fado","text":"<p>Remover todo c\u00f3digo relacionado a an\u00fancios e focar na extra\u00e7\u00e3o de: - title: T\u00edtulo da not\u00edcia (REVISAR seletores existentes) - description: Subt\u00edtulo/resumo da not\u00edcia (NOVO campo - identificar seletores) - body: Corpo completo da not\u00edcia (REVISAR seletores existentes) - tags: Tags/categorias da not\u00edcia (NOVO campo - identificar seletores)</p>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#4-remover-codigo-de-anuncios-opcional","title":"4. Remover C\u00f3digo de An\u00fancios (opcional)","text":"<p>Remover: - M\u00e9todos <code>find_items</code> - Vari\u00e1veis <code>n_expected_ads</code> - Locators para elementos de an\u00fancio (<code>.videoCube</code>, <code>#taboola-*</code>, etc.)</p>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#criando-um-novo-play","title":"\ud83c\udd95 Criando um Novo Play","text":"<p>Estrutura base para um novo play:</p> <pre><code>import time\nfrom playwright.sync_api import sync_playwright\nfrom plays.base import BasePlay\nfrom plays.items import EntryItem\nfrom plog import logger\n\nclass NovoPortalPlay(BasePlay):\n    name = \"novoportal\"\n\n    @classmethod\n    def match(cls, url):\n        return \"novoportal.com.br\" in url\n\n    def pre_run(self):\n        pass\n\n    def run(self) -&gt; EntryItem:\n        with sync_playwright() as p:\n            browser = self.launch_browser(p, viewport={\"width\": 1920, \"height\": 1080})\n            page = browser.new_page()\n            logger.info(f\"[{self.name}] Opening URL '{self.url}'...\")\n            page.goto(self.url, timeout=180_000)\n\n            # Aguardar carregamento do conte\u00fado principal\n            page.wait_for_selector(\"h1\", timeout=30000)\n\n            # 1. EXTRAIR T\u00cdTULO\n            entry_title = \"\"\n            try:\n                # Adaptar seletor para o portal espec\u00edfico\n                entry_title = page.locator(\"h1\").first.inner_text()\n            except Exception as e:\n                logger.warning(f\"[{self.name}] Failed to extract title: {str(e)}\")\n\n            # 2. EXTRAIR DESCRI\u00c7\u00c3O (quando dispon\u00edvel)\n            description = \"\"\n            try:\n                # Exemplos de seletores comuns para descri\u00e7\u00e3o/subt\u00edtulo\n                selectors = [\n                    \".subtitle\",\n                    \".description\", \n                    \".lead\",\n                    \".summary\",\n                    \"h2\",\n                    \".article-subtitle\"\n                ]\n\n                for selector in selectors:\n                    try:\n                        desc_element = page.locator(selector)\n                        if desc_element.count() &gt; 0:\n                            description = desc_element.inner_text()\n                            if description.strip():\n                                break\n                    except Exception:\n                        continue\n\n            except Exception as e:\n                logger.warning(f\"[{self.name}] Failed to extract description: {str(e)}\")\n\n            # 3. EXTRAIR CORPO DA NOT\u00cdCIA\n            body = \"\"\n            try:\n                # Exemplos de seletores comuns para o corpo da mat\u00e9ria\n                selectors = [\n                    \".article-content\",\n                    \".post-content\", \n                    \".entry-content\",\n                    \".content\",\n                    \"article\",\n                    \".text\",\n                    \".article-body\"\n                ]\n\n                for selector in selectors:\n                    try:\n                        content_element = page.locator(selector)\n                        if content_element.count() &gt; 0:\n                            body = content_element.inner_text()\n                            if body.strip():\n                                logger.info(f\"[{self.name}] Successfully extracted body using: {selector}\")\n                                break\n                    except Exception:\n                        continue\n\n                if not body.strip():\n                    logger.warning(f\"[{self.name}] Failed to extract article body\")\n\n            except Exception as e:\n                logger.warning(f\"[{self.name}] Failed to extract article body: {str(e)}\")\n\n            # 4. EXTRAIR TAGS (quando dispon\u00edvel)\n            tags = []\n            try:\n                # Exemplos de seletores comuns para tags\n                tag_selectors = [\n                    \".tags a\",\n                    \".categories a\", \n                    \".post-tags a\",\n                    \".article-tags a\",\n                    \"[rel='tag']\"\n                ]\n\n                for selector in tag_selectors:\n                    try:\n                        tag_elements = page.locator(selector)\n                        if tag_elements.count() &gt; 0:\n                            for i in range(tag_elements.count()):\n                                tag_text = tag_elements.nth(i).inner_text().strip()\n                                if tag_text:\n                                    tags.append(tag_text)\n                            if tags:\n                                break\n                    except Exception:\n                        continue\n\n            except Exception as e:\n                logger.warning(f\"[{self.name}] Failed to extract tags: {str(e)}\")\n\n            return EntryItem(\n                title=entry_title,\n                url=self.url,\n                description=description,\n                body=body,\n                tags=tags,\n            )\n</code></pre>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#exemplos-de-referencia","title":"\ud83d\udccb Exemplos de Refer\u00eancia","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#bons-exemplos-para-seguir","title":"\u2705 Bons Exemplos para Seguir:","text":"<ol> <li>gazetaDoPovo.py (spider + play) - Boa filtragem de URLs e extra\u00e7\u00e3o de conte\u00fado</li> <li>metropoles.py (play) - Extra\u00e7\u00e3o completa com t\u00edtulo, descri\u00e7\u00e3o, corpo e tags</li> </ol>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#exemplos-que-precisam-ser-atualizados","title":"\u274c Exemplos que Precisam ser Atualizados:","text":"<p>Todos os outros plays que ainda usam: - <code>take_screenshot</code> - C\u00f3digo para extra\u00e7\u00e3o de an\u00fancios (opcional) - <code>yield scrapy.Request</code> recursivo nos spiders</p>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#dicas-para-desenvolver","title":"\ud83d\udd0d Dicas para Desenvolver","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#para-spiders","title":"Para Spiders:","text":"<ol> <li>Inspecionar a p\u00e1gina inicial do portal no navegador</li> <li>Identificar os links de not\u00edcias (geralmente t\u00eam URLs com slugs longos)</li> <li>Criar filtros espec\u00edficos para separar not\u00edcias de outras p\u00e1ginas</li> <li>Testar a filtragem para garantir que s\u00f3 coleta URLs v\u00e1lidos</li> </ol>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#para-plays","title":"Para Plays:","text":"<ol> <li>\u26a0\ufe0f SEMPRE inspecionar NOVAMENTE cada portal - estruturas HTML mudam frequentemente</li> <li>Identificar seletores para TODOS os 4 campos:</li> <li>T\u00edtulo: Pode ter mudado desde a \u00faltima vers\u00e3o</li> <li>Descri\u00e7\u00e3o: NOVO campo - encontrar subt\u00edtulo/resumo</li> <li>Corpo: Pode ter mudado desde a \u00faltima vers\u00e3o  </li> <li>Tags: NOVO campo - encontrar categorias/etiquetas</li> <li>Testar m\u00faltiplos seletores como fallback (sites podem ter varia\u00e7\u00f5es)</li> <li>Validar na p\u00e1gina real - n\u00e3o confiar apenas no c\u00f3digo antigo</li> </ol>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#processo-de-identificacao-de-seletores","title":"\ud83d\udee0\ufe0f Processo de Identifica\u00e7\u00e3o de Seletores:","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#1-para-campos-existentes-titulo-corpo","title":"1. Para campos EXISTENTES (t\u00edtulo, corpo):","text":"<ul> <li>N\u00e3o assumir que seletores antigos ainda funcionam</li> <li>Re-inspecionar elementos na p\u00e1gina atual</li> <li>Comparar seletores antigos vs novos</li> </ul>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#2-para-campos-novos-description-tags","title":"2. Para campos NOVOS (description, tags):","text":"<ul> <li>Procurar por subt\u00edtulos, resumos, leads</li> <li>Identificar se\u00e7\u00f5es de tags, categorias, etiquetas</li> <li>Testar se elementos existem em diferentes tipos de not\u00edcia</li> </ul>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#ferramentas-uteis","title":"Ferramentas \u00dateis:","text":"<ul> <li>Inspetor do navegador (F12) para identificar seletores atualizados</li> <li>Console do navegador para testar seletores CSS em tempo real</li> </ul>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#proximos-passos","title":"\ud83d\ude80 Pr\u00f3ximos Passos","text":"<ol> <li>Atualizar spiders existentes removendo <code>yield scrapy.Request</code> recursivo</li> <li>Melhorar filtragem de URLs seguindo o padr\u00e3o do gazetaDoPovo</li> <li>\u26a0\ufe0f RE-INSPECIONAR todos os portais - estruturas HTML podem ter mudado</li> <li>Atualizar TODOS os seletores existentes (t\u00edtulo, corpo) - n\u00e3o assumir que ainda funcionam</li> <li>Identificar seletores para campos novos (description, tags)</li> <li>Atualizar plays existentes removendo <code>take_screenshot</code> e c\u00f3digo de an\u00fancios</li> <li>Implementar extra\u00e7\u00e3o completa (t\u00edtulo, descri\u00e7\u00e3o, corpo, tags)</li> <li>Testar cada portal individualmente com URLs reais atuais</li> <li>Validar extra\u00e7\u00e3o - verificar se todos os 4 campos s\u00e3o capturados corretamente</li> </ol>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#checklist-de-validacao","title":"\ud83d\udcdd Checklist de Valida\u00e7\u00e3o","text":""},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#spider","title":"Spider:","text":"<ul> <li>[ ] Remove <code>yield scrapy.Request</code> recursivo</li> <li>[ ] Implementa <code>allow_url()</code> com boa filtragem</li> <li>[ ] Coleta apenas URLs da p\u00e1gina inicial</li> <li>[ ] URLs coletados s\u00e3o realmente de not\u00edcias</li> </ul>"},{"location":"guia-de-contribuicao/TUTORIAL_SPIDERS_PLAYS/#play","title":"Play:","text":"<ul> <li>[ ] RE-INSPECIONA o portal no navegador atual</li> <li>[ ] ATUALIZA seletores existentes (t\u00edtulo, corpo) - n\u00e3o usar c\u00f3digo antigo cegamente</li> <li>[ ] IDENTIFICA seletores novos para description e tags</li> <li>[ ] Remove <code>take_screenshot()</code> e vari\u00e1veis relacionadas</li> <li>[ ] TESTA extra\u00e7\u00e3o de t\u00edtulo com seletores atualizados</li> <li>[ ] IMPLEMENTA extra\u00e7\u00e3o de descri\u00e7\u00e3o (NOVO campo)</li> <li>[ ] TESTA extra\u00e7\u00e3o de corpo com seletores atualizados</li> <li>[ ] IMPLEMENTA extra\u00e7\u00e3o de tags (NOVO campo)</li> <li>[ ] VALIDA que todos os 4 campos s\u00e3o extra\u00eddos corretamente</li> <li>[ ] Retorna <code>EntryItem</code> com os dados corretos</li> <li>[ ] TESTA com URLs reais do portal atual </li> </ul>"},{"location":"guia-de-contribuicao/politica-de-branches/","title":"Pol\u00edtica de Branches","text":""},{"location":"guia-de-contribuicao/politica-de-branches/#guia-de-contribuicao-para-branch","title":"Guia de Contribui\u00e7\u00e3o para branch","text":""},{"location":"guia-de-contribuicao/politica-de-branches/#objetivo","title":"Objetivo","text":"<p>Estabelecer um modelo padronizado de ramifica\u00e7\u00e3o baseado no Git Flow para todos os reposit\u00f3rios da organiza\u00e7\u00e3o EHFAKE, com o objetivo de garantir:</p> <ul> <li>Entregas previs\u00edveis e organizadas;</li> <li>Releases controladas e audit\u00e1veis;</li> <li>Fluxo de desenvolvimento sustent\u00e1vel;</li> <li>Manuten\u00e7\u00e3o segura do c\u00f3digo em produ\u00e7\u00e3o.</li> </ul>"},{"location":"guia-de-contribuicao/politica-de-branches/#estrutura-geral-de-branches","title":"Estrutura Geral de Branches","text":"Branch Fun\u00e7\u00e3o Origem Destino Conven\u00e7\u00e3o de nomes <code>main</code> C\u00f3digo est\u00e1vel e versionado (produ\u00e7\u00e3o) release / hotfix \u2014 \u2014 <code>develop</code> Integra\u00e7\u00e3o cont\u00ednua de funcionalidades main release <code>feature/*</code>, <code>bugfix/*</code> <code>feature/*</code> Desenvolvimento de novas funcionalidades develop develop <code>feature/&lt;issue&gt;-&lt;slug&gt;</code> <code>bugfix/*</code> Corre\u00e7\u00f5es durante o desenvolvimento develop develop <code>bugfix/&lt;issue&gt;-&lt;slug&gt;</code> <code>release/*</code> Prepara\u00e7\u00e3o de vers\u00f5es develop main &amp; develop <code>release/vX.Y.Z</code> <code>hotfix/*</code> Corre\u00e7\u00f5es cr\u00edticas diretamente em produ\u00e7\u00e3o main main &amp; develop <code>hotfix/vX.Y.Z</code> <code>doc/*</code> Atualiza\u00e7\u00f5es de documenta\u00e7\u00e3o (repo <code>docs</code>) main main <code>doc/&lt;issue&gt;-&lt;slug&gt;</code> <code>gh-pages</code> Site gerado automaticamente pelo MkDocs <code>gh-deploy</code> \u2014 \u2014"},{"location":"guia-de-contribuicao/politica-de-branches/#convencoes-de-nomenclatura","title":"Conven\u00e7\u00f5es de Nomenclatura","text":"<ul> <li><code>&lt;issue&gt;</code>: n\u00famero da issue vinculada (ex.: <code>42</code>);</li> <li><code>&lt;slug&gt;</code>: descri\u00e7\u00e3o resumida da mudan\u00e7a em kebab-case (ex.: <code>melhora-autenticacao</code>);</li> <li>Vers\u00f5es seguem o padr\u00e3o Semantic Versioning (SemVer): <code>MAJOR.MINOR.PATCH</code>.</li> </ul>"},{"location":"guia-de-contribuicao/politica-de-branches/#fluxo-de-desenvolvimento","title":"Fluxo de Desenvolvimento","text":"<ol> <li> <p>Feature <code>feature/123-melhora-busca</code> \u2190 <code>develop</code> \u2192 PR \u2192 <code>develop</code> \u2192 Squash &amp; Merge</p> </li> <li> <p>Release <code>release/v1.2.0</code> \u2190 <code>develop</code> \u2192 PR \u2192 <code>main</code> &amp; <code>develop</code> \u2192 tag <code>v1.2.0</code></p> </li> <li> <p>Hotfix <code>hotfix/v1.2.1</code> \u2190 <code>main</code> \u2192 PR \u2192 <code>main</code> &amp; <code>develop</code> \u2192 tag <code>v1.2.1</code></p> </li> <li> <p>Documenta\u00e7\u00e3o <code>doc/456-guia-instalacao</code> \u2190 <code>main</code> \u2192 PR \u2192 <code>main</code> \u2192 <code>mkdocs gh-deploy</code> \u2192 gh-pages</p> </li> </ol>"},{"location":"guia-de-contribuicao/politica-de-branches/#repositorios-e-regras-especificas","title":"Reposit\u00f3rios e Regras Espec\u00edficas","text":"Reposit\u00f3rio Branches principais Observa\u00e7\u00f5es docs <code>main</code>, <code>gh-pages</code> Atualiza\u00e7\u00f5es feitas via branches <code>doc/*</code>. Deploy autom\u00e1tico com <code>mkdocs gh-deploy</code>. datasets <code>main</code> Fluxo Git Flow simplificado; utiliza <code>feature/*</code>, <code>bugfix/*</code> e <code>release/*</code> conforme necess\u00e1rio. fakebuster <code>main</code>, <code>develop</code> Aplica\u00e7\u00f5es de Machine Learning com RAGFlow. Utiliza Git Flow completo. backend <code>main</code>, <code>develop</code> Servi\u00e7os e APIs da aplica\u00e7\u00e3o. Git Flow completo. frontend <code>main</code>, <code>develop</code> Interface e frontend web. Git Flow completo."},{"location":"guia-de-contribuicao/politica-de-branches/#regras-de-merge","title":"Regras de Merge","text":"<ul> <li>Todo Pull Request deve:</li> <li>Ter pelo menos 1 reviewer aprovado;</li> <li> <p>Passar por todas as valida\u00e7\u00f5es de CI com sucesso.</p> </li> <li> <p>Estrat\u00e9gias de merge:</p> </li> <li>Squash &amp; Merge: para <code>feature/*</code>, <code>bugfix/*</code> e <code>doc/*</code>;</li> <li>Merge commit: para <code>release/*</code> e <code>hotfix/*</code> (preservando o hist\u00f3rico de versionamento);</li> <li>Tags: aplicadas apenas na branch <code>main</code>.</li> </ul>"},{"location":"guia-de-contribuicao/politica-de-branches/#automacao","title":"Automa\u00e7\u00e3o","text":"<ul> <li>GitHub Actions:</li> <li>Valida\u00e7\u00e3o autom\u00e1tica de nomes de branch com express\u00f5es regulares;</li> <li> <p>Workflows espec\u00edficos para release, build e deploy por reposit\u00f3rio;</p> </li> <li> <p>Artefatos:</p> </li> <li>S\u00e3o publicados automaticamente nas releases (<code>Docker</code>, <code>npm</code>, modelos <code>ML</code>, etc.).</li> </ul>"},{"location":"guia-de-contribuicao/politica-de-branches/#historico-do-documento","title":"Hist\u00f3rico do Documento","text":"Vers\u00e3o Data Descri\u00e7\u00e3o Autor 1.0 18/04/2025 Cria\u00e7\u00e3o inicial Equipe EHFAKE 1.1 20/05/2025 Revis\u00e3o e melhoria Equipe EHFAKE"},{"location":"guia-de-contribuicao/politica-de-commits/","title":"Pol\u00edtica de Commits","text":""},{"location":"guia-de-contribuicao/politica-de-commits/#guia-de-contribuicao-para-commit","title":"Guia de Contribui\u00e7\u00e3o para commit","text":"<p>Este guia padroniza as mensagens de commit em todos os reposit\u00f3rios da organiza\u00e7\u00e3o EHFAKE, com o objetivo de manter um hist\u00f3rico de altera\u00e7\u00f5es limpo, rastre\u00e1vel e compat\u00edvel com ferramentas de automa\u00e7\u00e3o como geradores de changelog e versionamento sem\u00e2ntico (SemVer).</p>"},{"location":"guia-de-contribuicao/politica-de-commits/#especificacao-adotada","title":"Especifica\u00e7\u00e3o adotada","text":"<ul> <li>Baseada na conven\u00e7\u00e3o Conventional Commits v1.0.0.</li> <li>Corpo e rodap\u00e9 seguem o padr\u00e3o da conven\u00e7\u00e3o Angular.</li> <li>O tipo <code>chore</code> n\u00e3o \u00e9 permitido.</li> </ul>"},{"location":"guia-de-contribuicao/politica-de-commits/#tipos-de-commit-permitidos","title":"Tipos de Commit Permitidos","text":"Tipo Descri\u00e7\u00e3o Exemplos de uso <code>build</code> Mudan\u00e7as no sistema de build ou depend\u00eancias externas Ajustes em <code>Dockerfile</code>, atualiza\u00e7\u00e3o do <code>package.json</code> <code>static</code> Altera\u00e7\u00f5es em conte\u00fado est\u00e1tico Inclus\u00e3o de imagens, atualiza\u00e7\u00e3o de \u00edcones, textos fixos <code>ci</code> Configura\u00e7\u00f5es de integra\u00e7\u00e3o cont\u00ednua Cria\u00e7\u00e3o/ajuste de workflows do GitHub Actions <code>cd</code> Configura\u00e7\u00f5es de entrega cont\u00ednua Altera\u00e7\u00f5es em pipelines de deploy <code>docs</code> Atualiza\u00e7\u00f5es de documenta\u00e7\u00e3o Modifica\u00e7\u00f5es no <code>README</code>, inclus\u00e3o de novos guias <code>feat</code> Adi\u00e7\u00e3o de nova funcionalidade Implementa\u00e7\u00e3o de novo endpoint, novo componente React <code>fix</code> Corre\u00e7\u00e3o de erros ou falhas Tratamento de exce\u00e7\u00f5es, corre\u00e7\u00e3o de bug em layout <code>perf</code> Melhorias de performance Implementa\u00e7\u00e3o de cache, <code>lazy-loading</code> <code>refactor</code> Refatora\u00e7\u00f5es sem altera\u00e7\u00e3o no comportamento Extra\u00e7\u00e3o de fun\u00e7\u00f5es, reorganiza\u00e7\u00e3o de pastas <code>improve</code> Melhorias visuais ou funcionais sem nova feature Ajustes de UI, melhorias em UX, mensagens mais claras <code>style</code> Altera\u00e7\u00f5es de formata\u00e7\u00e3o e estilo de c\u00f3digo Ajustes com <code>prettier</code>, remo\u00e7\u00e3o de imports n\u00e3o utilizados <code>test</code> Cria\u00e7\u00e3o ou modifica\u00e7\u00e3o de testes Testes unit\u00e1rios, mocks, cobertura <code>revert</code> Revers\u00e3o de commit anterior <code>revert: feat(auth): adiciona OAuth2</code>"},{"location":"guia-de-contribuicao/politica-de-commits/#estrutura-da-mensagem","title":"Estrutura da Mensagem","text":"<pre><code>&lt;tipo&gt;(&lt;escopo opcional&gt;): &lt;t\u00edtulo no imperativo&gt;\n\n&lt;corpo explicativo opcional&gt;\n\n&lt;rodap\u00e9 com BREAKING CHANGE e/ou refer\u00eancias de issue&gt;\n</code></pre>"},{"location":"guia-de-contribuicao/politica-de-commits/#regras-gerais","title":"Regras Gerais","text":"<ul> <li>T\u00edtulo com no m\u00e1ximo 50 caracteres.</li> <li>Corpo com linhas de at\u00e9 72 caracteres.</li> <li> <p>Utilize sempre o modo imperativo e tempo presente:</p> </li> <li> <p>\u2705 <code>adiciona</code>, <code>corrige</code>, <code>refatora</code> </p> </li> <li> <p>\u274c <code>adicionado</code>, <code>corrigido</code>, <code>refatorado</code></p> </li> <li> <p>Escopos devem utilizar <code>kebab-case</code>, por exemplo:</p> </li> <li><code>frontend/ui</code></li> <li> <p><code>backend/auth</code></p> </li> <li> <p>Para altera\u00e7\u00f5es que quebram compatibilidade, adicione no rodap\u00e9:</p> </li> </ul> <p><code>text   BREAKING CHANGE: descri\u00e7\u00e3o da altera\u00e7\u00e3o incompat\u00edvel</code></p> <ul> <li>Para vincular issues, utilize no rodap\u00e9:</li> </ul> <p><code>text   Closes #123   Related to #456</code></p> <ul> <li>Os commits devem ser at\u00f4micos: cada commit deve representar uma \u00fanica altera\u00e7\u00e3o l\u00f3gica no c\u00f3digo.</li> </ul>"},{"location":"guia-de-contribuicao/politica-de-commits/#exemplos-de-commit","title":"Exemplos de Commit","text":"<pre><code>feat(backend/auth): adiciona rota\u00e7\u00e3o de refresh token\n\nImplementa novo fluxo de rota\u00e7\u00e3o usando JWT ID, reduzindo riscos de reuse...\n\nCloses #45\n</code></pre> <pre><code>fix(frontend/ui): corrige alinhamento do bot\u00e3o de login em telas &lt; 320px\n</code></pre> <pre><code>docs(datasets): atualiza README com processo de curadoria\n</code></pre>"},{"location":"guia-de-contribuicao/politica-de-commits/#validacao-automatizada","title":"Valida\u00e7\u00e3o Automatizada","text":"<p>Todos os reposit\u00f3rios EHFAKE possuem valida\u00e7\u00e3o de commits com:</p> <ul> <li>Husky \u2013 ganchos <code>pre-commit</code> e <code>pre-push</code></li> <li>Commitlint \u2013 valida mensagens conforme esta especifica\u00e7\u00e3o</li> <li>Semantic Release \u2013 gera changelogs e vers\u00f5es com base nos commits</li> </ul> <p>Commits inv\u00e1lidos s\u00e3o automaticamente rejeitados.</p>"},{"location":"guia-de-contribuicao/politica-de-commits/#historico-do-documento","title":"Hist\u00f3rico do documento","text":"Vers\u00e3o Data Descri\u00e7\u00e3o Autor 1.0 18/04/2025 Cria\u00e7\u00e3o do documento Equipe EHFAKE 1.1 19/04/2025 Reformula\u00e7\u00e3o Equipe EHFAKE"},{"location":"guia-de-contribuicao/politica-de-commits/#links-uteis","title":"Links \u00dateis","text":"<ul> <li>Conventional Commits</li> <li>Semantic Versioning (SemVer)</li> <li>Angular Commit Guidelines</li> </ul>"},{"location":"post_mortem/Post_mortem/","title":"Post-Mortem do Projeto EH-FAKE","text":""},{"location":"post_mortem/Post_mortem/#1-visao-geral-do-projeto","title":"1. Vis\u00e3o Geral do Projeto","text":"<p>O projeto EH-FAKE \u00e9 uma iniciativa desenvolvida por estudantes da Universidade de Bras\u00edlia (UnB) com o objetivo de combater a desinforma\u00e7\u00e3o, especificamente em an\u00fancios publicit\u00e1rios de sa\u00fade que circulam em grandes sites de not\u00edcias brasileiros. A solu\u00e7\u00e3o proposta visa identificar e classificar not\u00edcias desinformativas utilizando tecnologias de Machine Learning e Processamento de Linguagem Natural (PLN).</p>"},{"location":"post_mortem/Post_mortem/#2-estrutura-do-repositorio-github","title":"2. Estrutura do Reposit\u00f3rio GitHub","text":"<p>O reposit\u00f3rio EH-FAKE no GitHub \u00e9 composto por tr\u00eas sub-reposit\u00f3rios principais, cada um com uma fun\u00e7\u00e3o espec\u00edfica no ecossistema do projeto:</p>"},{"location":"post_mortem/Post_mortem/#21-check-up","title":"2.1. <code>check-up</code>","text":"<p>Este \u00e9 o cora\u00e7\u00e3o do projeto, contendo a l\u00f3gica principal para a an\u00e1lise de desinforma\u00e7\u00e3o. Suas funcionalidades incluem:</p> <ul> <li>Coleta de URLs: Utiliza a biblioteca Scrapy para rastrear e coletar URLs de not\u00edcias das p\u00e1ginas iniciais de dez portais de not\u00edcias brasileiros (Estad\u00e3o, Folha, Globo, IG, Metr\u00f3poles, R7, RBS, Terra, Veja, UOL).</li> <li>Raspagem de An\u00fancios: Emprega a biblioteca Playwright para simular a navega\u00e7\u00e3o em um navegador e capturar an\u00fancios nativos presentes nas p\u00e1ginas de not\u00edcias. Tamb\u00e9m \u00e9 respons\u00e1vel por arquivar esses an\u00fancios e registrar capturas de tela.</li> <li>Classifica\u00e7\u00e3o de An\u00fancios: Utiliza um Grande Modelo de Linguagem (LLM) para classificar os an\u00fancios coletados em 45 categorias pr\u00e9-definidas, com a op\u00e7\u00e3o de integra\u00e7\u00e3o com a API da OpenAI.</li> </ul>"},{"location":"post_mortem/Post_mortem/#22-docs","title":"2.2. <code>docs</code>","text":"<p>Este reposit\u00f3rio \u00e9 dedicado \u00e0 documenta\u00e7\u00e3o abrangente do projeto EH-FAKE. Ele fornece informa\u00e7\u00f5es cruciais sobre:</p> <ul> <li>Contexto do Projeto: Detalha o problema da desinforma\u00e7\u00e3o em an\u00fancios de sa\u00fade e a motiva\u00e7\u00e3o por tr\u00e1s do EH-FAKE.</li> <li>Estrutura da Solu\u00e7\u00e3o: Explica como a solu\u00e7\u00e3o \u00e9 arquitetada, incluindo os conceitos de Query, Pergunta, Explica\u00e7\u00e3o para IA e Classifica\u00e7\u00e3o (Desinformativo, Informativo, N\u00e3o se encaixa).</li> <li>Tecnologias Utilizadas: Lista e descreve as principais ferramentas e bibliotecas, como RAGFlow (para orquestra\u00e7\u00e3o de fluxos RAG) e NGROK (para exposi\u00e7\u00e3o de servidores locais).</li> <li>Requisitos Funcionais: Apresenta os requisitos do sistema, como coleta cont\u00ednua de not\u00edcias e pesquisa personalizada por tema (Estabilidade democr\u00e1tica, Sa\u00fade, Meio ambiente, Mulheres, Povos e comunidades tradicionais, Popula\u00e7\u00e3o negra, Popula\u00e7\u00e3o LGBTQIA+) e fonte da not\u00edcia.</li> <li>Instala\u00e7\u00e3o e Uso: Fornece instru\u00e7\u00f5es detalhadas para configurar e executar o projeto localmente, incluindo a cria\u00e7\u00e3o de ambiente virtual, instala\u00e7\u00e3o de depend\u00eancias e execu\u00e7\u00e3o dos scripts.</li> <li>Contribui\u00e7\u00e3o: Orienta\u00e7\u00f5es para novos contribuidores.</li> </ul>"},{"location":"post_mortem/Post_mortem/#3-analise-tecnica-e-estrutura-do-codigo","title":"3. An\u00e1lise T\u00e9cnica e Estrutura do C\u00f3digo","text":""},{"location":"post_mortem/Post_mortem/#31-check-up","title":"3.1. <code>check-up</code>","text":"<p>O reposit\u00f3rio <code>check-up</code> \u00e9 o componente central do projeto, respons\u00e1vel pela coleta, raspagem e classifica\u00e7\u00e3o de an\u00fancios. A estrutura do c\u00f3digo reflete essa modularidade:</p> <ul> <li><code>models.py</code>: Define os modelos de dados para o banco de dados, incluindo <code>Portal</code>, <code>Entry</code>, <code>Advertisement</code>, <code>URLQueue</code> e <code>QueueStatus</code>. Isso indica uma abordagem robusta para o armazenamento e gerenciamento dos dados coletados.</li> <li><code>spiders/</code>: Cont\u00e9m os spiders (rastreadores) implementados com a biblioteca Scrapy. Cada arquivo dentro deste diret\u00f3rio (<code>estadao.py</code>, <code>folha.py</code>, <code>globo.py</code>, etc.) \u00e9 respons\u00e1vel por coletar URLs de not\u00edcias de um portal espec\u00edfico. A exist\u00eancia de um <code>base.py</code> sugere uma estrutura de heran\u00e7a para reutiliza\u00e7\u00e3o de c\u00f3digo entre os spiders.</li> <li><code>plays/</code>: Abriga os scripts que utilizam a biblioteca Playwright para raspar os an\u00fancios das p\u00e1ginas de not\u00edcias. Assim como os spiders, cada arquivo (<code>estadao.py</code>, <code>folha.py</code>, etc.) corresponde a um portal espec\u00edfico, indicando a necessidade de adapta\u00e7\u00e3o do c\u00f3digo para a estrutura HTML de cada site. O <code>base.py</code> e <code>utils.py</code> provavelmente fornecem funcionalidades comuns.</li> <li><code>llm/</code>: Cont\u00e9m a l\u00f3gica para a classifica\u00e7\u00e3o de an\u00fancios usando um Grande Modelo de Linguagem (LLM). A men\u00e7\u00e3o a <code>categories.py</code> e <code>OPENAI_API_KEY</code> no README sugere que a classifica\u00e7\u00e3o \u00e9 baseada em categorias predefinidas e pode ser integrada com a API da OpenAI.</li> <li><code>docker/</code>: Inclui arquivos relacionados \u00e0 conteineriza\u00e7\u00e3o do ambiente, como <code>compose.yml</code> e <code>Dockerfile</code>, facilitando a configura\u00e7\u00e3o e execu\u00e7\u00e3o do projeto em diferentes ambientes.</li> <li><code>Makefile</code>: Define comandos para automatizar tarefas como iniciar servi\u00e7os (<code>make start</code>), inicializar o banco de dados (<code>make init_db</code>), executar migra\u00e7\u00f5es (<code>make migrate_db</code>), coletar URLs (<code>make crawl</code>) e raspar an\u00fancios (<code>make scrape</code>). Isso simplifica o fluxo de trabalho para desenvolvedores e usu\u00e1rios.</li> </ul>"},{"location":"post_mortem/Post_mortem/#32-docs","title":"3.2. <code>docs</code>","text":"<p>O reposit\u00f3rio <code>docs</code> \u00e9 uma documenta\u00e7\u00e3o bem estruturada do projeto, utilizando o MkDocs. A organiza\u00e7\u00e3o dos arquivos indica uma preocupa\u00e7\u00e3o com a clareza e facilidade de uso:</p> <ul> <li><code>docs/</code>: Cont\u00e9m os arquivos Markdown que comp\u00f5em a documenta\u00e7\u00e3o. A presen\u00e7a de subdiret\u00f3rios e arquivos como <code>index.md</code>, <code>contributing.md</code>, <code>tutorial_spiders_plays.md</code> demonstra uma organiza\u00e7\u00e3o l\u00f3gica do conte\u00fado.</li> <li><code>mkdocs.yml</code>: Arquivo de configura\u00e7\u00e3o do MkDocs, que define a estrutura da navega\u00e7\u00e3o, temas e plugins utilizados na documenta\u00e7\u00e3o.</li> <li><code>requirements.txt</code>: Lista as depend\u00eancias Python necess\u00e1rias para construir e servir a documenta\u00e7\u00e3o.</li> </ul>"},{"location":"post_mortem/Post_mortem/#4-objetivos-e-resultados-do-projeto","title":"4. Objetivos e Resultados do Projeto","text":""},{"location":"post_mortem/Post_mortem/#41-objetivos-iniciais","title":"4.1. Objetivos Iniciais","text":"<p>O principal objetivo do projeto EH-FAKE, conforme descrito na documenta\u00e7\u00e3o, \u00e9 desenvolver uma tecnologia baseada em Machine Learning para identificar e classificar not\u00edcias desinformativas, com foco inicial em an\u00fancios publicit\u00e1rios de sa\u00fade. Os objetivos espec\u00edficos incluem:</p> <ul> <li>Coleta Cont\u00ednua de Not\u00edcias: Estabelecer um sistema para coletar regularmente not\u00edcias de jornais e revistas parceiros.</li> <li>Pesquisa Personalizada: Permitir a pesquisa e classifica\u00e7\u00e3o de not\u00edcias com base em temas espec\u00edficos (estabilidade democr\u00e1tica, sa\u00fade, meio ambiente, mulheres, povos e comunidades tradicionais, popula\u00e7\u00e3o negra, popula\u00e7\u00e3o LGBTQIA+) e fontes.</li> <li>Identifica\u00e7\u00e3o de Desinforma\u00e7\u00e3o: Desenvolver um mecanismo para verificar a presen\u00e7a de desinforma\u00e7\u00e3o em an\u00fancios.</li> <li>Classifica\u00e7\u00e3o de An\u00fancios: Categorizar an\u00fancios em 'Desinformativo', 'Informativo' ou 'N\u00e3o se encaixa' usando LLMs.</li> </ul>"},{"location":"post_mortem/Post_mortem/#42-resultados-alcancados","title":"4.2. Resultados Alcan\u00e7ados","text":"<p>Com base na an\u00e1lise dos reposit\u00f3rios e da documenta\u00e7\u00e3o, o projeto EH-FAKE demonstrou os seguintes resultados e progressos:</p> <ul> <li>Infraestrutura de Coleta Funcional: O reposit\u00f3rio <code>check-up</code> implementa com sucesso spiders (Scrapy) para coleta de URLs e plays (Playwright) para raspagem de an\u00fancios de dez grandes portais de not\u00edcias brasileiros. A modularidade do c\u00f3digo permite a f\u00e1cil adi\u00e7\u00e3o de novos portais.</li> <li>Sistema de Classifica\u00e7\u00e3o Baseado em LLM: A arquitetura prev\u00ea a utiliza\u00e7\u00e3o de um LLM para classificar an\u00fancios, o que representa uma abordagem moderna e escal\u00e1vel para a detec\u00e7\u00e3o de desinforma\u00e7\u00e3o. A integra\u00e7\u00e3o com a API da OpenAI sugere flexibilidade na escolha do modelo.</li> <li>Documenta\u00e7\u00e3o Abrangente: O reposit\u00f3rio <code>docs</code> \u00e9 um ponto forte do projeto, fornecendo uma documenta\u00e7\u00e3o clara e detalhada sobre o contexto, estrutura, tecnologias, requisitos e instru\u00e7\u00f5es de uso. Isso \u00e9 crucial para a colabora\u00e7\u00e3o e a ado\u00e7\u00e3o do projeto.</li> <li>Modularidade e Reusabilidade: A separa\u00e7\u00e3o das funcionalidades em diferentes reposit\u00f3rios (<code>check-up</code>, <code>docs</code>, <code>airflow-rag</code>) e a estrutura interna de cada um (e.g., <code>base.py</code> para spiders e plays) indicam um design que favorece a modularidade e a reusabilidade do c\u00f3digo.</li> <li>Automa\u00e7\u00e3o de Workflow: A presen\u00e7a do <code>Makefile</code> no <code>check-up</code> e <code>airflow-rag</code> simplifica a execu\u00e7\u00e3o de tarefas complexas, como inicializa\u00e7\u00e3o de banco de dados, migra\u00e7\u00f5es e processos de coleta e raspagem, o que \u00e9 um indicativo de maturidade no desenvolvimento.</li> <li>Conteineriza\u00e7\u00e3o: O uso de Docker e Docker Compose facilita a configura\u00e7\u00e3o do ambiente de desenvolvimento e produ\u00e7\u00e3o, garantindo consist\u00eancia e reprodutibilidade.</li> </ul>"},{"location":"post_mortem/Post_mortem/#43-licoes-aprendidas-e-desafios","title":"4.3. Li\u00e7\u00f5es Aprendidas e Desafios","text":"<p>Embora o projeto demonstre um progresso significativo, algumas li\u00e7\u00f5es e desafios podem ser inferidos:</p> <ul> <li>Depend\u00eancia de Estruturas de Sites: A necessidade de criar spiders e plays espec\u00edficos para cada portal de not\u00edcias indica que o sistema \u00e9 sens\u00edvel a mudan\u00e7as na estrutura HTML dos sites. Manter esses componentes atualizados pode ser um desafio cont\u00ednuo.</li> <li>Complexidade da Classifica\u00e7\u00e3o de Desinforma\u00e7\u00e3o: A classifica\u00e7\u00e3o de desinforma\u00e7\u00e3o \u00e9 inerentemente complexa e subjetiva. A efic\u00e1cia do LLM depender\u00e1 da qualidade dos prompts, dos dados de treinamento e da capacidade de adapta\u00e7\u00e3o a novas formas de desinforma\u00e7\u00e3o.</li> <li>Escalabilidade da Coleta: A coleta cont\u00ednua de not\u00edcias de m\u00faltiplos portais e a raspagem de an\u00fancios podem demandar recursos computacionais significativos, especialmente \u00e0 medida que o volume de dados aumenta.</li> <li>Manuten\u00e7\u00e3o de Modelos de LLM: A performance do sistema de classifica\u00e7\u00e3o est\u00e1 diretamente ligada \u00e0 manuten\u00e7\u00e3o e atualiza\u00e7\u00e3o do modelo de linguagem, o que pode exigir expertise em Machine Learning e recursos computacionais.</li> <li>Colabora\u00e7\u00e3o e Comunica\u00e7\u00e3o: Em projetos de c\u00f3digo aberto com m\u00faltiplos contribuidores, a comunica\u00e7\u00e3o eficaz e a coordena\u00e7\u00e3o s\u00e3o essenciais para garantir a integra\u00e7\u00e3o suave das diferentes partes do sistema.</li> <li>Documenta\u00e7\u00e3o Cont\u00ednua: Embora a documenta\u00e7\u00e3o seja um ponto forte, a manuten\u00e7\u00e3o e atualiza\u00e7\u00e3o cont\u00ednua, especialmente em um projeto em desenvolvimento ativo, \u00e9 crucial para garantir que ela reflita o estado atual do c\u00f3digo.</li> </ul>"},{"location":"post_mortem/Post_mortem/#5-recomendacoes-e-proximos-passos","title":"5. Recomenda\u00e7\u00f5es e Pr\u00f3ximos Passos","text":"<p>Para o futuro do projeto EH-FAKE, as seguintes recomenda\u00e7\u00f5es podem ser consideradas:</p> <ul> <li>Monitoramento e Manuten\u00e7\u00e3o de Spiders/Plays: Implementar um sistema de monitoramento para detectar quebras nos spiders e plays devido a altera\u00e7\u00f5es nos layouts dos sites. Considerar abordagens mais resilientes \u00e0 mudan\u00e7a de layout, se poss\u00edvel.</li> <li>Aprimoramento do LLM: Continuar refinando o modelo de linguagem e os prompts para melhorar a precis\u00e3o da classifica\u00e7\u00e3o de desinforma\u00e7\u00e3o. Explorar t\u00e9cnicas de fine-tuning ou modelos espec\u00edficos para o dom\u00ednio de sa\u00fade.</li> <li>Otimiza\u00e7\u00e3o de Recursos: Investigar otimiza\u00e7\u00f5es para a coleta e raspagem de dados, como o uso de proxies, rota\u00e7\u00e3o de IPs ou processamento distribu\u00eddo, para lidar com a escalabilidade.</li> <li>Feedback e Valida\u00e7\u00e3o Humana: Integrar um ciclo de feedback humano para validar as classifica\u00e7\u00f5es do LLM e usar esses dados para retreinar e aprimorar o modelo.</li> <li>Expans\u00e3o para Outras M\u00eddias: Considerar a expans\u00e3o da an\u00e1lise para outras formas de m\u00eddia, como v\u00eddeos ou \u00e1udios, se relevante para o escopo do projeto.</li> <li>Engajamento da Comunidade: Continuar incentivando a contribui\u00e7\u00e3o da comunidade, talvez atrav\u00e9s de hackathons ou desafios, para acelerar o desenvolvimento e aprimoramento do projeto.</li> <li>Publica\u00e7\u00e3o de Resultados: Compartilhar os resultados e as descobertas do projeto com a comunidade acad\u00eamica e o p\u00fablico em geral para aumentar a conscientiza\u00e7\u00e3o sobre a desinforma\u00e7\u00e3o em an\u00fancios de sa\u00fade.</li> </ul>"},{"location":"post_mortem/Post_mortem/#6-conclusao","title":"6. Conclus\u00e3o","text":"<p>O projeto EH-FAKE \u00e9 uma iniciativa promissora e bem estruturada para combater a desinforma\u00e7\u00e3o em an\u00fancios de sa\u00fade. Com uma base t\u00e9cnica s\u00f3lida, uma documenta\u00e7\u00e3o abrangente e uma abordagem modular, ele tem o potencial de causar um impacto significativo. Os desafios identificados s\u00e3o inerentes a projetos dessa natureza, mas com um planejamento cont\u00ednuo e a colabora\u00e7\u00e3o da comunidade, o EH-FAKE pode evoluir para uma ferramenta ainda mais poderosa e eficaz.</p>"},{"location":"releases/apresentacao/","title":"Apresenta\u00e7\u00e3o da Release I","text":""},{"location":"releases/apresentacao/#contextualizacao","title":"Contextualiza\u00e7\u00e3o","text":"<p>O projeto Eh Fake? tem como objetivo o desenvolvimento de uma solu\u00e7\u00e3o de detec\u00e7\u00e3o de not\u00edcias desinformativas, utilizando Machine Learning e t\u00e9cnicas de recupera\u00e7\u00e3o aumentada de informa\u00e7\u00f5es (RAG).</p> <p>A primeira release marcou o in\u00edcio formal do projeto, com a estrutura\u00e7\u00e3o do ambiente de desenvolvimento, a defini\u00e7\u00e3o da governan\u00e7a, a elabora\u00e7\u00e3o do backlog inicial e a configura\u00e7\u00e3o das pr\u00e1ticas de qualidade.</p>"},{"location":"releases/apresentacao/#objetivos-da-release-i","title":"Objetivos da Release I","text":"<ul> <li>Estruturar a base do projeto em termos de reposit\u00f3rio, versionamento e documenta\u00e7\u00e3o.</li> <li>Definir e atribuir pap\u00e9is de governan\u00e7a de software.</li> <li>Rodar o ambiente de Machine Learning (RAGflow) localmente.</li> <li>Iniciar o preenchimento do banco vetorial.</li> <li>Desenvolver os documentos essenciais para organiza\u00e7\u00e3o e contribui\u00e7\u00e3o no projeto.</li> <li>Criar o roadmap e backlog inicial.</li> </ul>"},{"location":"releases/apresentacao/#entregas-realizadas","title":"Entregas Realizadas","text":"<ul> <li>Reposit\u00f3rio p\u00fablico criado com hist\u00f3rico limpo.</li> <li>Defini\u00e7\u00e3o de pap\u00e9is:</li> <li>Community Manager</li> <li>Mantenedores</li> <li>Release Managers</li> <li>Docs Leads</li> <li>Registro de Atas</li> <li>Configura\u00e7\u00e3o inicial do ambiente Docker (Dockerfile e docker-compose.yml).</li> <li>Execu\u00e7\u00e3o local do sistema utilizando RAGflow.</li> <li>Cria\u00e7\u00e3o dos documentos essenciais:</li> <li>README.md</li> <li>CONTRIBUTING.md</li> <li>CODE_OF_CONDUCT.md</li> <li>Licen\u00e7a de software livre (MIT).</li> <li>Desenvolvimento do backlog, com \u00e9picos, features e hist\u00f3rias de usu\u00e1rio.</li> <li>Elabora\u00e7\u00e3o do roadmap inicial.</li> </ul>"},{"location":"releases/apresentacao/#destaques-tecnicos","title":"Destaques T\u00e9cnicos","text":"<ul> <li>Utiliza\u00e7\u00e3o de <code>git-flow</code> para organiza\u00e7\u00e3o de branches.</li> <li>Aplica\u00e7\u00e3o de versionamento sem\u00e2ntico (SemVer).</li> <li>Primeiras popula\u00e7\u00f5es do banco vetorial com embeddings.</li> <li>Solu\u00e7\u00e3o provis\u00f3ria para execu\u00e7\u00e3o do RAGflow em ambiente local enquanto o servidor institucional (LAPPIS) n\u00e3o \u00e9 disponibilizado.</li> </ul>"},{"location":"releases/apresentacao/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>Migra\u00e7\u00e3o do ambiente para o servidor do LAPPIS.</li> <li>Continua\u00e7\u00e3o do desenvolvimento do front-end b\u00e1sico.</li> <li>Amplia\u00e7\u00e3o do conjunto de dados vetoriais.</li> <li>Avan\u00e7o nas features planejadas para a pr\u00f3xima release.</li> <li>Conclus\u00e3o e publica\u00e7\u00e3o da Release Notes oficial.</li> </ul> <p>Data de encerramento da Release I: 27 de abril de 2025.</p>"},{"location":"releases/arquitetura-solucao/","title":"Arquitetura de Solu\u00e7\u00e3o","text":"<p>Tipo de Arquitetura: \u2794 Aplica\u00e7\u00e3o Serverless com Gerenciamento via Docker, CI/CD Pipeline e Controle de Configura\u00e7\u00e3o + Arquitetura de Recupera\u00e7\u00e3o-Aumentada (RAG)</p> <p>Resumo: O projeto \u00e9 desenvolvido como uma aplica\u00e7\u00e3o serverless, hospedada na plataforma RAGFlow, aproveitando sua infraestrutura de orquestra\u00e7\u00e3o, armazenamento vetorial e integra\u00e7\u00e3o com modelos de linguagem (LLMs). A solu\u00e7\u00e3o adota a abordagem Retrieval-Augmented Generation (RAG), utilizando a recupera\u00e7\u00e3o de dados relevantes de bases vetoriais para a gera\u00e7\u00e3o de respostas contextualmente precisas. A arquitetura foi projetada para suportar escalabilidade e flexibilidade, com foco na automa\u00e7\u00e3o de processos de integra\u00e7\u00e3o e deployment, al\u00e9m da gest\u00e3o eficiente de vers\u00f5es e evolu\u00e7\u00f5es do sistema.</p>"},{"location":"releases/arquitetura-solucao/#componentes-principais","title":"Componentes principais","text":"Componente Descri\u00e7\u00e3o Plataforma RAGFlow Ambiente que centraliza toda a execu\u00e7\u00e3o, interface e dados, fornecendo as ferramentas de orquestra\u00e7\u00e3o e modelo de IA FakeBuster Aplicativo de IA hospedado no RAGFlow, respons\u00e1vel pela ingest\u00e3o de dados e consulta \u00e0 base vetorial para gerar respostas Base Vetorial Interna Base de conhecimento estruturada a partir um conjunto de dados de not\u00edcias coletadas e gerenciada pelo RAGFlow, utilizada para recupera\u00e7\u00e3o de informa\u00e7\u00f5es relevantes Interface Web Nativa Interface gerada automaticamente no RAGFlow para intera\u00e7\u00e3o com o usu\u00e1rio Documenta\u00e7\u00e3o Mantida em GitHub Pages, separada da aplica\u00e7\u00e3o, usando Markdown"},{"location":"releases/arquitetura-solucao/#fluxo-de-funcionamento","title":"Fluxo de Funcionamento","text":"<ol> <li>O usu\u00e1rio acessa o aplicativo FakeBuster via interface nativa do RAGFlow.</li> <li>O FakeBuster recebe a pergunta do usu\u00e1rio.</li> <li>O mecanismo de recupera\u00e7\u00e3o do RAGFlow consulta a base vetorial interna para buscar informa\u00e7\u00f5es relevantes.</li> <li>O modelo de linguagem (LLM) Sabi\u00e1 gera a resposta, utilizando o contexto recuperado da base vetorial.</li> <li>A resposta \u00e9 enviada ao usu\u00e1rio com as fontes citadas, caso aplic\u00e1vel.</li> </ol>"},{"location":"releases/arquitetura-solucao/#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Plataforma de Execu\u00e7\u00e3o: RAGFlow (SaaS)</li> <li>Armazenamento de Dados: Banco vetorial interno (embeddings gerenciados pelo RAGFlow)</li> <li>Modelo de IA: Sabi\u00e1 (Maritaca AI) via RAGFlow</li> <li>Framework de Recupera\u00e7\u00e3o: LlamaIndex (nativo no RAGFlow)</li> <li>Controle de Vers\u00e3o: Git + GitHub</li> <li>Documenta\u00e7\u00e3o: Markdown + GitHub Pages</li> </ul>"},{"location":"releases/arquitetura-solucao/#diagrama-da-arquitetura","title":"Diagrama da Arquitetura","text":""},{"location":"releases/arquitetura-solucao/#historico-do-documento","title":"Hist\u00f3rico do documento","text":"Vers\u00e3o Data Descri\u00e7\u00e3o Autor 1.0 21/04/2025 Cria\u00e7\u00e3o do documento Amanda Campos 1.1 27/04/2025 Atualiza\u00e7\u00e3o da arquitetura Amanda Campos"},{"location":"releases/checklist/","title":"Checklist do projeto de Ger\u00eancia de Configura\u00e7\u00e3o Evolu\u00e7\u00e3o de Software","text":""},{"location":"releases/checklist/#gerencia-e-controle-de-versao","title":"Ger\u00eancia e Controle de Vers\u00e3o","text":"<ul> <li>[x] Reposit\u00f3rio p\u00fablico no GitHub/GitLab (com hist\u00f3rico limpo e organizado)</li> <li>[x] Uso de <code>git-flow</code> ou similar para estrat\u00e9gia de branches</li> <li>[x] Versionamento sem\u00e2ntico (SemVer) aplicado</li> <li>[x] Tags e releases publicados com Release Notes claras</li> <li>[ ] GitHub Actions / GitLab CI configurado com:<ul> <li>[ ] Build automatizado</li> <li>[ ] Testes automatizados (unit\u00e1rios/integrados)</li> <li>[ ] Linter (ex: ESLint, Flake8, etc.)</li> <li>[ ] Valida\u00e7\u00e3o de seguran\u00e7a e depend\u00eancias (ex: Dependabot, Snyk)</li> </ul> </li> <li>[x] Arquivos de configura\u00e7\u00e3o de ambiente: <code>Dockerfile</code>, <code>docker-compose.yml</code>, <code>.env.example</code></li> </ul>"},{"location":"releases/checklist/#documentacao","title":"Documenta\u00e7\u00e3o","text":"<ul> <li>[x] <code>README.md</code> completo com:</li> <li>[x] Vis\u00e3o geral do projeto (com prints de como funciona o projeto)</li> <li>[x] Tecnologias utilizadas</li> <li>[x] Como rodar localmente (instala\u00e7\u00e3o + execu\u00e7\u00e3o)</li> <li>[x] Como contribuir (passo a passo) - getting started - https://blog.discourse.org/tag/getting-started/</li> <li>[x] Como usar a aplica\u00e7\u00e3o  (guia de usu\u00e1rio) - https://blog.discourse.org/tag/getting-started/</li> <li>[x] Licen\u00e7a</li> <li>[x] <code>CONTRIBUTING.md</code> com diretrizes de contribui\u00e7\u00e3o</li> <li>[x] <code>CODE_OF_CONDUCT.md</code> com boas pr\u00e1ticas de conviv\u00eancia</li> <li>[x] <code>CHANGELOG.md</code> com hist\u00f3rico de altera\u00e7\u00f5es</li> <li>[x] gitpage com:</li> <li>[x] Landing page - vis\u00e3o de produto - ex: https://www.discourse.org/</li> <li>[x] Arquitetura da solu\u00e7\u00e3o</li> <li>[x] Roadmap e backlog p\u00fablico</li> <li>[ ] Dicion\u00e1rio de dados (se aplic\u00e1vel)</li> <li>[x] Documenta\u00e7\u00e3o t\u00e9cnica de como contribuir (community)</li> </ul>"},{"location":"releases/checklist/#comunicacao-e-comunidade","title":"Comunica\u00e7\u00e3o e Comunidade","text":"<ul> <li>[x] Sistema de governan\u00e7a (ex: mantenedores, comit\u00eas, vota\u00e7\u00e3o)</li> <li>[x] Templates para issues e pull requests</li> <li>[x] Etiquetas (labels) para organizar issues (ex: good first issue, bug, enhancement)</li> <li>[x] Agendas p\u00fablicas de reuni\u00f5es (caso ocorram)</li> </ul>"},{"location":"releases/checklist/#licenciamento-e-aspectos-legais","title":"Licenciamento e Aspectos Legais","text":"<ul> <li>[x] <code>LICENSE</code> com licen\u00e7a de software livre (ex: MIT, GPL, Apache 2.0)</li> <li>[x] Verifica\u00e7\u00e3o de licen\u00e7as das depend\u00eancias utilizadas</li> <li>[x] Termos de uso e pol\u00edtica de privacidade (para projetos web/app)</li> </ul>"},{"location":"releases/checklist/#qualidade-e-testabilidade","title":"Qualidade e Testabilidade","text":"<ul> <li>[ ] Cobertura de testes m\u00ednima estabelecida e monitorada</li> <li>[ ] Testes end-to-end automatizados (se aplic\u00e1vel)</li> <li>[ ] Ferramentas de an\u00e1lise est\u00e1tica de c\u00f3digo</li> <li>[ ] Monitoramento de qualidade com badges (ex: Codecov, SonarCloud)</li> </ul>"},{"location":"releases/checklist/#sustentabilidade-e-crescimento","title":"Sustentabilidade e Crescimento","text":"<ul> <li>[x] Roadmap p\u00fablico com funcionalidades desejadas</li> <li>[x] Planejamento de onboarding de novos contribuidores (documentacao de onboarding)</li> </ul>"},{"location":"releases/checklist/#infraestrutura-e-deploy-opcional","title":"Infraestrutura e Deploy (Opcional)","text":"<ul> <li>[ ] Deploy automatizado (CI/CD) para ambiente de homologa\u00e7\u00e3o/produ\u00e7\u00e3o</li> <li>[ ] Infraestrutura como c\u00f3digo (IaC) para ambientes cloud (ex: Terraform, Ansible)</li> <li>[ ] Observabilidade b\u00e1sica: logs, m\u00e9tricas e alertas (ex: Prometheus, Grafana, Sentry)</li> </ul>"},{"location":"releases/release-1-0-0/","title":"Release Notes","text":"<p>Vers\u00e3o: 1.0.0 Data de Lan\u00e7amento: 28/04/2025</p>"},{"location":"releases/release-1-0-0/#resumo-da-versao","title":"Resumo da Vers\u00e3o","text":"<p>O EH-FAKE \u00e9 um projeto open source que utiliza a ferramenta RAGFlow para an\u00e1lise e classifica\u00e7\u00e3o de not\u00edcias, identificando conte\u00fados informativos e desinformativos (fake news). O projeto visa criar uma solu\u00e7\u00e3o escal\u00e1vel e precisa para o combate \u00e0 desinforma\u00e7\u00e3o.</p>"},{"location":"releases/release-1-0-0/#melhorias-implementadas","title":"Melhorias Implementadas","text":""},{"location":"releases/release-1-0-0/#checklist-de-software-livre","title":"Checklist de Software livre","text":"<ul> <li>[x] GitHub Pages para documenta\u00e7\u00e3o p\u00fablica  </li> <li>[x] Guia de Contribui\u00e7\u00e3o (<code>CONTRIBUTING.md</code>)  </li> <li>[x] Templates padronizados para:  </li> <li><code>Issues</code> (bug report, feature request)  </li> <li><code>Pull Requests</code> </li> <li>[x] C\u00f3digo de Conduta (<code>CODE_OF_CONDUCT.md</code>)  </li> <li>[x] Dockeriza\u00e7\u00e3o do projeto </li> <li>[x] Licen\u00e7a MIT (<code>LICENSE</code>)  </li> </ul>"},{"location":"releases/release-1-0-0/#roadmap-do-projeto","title":"Roadmap do projeto","text":"<ul> <li>[x] Roadmap </li> <li>[x] Depend\u00eancias mapeadas</li> <li>[x] Arquitetura documentada</li> <li>[x] Backlog</li> </ul>"},{"location":"releases/release-1-0-0/#configuracao-de-ambiente","title":"Configura\u00e7\u00e3o de ambiente","text":"<ul> <li>[x] Configura\u00e7\u00e3o do ambiente:  </li> <li>[x] Guia de Execu\u00e7\u00e3o passo a passo</li> <li>[x] Projeto executando em todas as m\u00e1quinas</li> </ul>"},{"location":"releases/release-1-0-0/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>Refinamento do que j\u00e1 foi implementado</li> <li>Ajustes no Roadmap</li> <li>Implementa\u00e7\u00e3o das funcionalidades de integra\u00e7\u00e3o com o RAGFlow</li> <li>Utiliza\u00e7\u00e3o do sabi\u00e1 como modelo padr\u00e3o do RAGFlow</li> <li>Gerenciamento de riscos</li> </ul> <p>Equipe de Desenvolvimento </p> <ul> <li>Amanda Campos - @acamposs  </li> <li>Raissa - @RaissaAndradeS  </li> <li>Leticia - @leticiakrpaiva  </li> <li>Fause - @FauseSkyWalker  </li> <li>Luis - @LuisMiranda10  </li> <li>Phillipe - @PhMoraiis  </li> <li>Tiago - @Tiago1604  </li> <li>Vinicius Vieira - @viniciusvieira00  </li> <li>Vinicius Mendes - @yabamiah  </li> </ul>"},{"location":"releases/release-1-1-0/","title":"Release Notes","text":"<p>Vers\u00e3o: 1.1.0 Data de Lan\u00e7amento: 20/05/2025</p>"},{"location":"releases/release-1-1-0/#resumo-da-versao","title":"Resumo da Vers\u00e3o","text":"<p>Houve uma mudan\u00e7a no projeto, agora com foco no \"aos fatos\", criando toda a parte de documenta\u00e7\u00e3o que est\u00e1 faltando, conforme criado para o projeto anterior.</p>"},{"location":"releases/release-1-1-0/#melhorias-implementadas","title":"Melhorias Implementadas","text":"<ul> <li>[x] GitHub Pages para documenta\u00e7\u00e3o p\u00fablica / land</li> <li>[x] Atualiza\u00e7\u00e3o do README  </li> <li>[x] Documenta\u00e7\u00e3o t\u00e9cnica da comunidade</li> <li>[x] C\u00f3digo de Conduta (CODE_OF_CONDUCT.md)  </li> <li>[x] Templates de Contribui\u00e7\u00e3o</li> <li>[x] Arquivos de CI</li> <li>[x] Roadmap </li> </ul>"},{"location":"releases/release-1-1-0/#configuracao-do-ambiente","title":"Configura\u00e7\u00e3o do ambiente:","text":"<ul> <li>[x] Guia de Execu\u00e7\u00e3o passo a passo</li> <li>[x] Projeto executando em todas as m\u00e1quinas</li> </ul>"},{"location":"releases/release-1-1-0/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>Refinamento do que j\u00e1 foi implementado</li> <li>Implementa\u00e7\u00e3o das spiders de novos portais</li> <li>Melhoria das spiders j\u00e1 existentes</li> <li>Delega\u00e7\u00e3o de atividades para a equipe (divis\u00e3o da equipe)</li> </ul>"},{"location":"releases/release-1-1-0/#equipe-de-desenvolvimento","title":"Equipe de Desenvolvimento","text":"<ul> <li>Amanda Campos - @acamposs  </li> <li>Raissa - @RaissaAndradeS  </li> <li>Leticia - @leticiakrpaiva  </li> <li>Fause - @FauseSkyWalker  </li> <li>Luis - @LuisMiranda10  </li> <li>Phillipe - @PhMoraiis  </li> <li>Tiago - @Tiago1604  </li> <li>Vinicius Vieira - @viniciusvieira00  </li> <li>Vinicius Mendes - @yabamiah  </li> </ul>"},{"location":"releases/release-1-2-0/","title":"Release Notes","text":"<p>Vers\u00e3o: 1.2.0 Data de Lan\u00e7amento: 27/05/2025  </p>"},{"location":"releases/release-1-2-0/#resumo-da-versao","title":"Resumo da Vers\u00e3o","text":"<p>A equipe foi dividida em dois grupos:  </p> <ol> <li>Grupo de Refatora\u00e7\u00e3o </li> <li>Grupo de Cria\u00e7\u00e3o </li> </ol> <p>Grupo de Refatora\u00e7\u00e3o Respons\u00e1vel por refatorar os scrapers e spiders atuais para atender aos requisitos mais recentes.  </p> <ul> <li>Amanda Campos - @acamposs  </li> <li>Raissa - @RaissaAndradeS  </li> <li>Leticia - @leticiakrpaiva  </li> <li>Fause - @FauseSkyWalker  </li> <li>Vinicius Vieira - @viniciusvieira00  </li> </ul> <p>Grupo de Cria\u00e7\u00e3o Respons\u00e1vel pela cria\u00e7\u00e3o de novos scrapers e spiders de portais de not\u00edcias.  </p> <ul> <li>Luis - @LuisMiranda10  </li> <li>Phillipe - @PhMoraiis  </li> <li>Tiago - @Tiago1604  </li> <li>Vinicius Mendes - @yabamiah  </li> </ul>"},{"location":"releases/release-1-2-0/#melhorias-implementadas","title":"Melhorias Implementadas","text":""},{"location":"releases/release-1-2-0/#refatoracao-das-extracoes-dos-portais-de-noticias-atuais","title":"Refatora\u00e7\u00e3o das extra\u00e7\u00f5es dos portais de not\u00edcias atuais","text":"<ul> <li> <p>[x] IG: Amanda Campos - @acamposs  </p> <ul> <li>[x] spider  </li> <li>[x] play  </li> </ul> </li> <li> <p>[ ] R7: Leticia - @leticiakrpaiva  </p> <ul> <li>[ ] spider  </li> <li>[ ] play  </li> </ul> </li> <li> <p>[ ] RBS: Vinicius Vieira - @viniciusvieira00 </p> <ul> <li>[ ] spider  </li> <li>[ ] play  </li> </ul> </li> <li> <p>[ ] Terra: Raissa - @RaissaAndradeS  </p> <ul> <li>[ ] spider  </li> <li>[ ] play  </li> </ul> </li> <li> <p>[ ] UOL: Fause - @FauseSkyWalker </p> <ul> <li>[ ] spider  </li> <li>[ ] play  </li> </ul> </li> <li> <p>[ ] Veja: Fause - @FauseSkyWalker </p> <ul> <li>[ ] spider  </li> <li>[ ] play  </li> </ul> </li> </ul>"},{"location":"releases/release-1-2-0/#criacao-de-extracoes-para-novos-portais-de-noticias","title":"Cria\u00e7\u00e3o de extra\u00e7\u00f5es para novos portais de not\u00edcias","text":"<p>Cria\u00e7\u00e3o de 2 a 4 extra\u00e7\u00f5es, no m\u00ednimo. Os portais foram escolhidos com base na fonte \"DADOS para \u00c9 Fake?\".</p> <p>(obs: sujeito a mudan\u00e7as)</p> <ul> <li> <p>[x] Aliados Brasil: Luis - @LuisMiranda10  </p> <ul> <li>[x] spider  </li> <li>[x] play  </li> </ul> </li> <li> <p>[ ] Imirante: Phillipe - @PhMoraiis  </p> <ul> <li>[ ] spider  </li> <li>[ ] play  </li> </ul> </li> <li> <p>[ ] O Povo: Tiago - @Tiago1604  </p> <ul> <li>[ ] spider  </li> <li>[ ] play  </li> </ul> </li> <li> <p>[x] Mais Goi\u00e1s: Vinicius Mendes - @yabamiah  </p> <ul> <li>[x] spider  </li> <li>[x] play  </li> </ul> </li> </ul>"},{"location":"releases/release-1-2-0/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>Conclus\u00e3o das tarefas pendentes dos spiders e execu\u00e7\u00e3o dos testes (<code>play</code>)  </li> <li>Monitoramento da estabilidade das novas extra\u00e7\u00f5es  </li> <li>Integra\u00e7\u00e3o com o pipeline automatizado  </li> <li>Avalia\u00e7\u00e3o da cobertura de not\u00edcias e refinamento das fontes  </li> </ul>"},{"location":"releases/release-1-2-0/#equipe-de-desenvolvimento","title":"Equipe de Desenvolvimento","text":"<ul> <li>Amanda Campos - @acamposs  </li> <li>Raissa - @RaissaAndradeS  </li> <li>Leticia - @leticiakrpaiva  </li> <li>Fause - @FauseSkyWalker  </li> <li>Luis - @LuisMiranda10  </li> <li>Phillipe - @PhMoraiis  </li> <li>Tiago - @Tiago1604  </li> <li>Vinicius Vieira - @viniciusvieira00  </li> <li>Vinicius Mendes - @yabamiah  </li> </ul>"},{"location":"releases/release-1-3-0/","title":"Release Notes","text":"<p>Vers\u00e3o: 1.3.0 Data de Lan\u00e7amento: 03/06/2025  </p>"},{"location":"releases/release-1-3-0/#resumo-da-versao","title":"Resumo da Vers\u00e3o","text":"<p>Continua\u00e7\u00e3o do desenvolvimento que foi iniciado na release 1.2.0V : </p> <ol> <li>Grupo de Refatora\u00e7\u00e3o </li> <li>Grupo de Cria\u00e7\u00e3o </li> </ol> <p>Grupo de Refatora\u00e7\u00e3o Respons\u00e1vel por refatorar os scrapers e spiders atuais para atender aos requisitos mais recentes.  </p> <ul> <li>Amanda Campos - @acamposs  </li> <li>Raissa - @RaissaAndradeS  </li> <li>Leticia - @leticiakrpaiva  </li> <li>Fause - @FauseSkyWalker  </li> <li>Vinicius Vieira - @viniciusvieira00  </li> </ul> <p>Grupo de Cria\u00e7\u00e3o Respons\u00e1vel pela cria\u00e7\u00e3o de novos scrapers e spiders de portais de not\u00edcias.  </p> <ul> <li>Luis - @Luisiranda10  </li> <li>Phillipe - @PhMoraiis  </li> <li>Tiago - @Tiago1604  </li> <li>Vinicius Mendes - @yabamiah  </li> </ul> <p>Al\u00e9m disso, a equipe essa semana ir\u00e1 realizar:</p> <ul> <li>Apresenta\u00e7\u00e3o sobre as decisi\u00e7\u00f5es que foram tomadas para o rumo do projeto, o que foi feito e os pr\u00f3ximos passos.</li> <li>Abertura das issues das tarefas realizadas</li> </ul>"},{"location":"releases/release-1-3-0/#melhorias-implementadas","title":"Melhorias Implementadas","text":"<ul> <li>ser\u00e1 adicionado posteriormente</li> </ul>"},{"location":"releases/release-1-3-0/#refatoracao-das-extracoes-dos-portais-de-noticias-atuais","title":"Refatora\u00e7\u00e3o das extra\u00e7\u00f5es dos portais de not\u00edcias atuais","text":"<ul> <li>ser\u00e1 adicionado posteriormente</li> </ul>"},{"location":"releases/release-1-3-0/#criacao-de-extracoes-para-novos-portais-de-noticias","title":"Cria\u00e7\u00e3o de extra\u00e7\u00f5es para novos portais de not\u00edcias","text":"<ul> <li>ser\u00e1 adicionado posteriormente</li> </ul>"},{"location":"releases/release-1-3-0/#criacao-das-issues-das-tarefas-feitas","title":"Cria\u00e7\u00e3o das issues das tarefas feitas","text":"<ul> <li>ser\u00e1 adicionado posteriormente</li> </ul>"},{"location":"releases/release-1-3-0/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>ser\u00e1 adicionado posteriormente</li> </ul>"},{"location":"releases/release-1-3-0/#equipe-de-desenvolvimento","title":"Equipe de Desenvolvimento","text":"<ul> <li>Amanda Campos - @acamposs  </li> <li>Raissa - @RaissaAndradeS  </li> <li>Leticia - @leticiakrpaiva  </li> <li>Fause - @FauseSkyWalker  </li> <li>Luis - @LuisMiranda10  </li> <li>Phillipe - @PhMoraiis  </li> <li>Tiago - @Tiago1604  </li> <li>Vinicius Vieira - @viniciusvieira00  </li> <li>Vinicius Mendes - @yabamiah  </li> </ul>"},{"location":"releases/sistema-governanca/","title":"Sistema de Governan\u00e7a","text":""},{"location":"releases/sistema-governanca/#papeis-definidos","title":"Pap\u00e9is Definidos","text":"<ul> <li> <p>Community Manager: Amanda Campos Respons\u00e1vel pela comunica\u00e7\u00e3o e media\u00e7\u00e3o entre os membros do projeto.</p> </li> <li> <p>Mantenedores: Fause Carlos, Luis Miranda Garantem a integridade dos reposit\u00f3rios e aprovam mudan\u00e7as cr\u00edticas.</p> </li> <li> <p>Release Managers: Vinicius Vieira, Tiago Lima Organizam e publicam as vers\u00f5es do projeto.</p> </li> <li> <p>Docs Leads: Raissa Andrade, Vinicius Martins Gerenciam e atualizam a documenta\u00e7\u00e3o t\u00e9cnica e de usu\u00e1rio.</p> </li> <li> <p>Registro de Atas: Let\u00edcia Paiva, Philipe Barbosa Respons\u00e1veis por registrar decis\u00f5es e discuss\u00f5es das reuni\u00f5es.</p> </li> </ul>"},{"location":"releases/sistema-governanca/#regras-de-colaboracao","title":"Regras de Colabora\u00e7\u00e3o","text":"<ul> <li>Uso obrigat\u00f3rio de templates de Issues e Pull Requests.</li> <li>Revis\u00e3o obrigat\u00f3ria de c\u00f3digo por ao menos um mantenedor.</li> <li>Vota\u00e7\u00f5es para decis\u00f5es estrat\u00e9gicas importantes.</li> <li>Governan\u00e7a aberta e transparente, priorizando a colabora\u00e7\u00e3o e a inclus\u00e3o.</li> </ul> Vers\u00e3o Data Descri\u00e7\u00e3o Autor 1.0 21/04/2025 Cria\u00e7\u00e3o do documento Amanda Campos"},{"location":"sobre/Pipeline_dados_scraps/","title":"Pipeline de Dados do Check-up","text":""},{"location":"sobre/Pipeline_dados_scraps/#contexto-do-projeto","title":"Contexto do Projeto","text":"<p>O projeto Check-up tem como objetivo principal analisar a presen\u00e7a de desinforma\u00e7\u00e3o em an\u00fancios publicit\u00e1rios de sa\u00fade que circulam em grandes sites de not\u00edcias do Brasil. Este documento detalha o fluxo completo de dados desde a coleta inicial de URLs at\u00e9 a apresenta\u00e7\u00e3o final no frontend web.</p>"},{"location":"sobre/Pipeline_dados_scraps/#visao-geral-do-pipeline","title":"\ud83d\udd04 Vis\u00e3o Geral do Pipeline","text":"<p>O pipeline de dados do Check-up \u00e9 composto por cinco etapas principais:</p> <ol> <li>\ud83d\udd77\ufe0f Crawling (Spiders): Coleta URLs de artigos dos portais de not\u00edcias</li> <li>\ud83d\udcf0 Scraping (Plays): Extrai conte\u00fado e an\u00fancios das p\u00e1ginas coletadas  </li> <li>\ud83d\uddc4\ufe0f Armazenamento: Salva dados no PostgreSQL e arquivos no MinIO</li> <li>\ud83d\udd17 API Backend: Serve dados via FastAPI</li> <li>\ud83d\udda5\ufe0f Frontend: Interface React para visualiza\u00e7\u00e3o</li> </ol> <p></p>"},{"location":"sobre/Pipeline_dados_scraps/#estrutura-de-dados","title":"\ud83d\udcca Estrutura de Dados","text":""},{"location":"sobre/Pipeline_dados_scraps/#modelos-principais-postgresql","title":"Modelos Principais (PostgreSQL)","text":"<p>Portal: Informa\u00e7\u00f5es dos portais de not\u00edcias</p> <pre><code>- id: Integer (Primary Key)\n- name: String (Nome do portal)\n- url: URL (URL base do portal)\n- slug: String (Identificador \u00fanico)\n- created_at: DateTime\n</code></pre> <p>URLQueue: Fila de URLs para processamento</p> <pre><code>- id: Integer (Primary Key)  \n- url: URL (URL do artigo)\n- created_at: DateTime\n- statuses: List[QueueStatus] (Status de processamento)\n</code></pre> <p>Entry: Artigos de not\u00edcias processados</p> <pre><code>- id: Integer (Primary Key)\n- portal_id: ForeignKey (Portal)\n- title: String (T\u00edtulo do artigo)\n- url: URL (URL do artigo)\n- description: String (Subt\u00edtulo/resumo)\n- body: Text (Corpo completo do artigo)\n- tags: JSON (Tags/categorias)\n- screenshot: String (Caminho no MinIO)\n- created_at: DateTime\n</code></pre> <p>Advertisement: An\u00fancios encontrados nos artigos</p> <pre><code>- id: Integer (Primary Key)\n- entry_id: ForeignKey (Entry)\n- url: URL (URL do an\u00fancio)\n- title: String (T\u00edtulo do an\u00fancio)\n- tag: String (Categoria do an\u00fancio)\n- thumbnail: URL (URL da thumbnail)\n- screenshot: String (Caminho no MinIO)\n- excerpt: String (Descri\u00e7\u00e3o)\n- category: Integer (Categoria LLM)\n- category_verbose: String (Descri\u00e7\u00e3o da categoria)\n- created_at: DateTime\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#etapa-1-spiders-coleta-de-urls","title":"\ud83d\udd77\ufe0f ETAPA 1: Spiders - Coleta de URLs","text":""},{"location":"sobre/Pipeline_dados_scraps/#funcao-dos-spiders","title":"Fun\u00e7\u00e3o dos Spiders","text":"<p>Os spiders s\u00e3o componentes Scrapy respons\u00e1veis por navegar nas p\u00e1ginas iniciais dos portais de not\u00edcias e coletar URLs de artigos v\u00e1lidos. Eles constituem o ponto de entrada do pipeline, identificando e filtrando links que ser\u00e3o posteriormente processados.</p>"},{"location":"sobre/Pipeline_dados_scraps/#como-funcionam","title":"\ud83d\udd27 Como Funcionam","text":"<ol> <li>Navega\u00e7\u00e3o: Acessam a p\u00e1gina inicial do portal (ex: <code>https://www.metropoles.com/</code>)</li> <li>Extra\u00e7\u00e3o: Identificam links de artigos usando seletores CSS/XPath</li> <li>Filtragem: Aplicam regras para aceitar apenas URLs de not\u00edcias v\u00e1lidas</li> <li>Armazenamento: Salvam URLs aprovadas na tabela <code>URLQueue</code></li> </ol>"},{"location":"sobre/Pipeline_dados_scraps/#portais-suportados","title":"\ud83d\udccb Portais Suportados","text":"<p>\u2705 Funcionais: - Metr\u00f3poles (<code>metropolesspider</code>) - IG (<code>igspider</code>)  - MaisGoi\u00e1s (<code>maisgoiasspider</code>) - AliadosBrasil (<code>aliadosbrasilspider</code>) - Veja (<code>vejaspider</code>) - R7 (<code>r7spider</code>) - UOL (<code>uolspider</code>) - Folha (<code>folhaspider</code>)</p>"},{"location":"sobre/Pipeline_dados_scraps/#configuracao-dos-spiders","title":"\u2699\ufe0f Configura\u00e7\u00e3o dos Spiders","text":"<p>Estrutura Base:</p> <pre><code>class MetropolesSpider(BaseSpider):\n    name = \"metropolesspider\"\n    start_urls = [\"https://www.metropoles.com/\"]\n    allowed_domains = [\"metropoles.com\"]\n\n    def allow_url(self, entry_url):\n        # L\u00f3gica de filtragem espec\u00edfica do portal\n        return True\n\n    def parse(self, response):\n        # Extrai URLs usando seletores CSS\n        for entry in response.css(\".noticia__titulo &gt; a\"):\n            url = entry.attrib.get(\"href\")\n            if url and self.allow_url(url):\n                yield URLItem(url=url)\n</code></pre> <p>Pipeline de Processamento:</p> <pre><code># pipelines.py\nclass PostgresPipeline:\n    def process_item(self, item, spider):\n        # Verifica se URL j\u00e1 existe\n        exists = self.session.query(URLQueue).filter(\n            URLQueue.url == item[\"url\"]\n        ).first()\n\n        if exists is None:\n            # Cria nova entrada na fila\n            URLQueue.create(self.session, item[\"url\"])\n\n        return item\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#execucao","title":"\ud83d\ude80 Execu\u00e7\u00e3o","text":"<p>Comandos individuais:</p> <pre><code>make crawl_metropoles  # Coleta URLs do Metr\u00f3poles\nmake crawl_ig          # Coleta URLs do IG\nmake crawl_veja        # Coleta URLs da Veja\n</code></pre> <p>Execu\u00e7\u00e3o em massa:</p> <pre><code>make crawl_all_working # Todos os portais funcionais\n</code></pre> <p>Verifica\u00e7\u00e3o:</p> <pre><code>-- Visualizar URLs coletadas\nSELECT COUNT(*) FROM urlqueue;\nSELECT url FROM urlqueue ORDER BY created_at DESC LIMIT 10;\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#etapa-2-plays-extracao-de-conteudo-e-anuncios","title":"\ud83c\udfad ETAPA 2: Plays - Extra\u00e7\u00e3o de Conte\u00fado e An\u00fancios","text":""},{"location":"sobre/Pipeline_dados_scraps/#funcao-dos-plays","title":"Fun\u00e7\u00e3o dos Plays","text":"<p>Os plays s\u00e3o componentes baseados em Playwright que acessam cada URL coletada pelos spiders e extraem tanto o conte\u00fado editorial quanto os an\u00fancios presentes na p\u00e1gina. Eles constituem o cora\u00e7\u00e3o do sistema de an\u00e1lise, capturando dados estruturados para posterior classifica\u00e7\u00e3o.</p>"},{"location":"sobre/Pipeline_dados_scraps/#como-funcionam_1","title":"\ud83d\udd27 Como Funcionam","text":"<ol> <li>Sele\u00e7\u00e3o de URLs: Buscam URLs na <code>URLQueue</code> filtradas por dom\u00ednio</li> <li>Navega\u00e7\u00e3o: Usam Playwright para abrir as p\u00e1ginas em um browser real</li> <li>Extra\u00e7\u00e3o Dual:</li> <li>Conte\u00fado Editorial: t\u00edtulo, descri\u00e7\u00e3o, corpo, tags</li> <li>An\u00fancios: t\u00edtulo, URL, thumbnail, categoria</li> <li>Armazenamento: Salvam dados nas tabelas <code>Entry</code> e <code>Advertisement</code></li> <li>Screenshots: Capturam imagens das p\u00e1ginas no MinIO</li> </ol>"},{"location":"sobre/Pipeline_dados_scraps/#estrutura-dos-dados-extraidos","title":"\ud83d\udcca Estrutura dos Dados Extra\u00eddos","text":"<p>EntryItem (Conte\u00fado Editorial):</p> <pre><code>@dataclass\nclass EntryItem:\n    title: str           # T\u00edtulo principal do artigo\n    url: str            # URL do artigo  \n    description: str    # Subt\u00edtulo/resumo\n    body: str          # Corpo completo do texto\n    tags: List[str]    # Tags/categorias\n    ads: List[AdItem]  # An\u00fancios encontrados\n    screenshot_path: str # Caminho da screenshot\n</code></pre> <p>AdItem (An\u00fancios):</p> <pre><code>@dataclass  \nclass AdItem:\n    title: str          # T\u00edtulo do an\u00fancio\n    url: str           # URL de destino\n    thumbnail_url: str # URL da imagem\n    tag: str          # Categoria/origem (ex: \"Taboola\")\n    screenshot_path: str # Screenshot do an\u00fancio\n    excerpt: str      # Descri\u00e7\u00e3o/texto\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#exemplo-de-play-funcional","title":"\ud83c\udfaf Exemplo de Play Funcional","text":"<p>Metr\u00f3poles Play:</p> <pre><code>class MetropolesPlay(BasePlay):\n    name = \"metropoles\"\n\n    @classmethod\n    def match(cls, url):\n        return \"metropoles.com\" in url\n\n    def run(self) -&gt; EntryItem:\n        with sync_playwright() as p:\n            browser = self.launch_browser(p)\n            page = browser.new_page()\n            page.goto(self.url, timeout=180_000)\n\n            # Extra\u00e7\u00e3o do conte\u00fado editorial\n            entry_title = page.locator(\"//h1\").first.inner_text()\n\n            description = \"\"\n            try:\n                description = page.locator(\".noticiaCabecalho__subtitulo\").inner_text()\n            except:\n                pass\n\n            # Corpo do artigo\n            body_paragraphs = []\n            selectors = [\".m-content\", \"article\", \".article-content\"]\n            for selector in selectors:\n                try:\n                    elements = page.locator(f\"{selector} p\").all()\n                    body_paragraphs = [p.inner_text() for p in elements]\n                    if body_paragraphs:\n                        break\n                except:\n                    continue\n\n            body = \" \".join(body_paragraphs)\n\n            # Extra\u00e7\u00e3o de an\u00fancios (Taboola)\n            ad_items = []\n            try:\n                page.locator(\"#taboola-below-article-thumbnails\").scroll_into_view_if_needed()\n                time.sleep(3)\n\n                ad_elements = page.locator(\".videoCube\").all()\n                for element in ad_elements:\n                    if element.is_visible():\n                        content = element.inner_html()\n                        ad_item = self.find_items(content)\n                        if ad_item.is_valid():\n                            ad_items.append(ad_item)\n            except:\n                pass\n\n            return EntryItem(\n                title=entry_title,\n                description=description, \n                body=body,\n                tags=[],\n                ads=ad_items,\n                url=self.url\n            )\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#processo-de-armazenamento","title":"\ud83d\uddc4\ufe0f Processo de Armazenamento","text":"<p>1. Sele\u00e7\u00e3o de URLs para Processamento:</p> <pre><code># scrape_no_openai.py\ndef get_urls_for_platform(session, domain):\n    \"\"\"Busca URLs da URLQueue filtradas por dom\u00ednio\"\"\"\n    return session.query(URLQueue).join(URLQueue.statuses).filter(\n        URLQueue.url.like(f\"%{domain}%\"),\n        QueueStatus.value == \"created\" \n    ).limit(100).all()\n</code></pre> <p>2. Processamento e Armazenamento:</p> <pre><code># Para cada URL processada:\n1. Play extrai conte\u00fado \u2192 EntryItem\n2. Salva Entry no PostgreSQL\n3. Para cada an\u00fancio \u2192 Advertisement no PostgreSQL  \n4. Screenshots salvos no MinIO\n5. Atualiza QueueStatus para \"completed\"\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#execucao_1","title":"\ud83d\ude80 Execu\u00e7\u00e3o","text":"<p>Comandos por portal:</p> <pre><code>make scrape_metropoles     # Scraping do Metr\u00f3poles\nmake scrape_ig            # Scraping do IG  \nmake scrape_maisgoias     # Scraping do MaisGoi\u00e1s\n</code></pre> <p>Execu\u00e7\u00e3o em massa:</p> <pre><code>make scrape_all_working   # Todos os portais funcionais\n</code></pre> <p>Comando direto (com argumentos):</p> <pre><code>docker compose exec scraper python scrape_no_openai.py --platform metropoles.com\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#monitoramento","title":"\ud83d\udcc8 Monitoramento","text":"<p>Verifica\u00e7\u00e3o no banco:</p> <pre><code>-- Artigos processados\nSELECT COUNT(*) FROM entry;\n\n-- An\u00fancios encontrados  \nSELECT COUNT(*) FROM advertisement;\n\n-- Status por portal\nSELECT \n    SUBSTRING(url FROM 'https://([^/]+)') as portal,\n    COUNT(*) as total_urls\nFROM urlqueue \nGROUP BY portal;\n</code></pre> <p>Verifica\u00e7\u00e3o no MinIO: - Console: <code>http://localhost:9001</code> - Credenciais: <code>minioadmin/minioadmin</code> - Buckets organizados por portal: <code>metropoles_com/</code>, <code>ig_com_br/</code>, etc.</p>"},{"location":"sobre/Pipeline_dados_scraps/#etapa-3-armazenamento-de-dados","title":"\ud83d\uddc4\ufe0f ETAPA 3: Armazenamento de Dados","text":""},{"location":"sobre/Pipeline_dados_scraps/#postgresql-banco-principal","title":"PostgreSQL - Banco Principal","text":"<p>O Check-up utiliza PostgreSQL como banco principal para armazenar dados estruturados. O schema \u00e9 definido em <code>models.py</code> usando SQLAlchemy ORM.</p> <p>Fluxo de Armazenamento:</p> <pre><code>graph TD\n    A[Spider coleta URL] --&gt; B[URLQueue - Fila de URLs]\n    B --&gt; C[Play processa URL]\n    C --&gt; D[Entry - Artigo principal]\n    C --&gt; E[Advertisement - An\u00fancios encontrados]\n    D --&gt; F[Portal - Info do portal]\n    E --&gt; F\n</code></pre> <p>Relacionamentos: - <code>Portal</code> 1:N <code>Entry</code> (Um portal tem muitos artigos) - <code>Entry</code> 1:N <code>Advertisement</code> (Um artigo tem muitos an\u00fancios) - <code>URLQueue</code> 1:N <code>QueueStatus</code> (Uma URL tem hist\u00f3rico de status)</p>"},{"location":"sobre/Pipeline_dados_scraps/#minio-armazenamento-de-arquivos","title":"MinIO - Armazenamento de Arquivos","text":"<p>O MinIO \u00e9 usado para armazenar screenshots e m\u00eddias capturadas durante o scraping.</p> <p>Estrutura de Buckets:</p> <pre><code>scraped-articles/\n\u251c\u2500\u2500 metropoles_com/\n\u2502   \u251c\u2500\u2500 2025/01/07/\n\u2502   \u2502   \u251c\u2500\u2500 entry_screenshots/\n\u2502   \u2502   \u2514\u2500\u2500 ad_screenshots/\n\u251c\u2500\u2500 ig_com_br/\n\u2502   \u251c\u2500\u2500 2025/01/07/\n\u2502   \u2502   \u251c\u2500\u2500 entry_screenshots/\n\u2502   \u2502   \u2514\u2500\u2500 ad_screenshots/\n</code></pre> <p>Processo de Upload:</p> <pre><code># storage.py\ndef upload_file(local_path, minio_path):\n    \"\"\"Upload de arquivo para MinIO\"\"\"\n    client.fput_object(\n        bucket_name=\"scraped-articles\",\n        object_name=minio_path,\n        file_path=local_path\n    )\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#status-de-processamento","title":"\ud83d\udd04 Status de Processamento","text":"<p>O sistema mant\u00e9m controle detalhado do status de cada URL atrav\u00e9s da tabela <code>QueueStatus</code>:</p> <p>Estados poss\u00edveis: - <code>created</code>: URL coletada, aguardando processamento - <code>processing</code>: URL sendo processada por um play - <code>completed</code>: Processamento conclu\u00eddo com sucesso - <code>failed</code>: Erro durante processamento - <code>retry</code>: Agendada para nova tentativa</p> <p>Exemplo de consulta de status:</p> <pre><code>SELECT \n    u.url,\n    qs.value as status,\n    qs.created_at\nFROM urlqueue u\nJOIN queuestatus qs ON u.id = qs.url_queue_id\nWHERE qs.value = 'completed'\nORDER BY qs.created_at DESC;\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#etapa-4-api-backend-fastapi","title":"\ud83d\udd17 ETAPA 4: API Backend (FastAPI)","text":""},{"location":"sobre/Pipeline_dados_scraps/#estrutura-da-api","title":"Estrutura da API","text":"<p>O backend em FastAPI (<code>web/server/main.py</code>) serve como ponte entre os dados armazenados e o frontend, fornecendo endpoints RESTful para acesso aos dados coletados.</p> <p>Endpoints Principais:</p> <p>1. Listar Portais:</p> <pre><code>@app.get(\"/portais\", response_model=List[str])\ndef listar_portais():\n    \"\"\"Retorna lista de portais com dados dispon\u00edveis\"\"\"\n    # Busca diretamente nos buckets do MinIO\n    portais = set()\n    for obj in minio_client.list_objects(MINIO_BUCKET, recursive=True):\n        portal = obj.object_name.split('/')[0]\n        portais.add(portal)\n    return sorted(list(portais))\n</code></pre> <p>2. Buscar Not\u00edcias:</p> <pre><code>@app.get(\"/noticias/{portal}\")\ndef buscar_noticias(\n    portal: str,\n    q: str = Query(None, description=\"Termo de busca\"),\n    page: int = Query(1, ge=1),\n    limit: int = Query(10, ge=1, le=100)\n):\n    \"\"\"Busca not\u00edcias de um portal espec\u00edfico com pagina\u00e7\u00e3o\"\"\"\n    # Busca arquivos JSON no MinIO do portal especificado\n    # Aplica filtros de busca se fornecidos\n    # Retorna dados paginados\n</code></pre> <p>3. Detalhes da Not\u00edcia:</p> <pre><code>@app.get(\"/noticia/{portal}/{filename}\")\ndef obter_noticia(portal: str, filename: str):\n    \"\"\"Retorna dados completos de uma not\u00edcia espec\u00edfica\"\"\"\n    # Busca arquivo JSON espec\u00edfico no MinIO\n    # Retorna conte\u00fado completo incluindo an\u00fancios\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#fluxo-de-dados-da-api","title":"\ud83d\udd04 Fluxo de Dados da API","text":"<pre><code>graph TD\n    A[Frontend Request] --&gt; B[FastAPI Endpoint]\n    B --&gt; C[Consulta MinIO]\n    C --&gt; D[Processa Dados JSON]\n    D --&gt; E[Aplica Filtros/Pagina\u00e7\u00e3o]\n    E --&gt; F[Retorna Response]\n    F --&gt; G[Frontend Atualiza UI]\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#formato-de-resposta","title":"\ud83d\udcca Formato de Resposta","text":"<p>Estrutura de Not\u00edcia:</p> <pre><code>{\n  \"title\": \"T\u00edtulo da not\u00edcia\",\n  \"url\": \"https://portal.com/noticia\",\n  \"description\": \"Subt\u00edtulo ou resumo\",\n  \"body\": \"Corpo completo da not\u00edcia...\",\n  \"tags\": [\"pol\u00edtica\", \"sa\u00fade\"],\n  \"timestamp\": \"2025-01-07T10:30:00Z\",\n  \"portal\": \"metropoles_com\",\n  \"ads\": [\n    {\n      \"title\": \"An\u00fancio de produto\",\n      \"url\": \"https://anuncio.com\",\n      \"tag\": \"Taboola\",\n      \"thumbnail\": \"https://thumb.com/img.jpg\"\n    }\n  ]\n}\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#etapa-5-frontend-react","title":"\ud83d\udda5\ufe0f ETAPA 5: Frontend (React)","text":""},{"location":"sobre/Pipeline_dados_scraps/#arquitetura-do-frontend","title":"Arquitetura do Frontend","text":"<p>O frontend em React (<code>web/client/src/</code>) oferece interface intuitiva para explorar os dados coletados, com foco na visualiza\u00e7\u00e3o de not\u00edcias e an\u00fancios de sa\u00fade.</p> <p>Componentes Principais:</p> <p>1. Home.tsx - P\u00e1gina Principal:</p> <pre><code>const Home = () =&gt; {\n  const [portais, setPortais] = useState&lt;string[]&gt;([]);\n  const [noticias, setNoticias] = useState&lt;any[]&gt;([]);\n  const [portalSelecionado, setPortalSelecionado] = useState&lt;string | null&gt;(null);\n\n  // Busca lista de portais dispon\u00edveis\n  useEffect(() =&gt; {\n    fetch(`${API_URL}/portais`)\n      .then(res =&gt; res.json())\n      .then(setPortais);\n  }, []);\n\n  // Busca not\u00edcias quando portal \u00e9 selecionado\n  useEffect(() =&gt; {\n    if (portalSelecionado) {\n      fetch(`${API_URL}/noticias/${portalSelecionado}`)\n        .then(res =&gt; res.json())\n        .then(setNoticias);\n    }\n  }, [portalSelecionado]);\n}\n</code></pre> <p>2. Funcionalidades Implementadas:</p> <ul> <li>Sele\u00e7\u00e3o de Portal: Tabs para escolher entre portais dispon\u00edveis</li> <li>Busca: Campo de texto para filtrar not\u00edcias por t\u00edtulo/conte\u00fado</li> <li>Pagina\u00e7\u00e3o: Navega\u00e7\u00e3o atrav\u00e9s de grandes volumes de dados</li> <li>Visualiza\u00e7\u00e3o Detalhada: Modal com conte\u00fado completo da not\u00edcia</li> <li>Estat\u00edsticas: Contadores de not\u00edcias por portal</li> </ul>"},{"location":"sobre/Pipeline_dados_scraps/#interface-do-usuario","title":"\ud83c\udfa8 Interface do Usu\u00e1rio","text":"<p>Layout Principal:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Check-up - An\u00e1lise de An\u00fancios de Sa\u00fade \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [Metr\u00f3poles] [IG] [Veja] [R7] [UOL]...  \u2502 &lt;- Tabs dos portais\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \ud83d\udd0d [___________________] [Buscar]       \u2502 &lt;- Campo de busca\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 \ud83d\udcf0 T\u00edtulo da Not\u00edcia               \u2502 \u2502\n\u2502 \u2502 \ud83c\udff7\ufe0f  Portal: Metr\u00f3poles             \u2502 \u2502 &lt;- Cards de not\u00edcias\n\u2502 \u2502 \ud83d\udcc5 07/01/2025 - \ud83d\udd17 2 an\u00fancios      \u2502 \u2502\n\u2502 \u2502 [Ver Detalhes]                     \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2190 1 2 3 ... 10 \u2192                       \u2502 &lt;- Pagina\u00e7\u00e3o\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#fluxo-de-interacao","title":"\ud83d\udd04 Fluxo de Intera\u00e7\u00e3o","text":"<ol> <li>Carregamento Inicial: Busca lista de portais dispon\u00edveis</li> <li>Sele\u00e7\u00e3o de Portal: Usu\u00e1rio clica em tab do portal desejado</li> <li>Exibi\u00e7\u00e3o de Not\u00edcias: Lista paginada de not\u00edcias do portal</li> <li>Busca: Usu\u00e1rio pode filtrar por termos espec\u00edficos</li> <li>Detalhamento: Modal mostra conte\u00fado completo + an\u00fancios</li> <li>Navega\u00e7\u00e3o: Pagina\u00e7\u00e3o para explorar mais resultados</li> </ol>"},{"location":"sobre/Pipeline_dados_scraps/#recursos-de-ux","title":"\ud83d\udcf1 Recursos de UX","text":"<ul> <li>Loading States: Skeletons durante carregamento</li> <li>Error Handling: Mensagens de erro amig\u00e1veis  </li> <li>Responsive Design: Adapt\u00e1vel a diferentes tamanhos de tela</li> <li>Busca Reativa: Resultados atualizados em tempo real</li> <li>Indicadores Visuais: Badges para n\u00famero de an\u00fancios</li> </ul>"},{"location":"sobre/Pipeline_dados_scraps/#fluxo-completo-de-dados","title":"\ud83d\udd04 FLUXO COMPLETO DE DADOS","text":""},{"location":"sobre/Pipeline_dados_scraps/#pipeline-end-to-end","title":"Pipeline End-to-End","text":"<pre><code>graph TD\n    A[Portal de Not\u00edcias] --&gt; B[Spider - Scrapy]\n    B --&gt; C[URLQueue - PostgreSQL]\n    C --&gt; D[Play - Playwright]\n    D --&gt; E[Entry - PostgreSQL]\n    D --&gt; F[Advertisement - PostgreSQL]\n    D --&gt; G[Screenshots - MinIO]\n    E --&gt; H[FastAPI Backend]\n    F --&gt; H\n    G --&gt; H\n    H --&gt; I[React Frontend]\n    I --&gt; J[Interface do Usu\u00e1rio]\n\n    style A fill:#e1f5fe\n    style J fill:#e8f5e8\n    style H fill:#fff3e0\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#cronograma-de-execucao","title":"\u23f1\ufe0f Cronograma de Execu\u00e7\u00e3o","text":"<p>1. Configura\u00e7\u00e3o (\u00fanica vez):</p> <pre><code>make setup  # ~5-10 minutos\n</code></pre> <p>2. Coleta de URLs (di\u00e1ria):</p> <pre><code>make crawl_all_working  # ~10-15 minutos\n# Resultado: ~500-1000 URLs novas por portal\n</code></pre> <p>3. Extra\u00e7\u00e3o de Conte\u00fado (cont\u00ednua):</p> <pre><code>make scrape_all_working  # ~2-4 horas\n# Resultado: ~100-200 artigos processados por hora\n</code></pre> <p>4. Visualiza\u00e7\u00e3o (imediata):</p> <pre><code># Frontend dispon\u00edvel em: http://localhost:5173\n# API dispon\u00edvel em: http://localhost:8000\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#comandos-de-execucao","title":"\ud83d\ude80 Comandos de Execu\u00e7\u00e3o","text":""},{"location":"sobre/Pipeline_dados_scraps/#setup-inicial","title":"Setup Inicial","text":"<pre><code># Configura\u00e7\u00e3o completa do ambiente\nmake setup\n\n# Verificar se servi\u00e7os est\u00e3o rodando\ndocker compose ps\n\n# Acessar logs em tempo real\ndocker compose logs -f\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#execucao-do-pipeline","title":"Execu\u00e7\u00e3o do Pipeline","text":"<p>1. Coleta de URLs (Crawling):</p> <pre><code># Todos os portais funcionais\nmake crawl_all_working\n\n# Portais individuais\nmake crawl_metropoles\nmake crawl_ig\nmake crawl_veja\nmake crawl_uol\n</code></pre> <p>2. Extra\u00e7\u00e3o de Conte\u00fado (Scraping):</p> <pre><code># Todos os portais funcionais  \nmake scrape_all_working\n\n# Portais individuais\nmake scrape_metropoles\nmake scrape_ig\nmake scrape_maisgoias\n</code></pre> <p>3. Pipeline Completo:</p> <pre><code># Executa crawl + scraping sequencialmente\nmake pipeline_complete\n\n# Workflow otimizado\nmake collect_working\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#monitoramento-e-debugging","title":"Monitoramento e Debugging","text":"<p>Acesso ao container:</p> <pre><code>make bash\n</code></pre> <p>Verifica\u00e7\u00f5es no banco:</p> <pre><code># Dentro do container\npython -c \"\nfrom models import *\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\nfrom decouple import config\n\nengine = create_engine(config('DATABASE_URL'))\nsession = Session(engine)\n\nprint('URLs na fila:', URLQueue.select().count())\nprint('Artigos processados:', Entry.select().count()) \nprint('An\u00fancios encontrados:', Advertisement.select().count())\n\"\n</code></pre> <p>Verifica\u00e7\u00e3o do MinIO: - Interface: <code>http://localhost:9001</code> - Credenciais: <code>minioadmin/minioadmin</code></p> <p>Frontend e API: - Frontend: <code>http://localhost:5173</code> - API: <code>http://localhost:8000</code> - Docs da API: <code>http://localhost:8000/docs</code></p>"},{"location":"sobre/Pipeline_dados_scraps/#troubleshooting","title":"Troubleshooting","text":"<p>Problema: Playwright browsers n\u00e3o instalados</p> <pre><code>docker compose down\ndocker compose build --no-cache\nmake start\n</code></pre> <p>Problema: Banco n\u00e3o conecta</p> <pre><code>make wait-for-db\n# ou\nmake stop &amp;&amp; make start\n</code></pre> <p>Problema: MinIO n\u00e3o acess\u00edvel</p> <pre><code>docker compose ps\n# Verificar se container healthcheck_minio est\u00e1 \"healthy\"\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#configuracao-e-customizacao","title":"\ud83d\udd27 Configura\u00e7\u00e3o e Customiza\u00e7\u00e3o","text":""},{"location":"sobre/Pipeline_dados_scraps/#variaveis-de-ambiente-env","title":"Vari\u00e1veis de Ambiente (.env)","text":"<pre><code># Banco de dados\nDATABASE_URL=postgresql://postgres:supersecret@healthcheck_db:5432/healthcheck\n\n# MinIO\nMINIO_ENDPOINT=minio:9000\nMINIO_ACCESS_KEY=minioadmin\nMINIO_SECRET_KEY=minioadmin\nMINIO_SECURE=False\n\n# Opcional: OpenAI para classifica\u00e7\u00e3o\nOPENAI_API_KEY=sk-...\n\n# Credenciais de portais (se necess\u00e1rio)\nFOLHA_USERNAME=usuario\nFOLHA_PASSWORD=senha\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#adicionando-novo-portal","title":"Adicionando Novo Portal","text":"<p>1. Criar Spider:</p> <pre><code># spiders/novoportal.py\nclass NovoPortalSpider(BaseSpider):\n    name = \"novoportalspider\"\n    start_urls = [\"https://novoportal.com/\"]\n    # ... implementa\u00e7\u00e3o espec\u00edfica\n</code></pre> <p>2. Criar Play:</p> <pre><code># plays/novoportal.py\nclass NovoPortalPlay(BasePlay):\n    name = \"novoportal\"\n\n    @classmethod\n    def match(cls, url):\n        return \"novoportal.com\" in url\n    # ... implementa\u00e7\u00e3o espec\u00edfica\n</code></pre> <p>3. Adicionar comandos no Makefile:</p> <pre><code>crawl_novoportal:\n    docker compose run scraper python crawl.py novoportalspider\n\nscrape_novoportal:\n    docker compose exec scraper python scrape_no_openai.py --platform novoportal.com\n</code></pre> <p>4. Testar:</p> <pre><code>make crawl_novoportal\nmake scrape_novoportal\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#metricas-e-analises","title":"\ud83d\udcc8 M\u00e9tricas e An\u00e1lises","text":""},{"location":"sobre/Pipeline_dados_scraps/#consultas-uteis","title":"Consultas \u00dateis","text":"<p>Resumo geral:</p> <pre><code>SELECT \n    COUNT(DISTINCT p.name) as total_portais,\n    COUNT(DISTINCT e.id) as total_artigos,\n    COUNT(DISTINCT a.id) as total_anuncios,\n    AVG(ads_por_artigo.count) as media_anuncios_por_artigo\nFROM portal p\nLEFT JOIN entry e ON p.id = e.portal_id\nLEFT JOIN advertisement a ON e.id = a.entry_id\nLEFT JOIN (\n    SELECT entry_id, COUNT(*) as count \n    FROM advertisement \n    GROUP BY entry_id\n) ads_por_artigo ON e.id = ads_por_artigo.entry_id;\n</code></pre> <p>Top portais por volume:</p> <pre><code>SELECT \n    p.name,\n    COUNT(e.id) as artigos,\n    COUNT(a.id) as anuncios\nFROM portal p\nLEFT JOIN entry e ON p.id = e.portal_id  \nLEFT JOIN advertisement a ON e.id = a.entry_id\nGROUP BY p.name\nORDER BY artigos DESC;\n</code></pre> <p>Evolu\u00e7\u00e3o temporal:</p> <pre><code>SELECT \n    DATE(e.created_at) as data,\n    COUNT(e.id) as artigos_processados,\n    COUNT(a.id) as anuncios_encontrados\nFROM entry e\nLEFT JOIN advertisement a ON e.id = a.entry_id\nWHERE e.created_at &gt;= CURRENT_DATE - INTERVAL '7 days'\nGROUP BY DATE(e.created_at)\nORDER BY data DESC;\n</code></pre>"},{"location":"sobre/Pipeline_dados_scraps/#proximos-passos","title":"\ud83c\udfaf Pr\u00f3ximos Passos","text":""},{"location":"sobre/Pipeline_dados_scraps/#melhorias-planejadas","title":"Melhorias Planejadas","text":"<ol> <li>Classifica\u00e7\u00e3o Autom\u00e1tica: Integra\u00e7\u00e3o com LLM para categoriza\u00e7\u00e3o de an\u00fancios</li> <li>Dashboard Analytics: Painel com m\u00e9tricas em tempo real</li> <li>Alertas: Notifica\u00e7\u00f5es para an\u00fancios suspeitos de desinforma\u00e7\u00e3o  </li> <li>API P\u00fablica: Endpoints para acesso externo aos dados</li> <li>Exporta\u00e7\u00e3o: Funcionalidades para download de relat\u00f3rios</li> </ol>"},{"location":"sobre/Pipeline_dados_scraps/#escalabilidade","title":"Escalabilidade","text":"<ol> <li>Processamento Paralelo: Multiple workers para scraping</li> <li>Cache Redis: Cache de consultas frequentes</li> <li>CDN: Distribui\u00e7\u00e3o de screenshots via CDN</li> <li>Monitoring: Prometheus + Grafana para m\u00e9tricas</li> <li>Auto-scaling: Ajuste autom\u00e1tico de recursos baseado na carga</li> </ol>"},{"location":"sobre/backlog/","title":"Backlog do Projeto \u2013 eh-fake","text":"<p>Desenvolver uma solu\u00e7\u00e3o baseada em IA (Machine Learning + RAG) para identificar e classificar automaticamente not\u00edcias como desinformativas, informativas ou neutras, com foco em apoio \u00e0 checagem e combate \u00e0 desinforma\u00e7\u00e3o.</p>"},{"location":"sobre/backlog/#objetivo-geral","title":"Objetivo Geral","text":"<p>Refatorar e escalar a ferramenta Check-up, que realiza coleta de not\u00edcias de portais jornal\u00edsticos, visando aprimorar o scraping, aumentar a cobertura de fontes e implementar orquestra\u00e7\u00e3o e automa\u00e7\u00e3o dos processos com Apache Airflow. O objetivo final \u00e9 fornecer uma base robusta de dados para an\u00e1lise de desinforma\u00e7\u00e3o.</p>"},{"location":"sobre/backlog/#historias-de-usuario","title":"Hist\u00f3rias de Usu\u00e1rio","text":""},{"location":"sobre/backlog/#como-pesquisador-de-desinformacao-quero","title":"Como pesquisador de desinforma\u00e7\u00e3o, quero:","text":"<ul> <li>Consultar um banco de dados com not\u00edcias atualizadas de diversos portais</li> <li>Ter acesso \u00e0s classifica\u00e7\u00f5es de not\u00edcias (informativa, desinformativa, fora de escopo)</li> <li>Confiar na consist\u00eancia dos dados, com poucos erros de scraping</li> </ul>"},{"location":"sobre/backlog/#como-desenvolvedor-quero","title":"Como desenvolvedor, quero:","text":"<ul> <li>Refatorar os scrapers para que sejam mais robustos e f\u00e1ceis de manter</li> <li>Orquestrar os scrapers no Airflow, com logs claros e retries autom\u00e1ticos</li> <li>Ter documenta\u00e7\u00e3o clara sobre como adicionar novos scrapers ou pipelines</li> </ul>"},{"location":"sobre/backlog/#como-analista-de-dados-quero","title":"Como analista de dados, quero:","text":"<ul> <li>Receber dados limpos, organizados e bem estruturados para an\u00e1lises posteriores</li> <li>Verificar rapidamente se algum scraper falhou ou parou de funcionar</li> </ul>"},{"location":"sobre/backlog/#epicos-e-funcionalidades","title":"\u00c9picos e Funcionalidades","text":""},{"location":"sobre/backlog/#ep001-refatoracao-dos-scrapers","title":"EP001 \u2013 Refatora\u00e7\u00e3o dos Scrapers","text":"<ul> <li>FT001.1 \u2013 Revisar e otimizar scrapers atuais (10 portais)</li> <li>FT001.2 \u2013 Corrigir bugs e tratar casos de exce\u00e7\u00e3o nos scrapers</li> <li>FT001.3 \u2013 Padronizar a sa\u00edda dos scrapers (schema unificado)</li> </ul>"},{"location":"sobre/backlog/#ep002-expansao-da-coleta-de-dados","title":"EP002 \u2013 Expans\u00e3o da Coleta de Dados","text":"<ul> <li>FT002.1 \u2013 Adicionar scraping de novos portais e fontes de not\u00edcia (meta: +10)</li> <li>FT002.2 \u2013 Criar mecanismo para f\u00e1cil adi\u00e7\u00e3o de novos scrapers no futuro</li> <li>FT002.3 \u2013 Implementar verifica\u00e7\u00e3o autom\u00e1tica de falhas nos scrapers</li> </ul>"},{"location":"sobre/backlog/#ep003-orquestracao-e-automacao","title":"EP003 \u2013 Orquestra\u00e7\u00e3o e Automa\u00e7\u00e3o","text":"<ul> <li>FT003.1 \u2013 Migrar scrapers para DAGs no Apache Airflow</li> <li>FT003.2 \u2013 Implementar monitoramento, retries e logs estruturados</li> <li>FT003.3 \u2013 Criar pipeline de pr\u00e9-processamento e armazenamento dos dados</li> </ul>"},{"location":"sobre/backlog/#ep004-classificacao-de-noticias","title":"EP004 \u2013 Classifica\u00e7\u00e3o de Not\u00edcias","text":"<ul> <li>FT004.1 \u2013 Implementar classifica\u00e7\u00e3o das not\u00edcias em: <ul> <li>Informativa</li> <li>Desinformativa</li> <li>Fora de escopo</li> </ul> </li> <li>FT004.2 \u2013 Criar pipeline de classifica\u00e7\u00e3o integrado ao Airflow</li> <li>FT004.3 \u2013 Validar resultados manualmente e ajustar regras ou modelo</li> </ul>"},{"location":"sobre/backlog/#ep005-documentacao-e-sustentacao","title":"EP005 \u2013 Documenta\u00e7\u00e3o e Sustenta\u00e7\u00e3o","text":"<ul> <li>FT005.1 \u2013 Documentar arquitetura, scrapers, pipelines e processos</li> <li>FT005.2 \u2013 Criar guia de contribui\u00e7\u00e3o e manuten\u00e7\u00e3o dos scrapers</li> <li>FT005.3 \u2013 Publicar documenta\u00e7\u00e3o no GitHub Pages</li> </ul>"},{"location":"sobre/backlog/#mapa-de-dependencias","title":"Mapa de Depend\u00eancias","text":"Feature Depende de Motivo da depend\u00eancia FT003.1 \u2013 Migrar para Airflow FT001.3 \u2013 Padronizar sa\u00edda Dados precisam estar padronizados para criar DAGs robustas FT002.3 \u2013 Verifica\u00e7\u00e3o autom\u00e1tica de falhas FT003.2 \u2013 Monitoramento no Airflow Monitoramento \u00e9 necess\u00e1rio para notificar falhas FT004.2 \u2013 Pipeline de classifica\u00e7\u00e3o FT003.3 \u2013 Pipeline de armazenamento Classifica\u00e7\u00e3o ocorre sobre dados coletados e armazenados FT004.3 \u2013 Valida\u00e7\u00e3o manual FT004.1 \u2013 Classifica\u00e7\u00e3o S\u00f3 valida ap\u00f3s a classifica\u00e7\u00e3o estar operando FT005.3 \u2013 Publica\u00e7\u00e3o da documenta\u00e7\u00e3o FT005.1 e FT005.2 Documenta\u00e7\u00e3o precisa estar conclu\u00edda antes da publica\u00e7\u00e3o"},{"location":"sobre/backlog/#roadmap-e-releases","title":"Roadmap e Releases","text":"Release Data Entregas principais \u00c9pico R1 13/05/2025 \u2714\ufe0f Kick-off e primeira revis\u00e3o dos scrapers atuais EP001 \u2013 Refatora\u00e7\u00e3o dos Scrapers R2 21/05/2025 \u2714\ufe0f Refatora\u00e7\u00e3o completa dos 10 scrapers + padroniza\u00e7\u00e3o EP001 \u2013 Refatora\u00e7\u00e3o dos Scrapers R3 28/05/2025 \u2714\ufe0f Adi\u00e7\u00e3o de novos scrapers (meta +5) EP002 \u2013 Expans\u00e3o da Coleta de Dados R4 04/06/2025 \u2714\ufe0f Adi\u00e7\u00e3o de mais scrapers (meta +5) + modulariza\u00e7\u00e3o EP002 \u2013 Expans\u00e3o da Coleta de Dados R5 11/06/2025 \u2714\ufe0f In\u00edcio da migra\u00e7\u00e3o para Airflow + primeiros DAGs EP003 \u2013 Orquestra\u00e7\u00e3o e Automa\u00e7\u00e3o R6 18/06/2025 \u2714\ufe0f Pipeline completo no Airflow + monitoramento/logs EP003 \u2013 Orquestra\u00e7\u00e3o e Automa\u00e7\u00e3o R7 25/06/2025 \u2714\ufe0f Classifica\u00e7\u00e3o de not\u00edcias + documenta\u00e7\u00e3o finalizada EP004 \u2013 Classifica\u00e7\u00e3o de Not\u00edcias, EP005 \u2013 Documenta\u00e7\u00e3o e Sustenta\u00e7\u00e3o"},{"location":"sobre/backlog/#timeline","title":"Timeline","text":"Semana Per\u00edodo Entregas principais 1 13-17/05 R1 \u2013 In\u00edcio + revis\u00e3o geral dos scrapers existentes 2 18-24/05 R2 \u2013 Refatora\u00e7\u00e3o dos scrapers + padroniza\u00e7\u00e3o dos dados 3 25-31/05 R3 \u2013 Adicionar 5 novos scrapers 4 01-07/06 R4 \u2013 Adicionar +5 scrapers + estrutura modular para scrapers 5 08-14/06 R5 \u2013 Migrar scrapers para DAGs no Airflow 6 15-21/06 R6 \u2013 Implementar monitoramento, retries e pipeline completo 7 22-25/06 R7 \u2013 Implementar classifica\u00e7\u00e3o + valida\u00e7\u00e3o + documenta\u00e7\u00e3o"},{"location":"sobre/backlog/#links-uteis","title":"Links \u00dateis","text":"<p>\u2022\u2060  \u2060RAGFlow \u2022\u2060  \u2060Maritaca AI \u2013 Sabi\u00e1 \u2022\u2060  \u2060LlamaIndex \u2022\u2060  \u2060NeMo Guardrails \u2022\u2060  \u2060Temas mapeados \u2013 Google Sheets</p>"},{"location":"sobre/backlog/#historico-do-documento","title":"Hist\u00f3rico do documento","text":"Vers\u00e3o Data Descri\u00e7\u00e3o Autor 1.0 21/04/2025 Cria\u00e7\u00e3o do documento Amanda Campos 1.1 27/04/2025 Atualiza\u00e7\u00e3o do Roadmap Luis Miranda 1.2 05/05/2025 atualiza\u00e7\u00e3o das hist\u00f3rias de usu\u00e1rios e roadmap Amanda Campos"},{"location":"sobre/check-up/","title":"Check-up","text":"<p>O Check-up \u00e9 um subprojeto essencial dentro do ecossistema Eh-Fake, focado em garantir a qualidade dos dados coletados semanalmente por spiders. Ele atua como uma camada de verifica\u00e7\u00e3o e organiza\u00e7\u00e3o das informa\u00e7\u00f5es extra\u00eddas, validando a consist\u00eancia, a estrutura e a confiabilidade das fontes.</p>"},{"location":"sobre/check-up/#objetivos-principais","title":"Objetivos principais:","text":"<ul> <li>Automatizar a verifica\u00e7\u00e3o e limpeza dos dados coletados.</li> <li>Identificar problemas em spiders existentes.</li> <li>Organizar e publicar releases semanais com conte\u00fado validado.</li> <li>Ajudar o time de desenvolvimento a manter padr\u00f5es de qualidade.</li> </ul> <p>Atualmente, o projeto \u00e9 dividido entre duas frentes:</p> <ol> <li>Desenvolvimento de novas spiders</li> <li>Melhoria cont\u00ednua das spiders existentes</li> </ol> <p>O roadmap e as tarefas s\u00e3o planejados em ciclos semanais com foco em entregas incrementais e documentadas.</p>"},{"location":"sobre/equipe/","title":"Equipe","text":"<p>O Eh-Fake \u00e9 desenvolvido por uma equipe multidisciplinar de estudantes engajados na constru\u00e7\u00e3o de solu\u00e7\u00f5es criativas contra a desinforma\u00e7\u00e3o.</p>"},{"location":"sobre/equipe/#estrutura-da-equipe","title":"Estrutura da Equipe","text":"Nome GitHub Sistema de governan\u00e7a Amanda Campos @acamposs Community Manager Let\u00edcia Paiva @leticiakrpaiva Registro de Atas Luis Miranda @LuisMiranda10 Mantenedores Philipe Barbosa @PhMoraiis Registro de Atas Raissa Andrade @RaissaAndradeS Docs Leads Tiago Albuquerque @Tiago1604 Release Managers Vin\u00edcius Mendes @yabamiah Docs Leads Fause Carlos @FauseSkyWalker Mantenedor Vinicius Vieira @viniciusvieira00 Release Managers <p>Obs.: A equipe adota pr\u00e1ticas \u00e1geis com reuni\u00f5es semanais e divis\u00e3o clara de responsabilidades para facilitar a colabora\u00e7\u00e3o e a entrega cont\u00ednua.</p> <p>Se voc\u00ea deseja contribuir, confira nosso Guia de Contribui\u00e7\u00e3o.</p>"},{"location":"sobre/o-que-e/","title":"O que \u00e9 o Eh-Fake?","text":"<p>O Eh-Fake \u00e9 uma iniciativa acad\u00eamica que une jornalismo investigativo para combater a desinforma\u00e7\u00e3o e fomentar o pensamento cr\u00edtico. O projeto busca desenvolver ferramentas tecnol\u00f3gicas e estrat\u00e9gias de comunica\u00e7\u00e3o capazes de identificar, analisar e desmentir fake news com linguagem acess\u00edvel e visualmente atrativa.</p> <p>Na disciplina de Gest\u00e3o de Configura\u00e7\u00e3o e Evolu\u00e7\u00e3o de Software, os principais objetivos relacionados ao projeto incluem:</p> <ul> <li>Melhorar a ferramenta Check-up e transform\u00e1-la em um produto de software robusto.</li> <li>Aumentar a curva de aprendizagem sobre versionamentos de projetos e suas vulnerabilidades.</li> <li>Expandir o conhecimento em IA generativa para aprimorar as solu\u00e7\u00f5es desenvolvidas.</li> </ul> <p>Al\u00e9m de sua fun\u00e7\u00e3o pr\u00e1tica, o projeto tamb\u00e9m serve como estudo de caso para boas pr\u00e1ticas em desenvolvimento colaborativo, documenta\u00e7\u00e3o t\u00e9cnica, testes automatizados e entregas cont\u00ednuas.</p>"}]}